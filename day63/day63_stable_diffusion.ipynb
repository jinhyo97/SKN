{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "\n",
    "paper: [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)\n",
    "\n",
    "<br>\n",
    "\n",
    "이미지를 생성하는 딥러닝 기반 모델. <br>\n",
    "이 모델은 딥러닝의 **GAN**과 확산 모델의 기술을 결합하여 이미지의 세밀함과 현실감을 높힘. <br>\n",
    "기본적으로 입력된 텍스트를 토대로, 그 의미와 관련 있는 이미지를 점진적으로 생성해 가는 방식으로 작동.\n",
    "\n",
    "\n",
    "Auto Encoder 모델을 계층화하였으며, 이미지 합성 분야에서 획기적인 결과. <br>\n",
    "diffusion model은 다른 생성 모델들과 다르게 쉽게 다른 분야 (inpainting, 색상 추가-colorization)에 적용 가능. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Model\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvwAn1%2FbtrNwkjFAn3%2FAvT141LiMsckI2XpEZtYSK%2Fimg.png\" width=600>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FETLeE%2Fbtr6bu1KNlU%2FoRi4IGQcMyXOA1Bcc2HUSK%2Fimg.png\" width=600>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnNaWO%2Fbtr6ANOKwPu%2F6fkAIJ9hUMKckICCqr46XK%2Fimg.png\" width=600>\n",
    "\n",
    "data에 noise를 조금씩 더해가거나 noise로부터 조금씩 복원해가는 과정을 통해 data를 생성하는 모델. <br>\n",
    "- $q(x_t|x_{t-1})$: 오른쪽에서 왼쪽 방향으로 noise를 점점 더해가는 forward process\n",
    "- $p_{\\theta}(x_{t-1}|x_{t})$: forward process를 반대로 추정하는 reverse process <br>\n",
    "    noise로부터 원본 이미지를 복원\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존의 문제\n",
    "\n",
    "1\\. 기존 diffusion model은 연산량이 많음. <br>\n",
    "2\\. GAN의 적대적 학습은 복잡한 multi-modal 분포를 모델링 하는데 어렵기 때문에 제한된 사용처밖에 사용할 수 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "1. 순수 Transformer 기반 접근 방식과 달리, 고차원 데이터에 대해 보다 효율적이고 세밀한 재구성을 제공.\n",
    "-> 이를 통해 메가픽셀 수준의 고해상도 이미지 생성이 가능하며, 압축 단계에서 중요한 정보만 유지해 불필요한 계산을 줄임.\n",
    "\n",
    "2. unconditional image synthesis, inpainting, stochastic super-resolution 등 다양한 작업과 데이터셋에서 경쟁력 있는 성능을 보이며, 픽셀 기반 확산 모델 대비 추론 비용을 크게 절감.\n",
    "\n",
    "3. reconstruction 및 generative ability의 섬세한 가중치 조절 불필요 <br>\n",
    "-> reconstruction과 generative ability를 동시에 학습할 필요가 없음. <br>\n",
    "-> 이에 따라 잠재 공간의 정규화가 거의 필요하지 않아 매우 정확한 재구성을 보장. <br>\n",
    "\n",
    "4. super-resolution, inpanting, semantic synthesis와 같은 작업에 모델을 컨볼루션 방식으로 적용하여 ~1024²px 크기의 이미지 생성. <br>\n",
    "\n",
    "5. cross-attention 기반의 general-purpose conditioning mechanism을 설계하여 multi modal 학습 가능 <br>\n",
    "-> 이를 통해 클래스 조건부, 텍스트-이미지, 레이아웃-이미지 모델을 학습할 수 있습니다.\n",
    "\n",
    "6. 다양한 작업에 재사용할 수 있도록 pre-trained된 latent diffusion model 및 autoencoding 모델 공개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제안 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual Image Compression\n",
    "\n",
    "Perceptual Compression: auto-encoder에서 latent space를 학습하는 것. <br>\n",
    "latent space의 분산이 크면 latent space가 가지고 있는 정보가 이질적이므로 작은 분산을 가지도록 regularization 추가.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:18px\"> Regularization </font>\n",
    "- KL-reg : 학습된 Latent에 약간의 KL-penalty를 줌\n",
    "- VQ-reg : Decoder안에 Vector Quantization을 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Diffusion Models\n",
    "\n",
    "학습된 Perceptual Compression 모델을 사용하면 Latent Space에사 미세한 디테일을 효율적으로 찾을 수 있음. <br> \n",
    "고차원인 pixel space와 대비하여 latent space를 사용하는 것은 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1\\. semantic한 정보에 더 초점을 맞출 수 있음. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2\\. 저차원에서 학습을 진행하기 때문에 계산이 더 효율적. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning Mechanisms\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwHS9O%2FbtsI22sEh72%2FTeDnLSGXM4ucBuMFE7QAJ0%2Fimg.png\" width=500>\n",
    "\n",
    "<br>\n",
    "\n",
    "U-Net backnone과 Cross-Attention 매커니즘을 활용하여 Diffusion Model을 더 유연하게 조건을 줄 수 있는 generator로 만듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebUI\n",
    "\n",
    "Stable Diffusion과 같은 text-image 생성 모델을 보다 쉽게 사용할 수 있도록 함. <br>\n",
    "WebUI를 통해 코드 작성 없이도 다양한 이미지 생성 설정을 조정할 수 있음. <br>\n",
    "Automatic1111 프로젝트가 대표적. <br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "설치 링크: https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb#scrollTo=PjzwxTkPSPHf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "모델의 ckpt나 safetensor를 받아 모델 추가 가능 <br>\n",
    "**sd/StableDiffusion/stable-diffusion-webui/models/Stable-diffusion** 아래에 모델 복사 후 사용 <br>\n",
    "모델 추가 후 새로고침. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "모델 다운 사이트: <br>\n",
    "huggingface: https://huggingface.co/models <br>\n",
    "civitai: https://civitai.com/models <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**PreSet Utils**: 이미지 생성 설정들의 프리셋을 만들어 사용 가능 <br>\n",
    "url: https://github.com/Gerschel/sd_web_ui_preset_utils\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stable Diffusion WebUI Aspect Ratio selector**: 이미지 비율 자동 설정 버튼 <br>\n",
    "url: https://github.com/alemelis/sd-webui-ar.git\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stable Diffusion Dynamic Thresholding**: CFG scale이 올라갈수록 생기는 색 변화를 제거 <br>\n",
    "url: https://github.com/mcmonkeyprojects/sd-dynamic-thresholding\n",
    "\n",
    "<br>\n",
    "\n",
    "**tagcomplete**: prompt 태그 자동 완성 <br>\n",
    "url: https://github.com/DominikDoom/a1111-sd-webui-tagcomplete\n",
    "\n",
    "<br>\n",
    "\n",
    "**webui-wd14-tagger**: 사진을 분석하여 prompt 태그 자동 생성 <br>\n",
    "url: https://github.com/picobyte/stable-diffusion-webui-wd14-tagger\n",
    "\n",
    "<br>\n",
    "\n",
    "**sd-webui-controlnet**: 포즈(뼈대)나 이미지를 제시하여 원하는 이미지 생성을 가능케 함<br>\n",
    "url: https://github.com/Mikubill/sd-webui-controlnet\n",
    "\n",
    "<br>\n",
    "\n",
    "**openpose-editor**: 컨트롤넷에 사용할 뼈대를 직접 만들수 있도록 함 <br>\n",
    "url: https://github.com/fkunn1326/openpose-editor\n",
    "\n",
    "<br>\n",
    "\n",
    "**posex**: 3D 오픈포즈 에디터 <br>\n",
    "url: https://github.com/hnmr293/posex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "<font style=\"font-size:20px\"> prompt </font>\n",
    "\n",
    "텍스트를 입력하여 원하는 이미지 생성이 되도록 입력. <br>\n",
    "아래의 두 가지로 구분. <br>\n",
    "\n",
    "positive prompt: 모델에서 이미지 생성 시 반영되길 원하는 내용. <br>\n",
    "negative prompt: 모델에서 이미지 생성 시 반영되지 않길 원하는 내용. <br>\n",
    "\n",
    "다만, 위의 내용을 100% 반영하지 않기에, 원하는 이미지를 얻기까지 많은 노력 요구. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<font style=\"font-size:20px\"> Prompt Weight </font>\n",
    "\n",
    "프롬프트에는 가중치 부여 가능 <br>\n",
    "\n",
    "| |(keyword)|((keyword))|[keyword]|[[keyword]]|\n",
    "|-|---------|-----------|---------|-----------|\n",
    "|weight|1.1|1.2|0.9|0.8|\n",
    "\n",
    "()로 한 번 묶을 때마다 0.1 증가. <br>\n",
    "[]로 한 번 묶을 때마다 0.1 감소. <br>\n",
    "0.1 ~ 1.4의 값 할당 추천. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Samplnig Method </font>\n",
    "\n",
    "sampling method: 어떤 샘플링 방식으로 이미지를 생성할 것인지 결정. <br>\n",
    "방식에 따라서 이미지에 미묘한 차이가 있고, 생성 속도도 다름. <br>\n",
    "주로 Eular a, DPM++ 2M Karras, DPM++ SDE Karras 등이 사용. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Samplnig Step </font>\n",
    "\n",
    "step이 높을수록 고품질의 이미지 생성 가능. <br>\n",
    "보통 20~30 사용. <br>\n",
    "수치가 높으면 더 많은 생성 시간 소요. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Upscaler </font>\n",
    "\n",
    "upscaling 방식 지정. <br>\n",
    "2D 이미지는 주로 R-ESRGAN 4x+ Anime6B, Latent 등 사용 <br>\n",
    "실사 이미지는 주로 R-ESRGAN 사용 <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Denoising strength </font>\n",
    "\n",
    "값이 높으면 기존 그림에서 더 많은 변화를, 낮으면 기존 그림과 유사하도록 생성. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Batch count </font>\n",
    "\n",
    "이미지를 몇 장 생성할 지 결정. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> Batch size </font>\n",
    "\n",
    "하나의배치에 들어갈 이미지 수 결정. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> CFG Scale </font>\n",
    "\n",
    "프롬프트의 영향력을 설정. <br>\n",
    "수치가 높으면 프롬프트에 적힌 대로 생성될 확률이 높아짐 <br>\n",
    "수치가 낮으면 AI의 자유도가 올라감. <br>\n",
    "너무 낮으면 이미지가 흐려질 수 있고, 너무 높으면 오히려 이미지가 정상적이 않을 수 있기 때문에 적정 수준에서 값 설정. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img2img\n",
    "\n",
    "clip 분석 이후 deepbooru를 통해 positive prompt 획득 가능. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> infaint </font>\n",
    "\n",
    "이미지를 수정하여 새로운 이미지를 만드는 방법. <br>\n",
    "수정할 부분을 칠한 후 generate 클릭. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Lora\n",
    "\n",
    "Lora는 기존 학습 모델에 새로운 이미지를 학습시킨 모델 파일. <br>\n",
    "기존 학습 모델에 본인이 적용하고 싶은 스타일을 추가로 학습시킨 파일로, <br>\n",
    "프롬프트에 적용시켜 좀 더 원하는 이미지를 생성할 수 있도록 함. <br>\n",
    "Lora는 본인이 직접 만들 수도 있고, 사이트에서 다운로드하여 이용 가능. <br>\n",
    "\n",
    "ex)\n",
    "게임을 제작 시 주인공 캐릭터의 로라를 만들어 여러 주인공 이미지 생성. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**sd/StableDiffusion/stable-diffusion-webui/models/Lora**에 추가하여 사용 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## VAE\n",
    "\n",
    "이미지를 보정해주는 역할. <br>\n",
    "만약 생성된 이미지가 흐리거나 퀄리티를 높이고 싶다면 VAE 적용. <br>\n",
    "크게 2D 애니메이션 풍과 실사 이미지 중에서 선택.\n",
    "\n",
    "<br>\n",
    "\n",
    "**sd/StableDiffusion/stable-diffusion-webui/models/VAE**에 추가하여 사용 <br>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Embedding\n",
    "\n",
    "임베딩은 이미 학습된 프롬프트를 한번에 적용해주는 파일. <br>\n",
    "positive prompt와 negative prompt에 모두 사용이 가능. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**sd/StableDiffusion/stable-diffusion-webui/embeddings**에 추가하여 사용 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
