{"cells":[{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import psycopg2\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"JM5N_uD9lmAY"},"source":["# Crawling\n","\n","스크레이핑(Scraping)이라고도 하며 웹 페이지 내의 데이터를 추출하는 것을 의미 <br>\n","데이터를 수집하기 위한 방법으로 많이 사용 <br>\n","크게 두 가지의 방법이 존재\n","1. 정적크롤링 <br>\n","정적 데이터를 수집하는 방법 <br>\n","정적 데이터란 페이지 내에 원하는 정보가 모두 들어남\n","\n","2. 동적크롤링 <br>\n","동적 데이터를 수집하는 방법 <br>\n","동적 데이터란 클릭, 로그인 등의 행위를 통해 원하는 데이터에 접근 가능 <br>\n","\n","\n","|    |정적 크롤링|동적 크롤링|\n","|----|--------|--------|\n","|방법 |주소 사용  |브라우저 사용|\n","|수집 범위|제한적  |제한 없음|\n","|속도|매우 빠름|매우 느림|\n","\n","<br>\n","\n","크롤링 시 사이트에서 크롤링을 허용하는지를 반드시 확인해야 함 <br>\n","robots.txt를 뒤에 붙여 확인 가능 <br>\n","강제는 아니나 이를 무시하면 추후 법률적 문제가 생길 수 있음\n","```\n","www.daum.net/robots.txt\n","User-agent: *\n","Disallow: /\n","```\n","*: All <br>\n","/: All Directories\n"]},{"cell_type":"markdown","metadata":{"id":"4JLYuAdAtoTc"},"source":["## HTTP\n","\n","WWW(World Wide Web, W3) 상에서 정보를 주고받을 수 있는 프로토콜 <br>\n","클라이언트와 서버 사이에 이루어지는 요청/응답 프로토콜"]},{"cell_type":"markdown","metadata":{"id":"VFv-YAqLvQ4j"},"source":["### API\n","API(Application Programming Interface) <br>\n","- Application: 고유한 기능을 가진 모든 소프트웨어\n","- Interface: 두 애플리케이션 간의 규약 <br>\n","이 계약은 요청과 응답을 사용하여 두 애플리케이션이 서로 통신하는 방법을 정의합니다."]},{"cell_type":"markdown","metadata":{"id":"8MhlEUlxuRTs"},"source":["### REST\n","REST(Representational State Transfer): 자원을 이름으로 구분하여 해당 자원의 상태를 주고받는 것 <br>\n","\n","REST 구성\n","1. 자원(Resource): HTTP URI\n","2. 자원에 대한 행위(Verb): HTTP Method\n","3. 자원에 대한 행위의 내용(Representations): HTTP Message Pay Load\n","\n","\n","- HTTP URI(Uniform Resource Identifier)를 통해 자원(Resource)을 명시\n","- HTTP Method(POST, GET, PUT, DELETE)를 사용하여 URI에 대한 CRUD Operation을 적용 <br>\n","<br>\n","\n","HTTP Methods\n","- GET: 자원 검색\n","- POST: 자원 작성\n","- PUT: 자원 업데이트\n","- DELETE: 데이터 삭제\n","- HEAD: 자원 검색 (GET과 유사하나 상태 줄과 헤더만 반환)\n","- OPTIONS: 자원이 지원하고 있는 메소드의 취득\n","- PATCH: 자원 일부 수정 (PUT과 유사하나 일부만 수정)\n","- CONNECT: 자원의 터널 접속을 변경\n","- TRACE: 리소스에 대한 경로를 따라 메시지 루프백 테스트를 수행\n","<br>\n","\n","HTTP Status\n","1. 1xx(Informational): 요청 처리중\n","2. 2xx(Successful): 요청 정상 처리 <br>\n","200: 요청 성공\n","3. 3xx(Redirection): 요청을 완료하려면 추가 행동이 필요\n","4. 4xx(Client Error): 클라이언트 오류, 잘못된 문법등으로 요청을 수행할 수 없음 <br>\n","400: Bad Request, 클라이언트의 잘못된 요청으로 서버가 요청을 처리할 수 없음 <br>\n","401: Unauthorized, 해당 리소스에 대한 인증이 필요함 <br>\n","403: Forbidden, 서버가 요청을 이해했지만 승인을 거부함 <br>\n","404: Not Found, 리소스를 찾을 수 없음 <br>\n","5. 5xx(Server Error): 서버 오류"]},{"cell_type":"markdown","metadata":{"id":"QtWV6XNfwm65"},"source":["### REST API\n","REST의 원리를 따르는 API <br>\n","※ RESTful: REST의 원리를 따르는 시스템"]},{"cell_type":"markdown","metadata":{"id":"gIr-wYY_-L_r"},"source":["## HTML\n","HTML(HyperText Markup Language)은 웹 페이지 표시를 위해 개발된 지배적인 마크업 언어 <br>\n","HTML은 웹 페이지 콘텐츠 안의 꺾쇠 괄호에 둘러싸인 \"태그\"로 되어있는 HTML 요소 형태로 작성 <br>\n","HTML은 웹 브라우저와 같은 HTML 처리 장치의 행동에 영향을 주는 자바스크립트, 본문과 그 밖의 항목의 외관과 배치를 정의하는 CSS 같은 스크립트를 포함하거나 불러올 수 있음 <br>\n","<br>\n","HTML 선택자: HTML에서는 다수의 동일한 태그가 존재하는데 각 태그를 구별할 수 있도록 선택자를 이용\n","```html\n","<div> \n","\t<div> \n","      <a> c </a> \n","      <span> c++ </span> \n","    </div> \n","    \n","    <div> \n","      <a> java </a> \n","      <span> python </span> \n","    </div> \n","</div>\n","```\n","\n","```html\n","<div id=\"contents\"> \n","\t<div class=\"data1\"> \n","      <span class=\"language\"> c++ </span> \n","      <span class=\"language\"> java </span> \n","      <span class=\"language\"> python </span> \n","  </div> \n","    \n","  <div class=\"data2\"> \n","      <a class=\"framework\"> tensorflow </a> \n","      <a class=\"framework\"> pytorch </a> \n","      <a class=\"framework\"> spring </a> \n","  </div> \n","</div>\n","```"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["<html><body><p>{\n","  \"id\": 1,\n","  \"name\": \"Leanne Graham\",\n","  \"username\": \"Bret\",\n","  \"email\": \"Sincere@april.biz\",\n","  \"address\": {\n","    \"street\": \"Kulas Light\",\n","    \"suite\": \"Apt. 556\",\n","    \"city\": \"Gwenborough\",\n","    \"zipcode\": \"92998-3874\",\n","    \"geo\": {\n","      \"lat\": \"-37.3159\",\n","      \"lng\": \"81.1496\"\n","    }\n","  },\n","  \"phone\": \"1-770-736-8031 x56442\",\n","  \"website\": \"hildegard.org\",\n","  \"company\": {\n","    \"name\": \"Romaguera-Crona\",\n","    \"catchPhrase\": \"Multi-layered client-server neural-net\",\n","    \"bs\": \"harness real-time e-markets\"\n","  }\n","}</p></body></html>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["response = requests.get(\"https://jsonplaceholder.typicode.com/users/1\")\n","bs = BeautifulSoup(response.text, 'lxml')\n","bs"]},{"cell_type":"markdown","metadata":{"id":"l5jkE-4Bltzz"},"source":["## 정적크롤링"]},{"cell_type":"markdown","metadata":{"id":"D3On4hA8-WD7"},"source":["### 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"d6joc_SRs3Lc"},"source":["#### requests\n","\n","requests: 파이썬용 http 라이브러리 <br>\n","reference: https://requests.readthedocs.io/en/latest/\n","\n","메소드별 사용법\n","```python\n","GET: requests.get()\n","POST: requests.post()\n","PUT: requests.put()\n","DELETE: requests.delete()\n","```\n","\n","```python\n","import requests\n","\n","requests.get(\"https://jsonplaceholder.typicode.com/users/1\")\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"KWYAKeA21UYG"},"source":["##### Response Body\n","\n","요청이 정상적으로 처리가 되면, response body에 요청한 데이터가 담겨져 옴. <br>\n","response body 크게 3가지 방식으로 읽을 수 있음 <br>\n","1. content: binary 원문을 읽음\n","```python\n","response.content\n","```\n","2. text: utf-8로 인코딩 된 문자열로 읽음\n","```python\n","response.text\n","```\n","3. json: 응답이 json이면 dict로 읽음\n","```python\n","response.json()\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rTdgFrTr25Mq"},"source":["##### Request\n","\n","param: 주소에 포함된 변수를 담음<br>\n","ex) https://www.naver.com/post/12345 <br>\n","-> 12345 <br>\n","query: 주소 바깥? 이후의 주소를 담음<br>\n","ex) https://www.naver.com/post/post_id=12345&id=1 <br>\n","-> 12345, 1\n","body: XML, JSON 등의 데이터를 담음, 주소에서는 확인 불가<br>\n","<br>\n","\n","requests에서는 아래와 같이 사용\n","- get\n","```python\n","response = requests.get(\"https://naver.com/post\", params={\"post_id\": \"12345\", \"id\": \"1\"})\n","```\n","\n","- post, put: HTML 데이터 전송\n","```python\n","response = requests.get(\"https://naver.com/post\", data={\"post_id\": \"12345\", \"id\": \"1\"})\n","```\n","json 형태로도 요청 가능\n","```python\n","response = requests.get(\"https://naver.com/post\", json={\"post_id\": \"12345\", \"id\": \"1\"})\n","```"]},{"cell_type":"markdown","metadata":{"id":"NHAgX_3A5zg9"},"source":["##### headers\n","\n","일부 웹 사이트는 bot agent를 차단 <br>\n","이 경우 header의 user-agent를 아래와 같이 넘기면 해결 <br>\n","```python\n","requests.get(\"https://naver.com/post\", headers={'User-Agent': 'Mozilla 5.0'})\n","```"]},{"cell_type":"markdown","metadata":{"id":"wPAvkXkMtOfV"},"source":["#### BeautifulSoup\n","\n","BeautifulSoup: html, xml 등으로부터 원하는 정보를 가지고 올 수 있도록 하는 라이브러리 <br>\n","reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n","\n","```python\n","import requests\n","from bs4 import BeautifulSoup\n","\n","response = requests.get(url)\n","bs = BeautifulSoup(response.text, 'lxml')\n","```"]},{"cell_type":"markdown","metadata":{"id":"ZFgq8Bon82_j"},"source":["###### Parser\n","\n","|parser|특징|설치|속도|사용방법|\n","|------|---|---|---|------|\n","|html.parser||기본|보통|BeautifulSoup(html_doc, 'html.parser')|\n","|lxml|xml 지원|lxml 필요|빠름|BeautifulSoup(html_doc, 'lxml')|\n","|xml|xml 지원|lxml 필요|빠름|BeautifulSoup(html_doc, 'xml')|\n","|html5lib|브라우저와 동일|html5lib 필요|느림|BeautifulSoup(html_doc, 'html5lib')|"]},{"cell_type":"markdown","metadata":{"id":"OBuY_GBRAb8t"},"source":["##### find\n","속성과 값을 이용하여 원하는 값을 찾음\n","\n","find: 매칭되는 값 중 상위 1개 반환\n","find_all: 매칭되는 전체 반환\n","\n","특정 태그 추출\n","```python\n","soup.find_all('p') # p 태그 추출\n","```\n","<br>\n","\n","특정 클래스 추출\n","```python\n","soup.find_all(class_='a') # a 클래스 추출\n","```\n","<br>\n","\n","특정 태그와 class 추출\n","```python\n","soup.find_all('p', attrs={'class': 'a'}) # p 태그와 a 클래스 모두를 갖는 값 추출\n","```\n","<br>\n","\n","특정 id 추출\n","```python\n","soup.find_all(id='b') # b id를 갖는 값 추출\n","```\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"uP7Vzn9uAcB2"},"source":["##### select\n","CSS Selector로 태그를 찾아 반환 <br>\n","CSS에서 HTML을 태깅하는 방법을 활용 <br>\n","<br>\n","select_one: 매칭되는 값 중 상위 1개 반환 <br>\n","select: 매칭되는 전체 반환 <br>\n","<br>\n","\n","특정 태그 추출\n","```python\n","soup.select('p') # p 태그 추출\n","```\n","<br>\n","\n","특정 클래스 추출\n","```python\n","soup.select('.a') # a 클래스 추출\n","```\n","<br>\n","\n","특정 태그와 class 추출\n","```python\n","soup.select('p.a') # p 태그와 a 클래스 모두를 갖는 값 추출\n","```\n","<br>\n","\n","특정 id 추출\n","```python\n","soup.select('#b') # b id를 갖는 값 추출\n","```\n","<br>\n","\n","특정 태그와 id 추출\n","```python\n","soup.select('p#b') # p 태그와 b id 모두를 갖는 값 추출\n","```\n","<br>\n","\n","특정 태그와 class, id 모두 추출\n","```python\n","soup.select('p.a#b') # p 태그와 a 클래스 b id 모두 갖는 값 추출\n","```\n","<br>\n","\n","특정 태그 아래에 있는 태그 찾기\n","```python\n","soup.select('div p') # div 아래 p태그가 있는 값 추출\n","soup.select('div > p') # div 바로 아래 p태그가 있는 값 추출\n","soup.select(\"div > #link\") # div 바로 아래 link id가 있는 값 추출\n","```\n","<br>\n","\n","형제 태그 찾기\n","```python\n","soup.select(\"#link + .sister\") # link 태그와 형제 태그 중 바로 직후 1개\n","soup.select(\"#link ~ .sister\") # link 태그와 형제 태그 중 뒤에 태그 전부\n","```\n","<br>\n","\n","여러 태그 중 i번째 태그 추출\n","```python\n","soup.select('a:nth-of-type(i)') # 추출된 a태그 중 i번째 값 반환\n","```\n","<br>\n","<br>\n","\n","정규표현식 활용 <br>\n","```python\n","soup.select('[class~=a]') # class 속성 중 a를 포함하는 태그\n","soup.select('a[href]') # a 태그 중 href 속성이 존재하는 태그\n","soup.select('a[href=\"https://www.naver.com\"]') # a 태그 중 href 속성이 https://www.naver.com과 매칭되는 태그\n","soup.select('a[href^=\"https://\"]') # a 태그 중 href 속성이 https://로 시작하는 태그\n","soup.select('a[href$=\"ac.kr\"]') # a 태그 중 href 속성이 ac.kr로 끝나는 태그\n","soup.select('a[href*=\"naver\"]') # a 태그 중 href 속성 중 naver를 가지는 태그\n","```\n","\n","<br>\n","<br>\n","\n","출력\n","```python\n","soup.strings # 값 반환\n","soup.stripped_strings # 공백을 제거한 값 반환\n","```"]},{"cell_type":"markdown","metadata":{"id":"9Mbvujsc7lcB"},"source":["### 실습"]},{"cell_type":"markdown","metadata":{"id":"H-PeuwJaMk7r"},"source":["##### 예제\n","\n","```html\n","<div id=\"contents\"> \n","    <div class=\"data1\"> \n","      <span class=\"language\"> c++ </span> \n","      <span class=\"language\"> java </span> \n","      <span class=\"language\"> python </span> \n","  </div> \n","\n","  <div class=\"data2\"> \n","      <a class=\"framework\"> tensorflow </a> \n","      <a class=\"framework\"> pytorch </a> \n","      <a class=\"framework\"> spring </a> \n","  </div> \n","</div>\n","```"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["response = '''\n","<div> \n","\t<div> \n","      <a> c </a> \n","      <span> c++ </span> \n","    </div> \n","    \n","    <div> \n","      <a> java </a> \n","      <span> python </span> \n","    </div> \n","</div>\n","'''"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["bs4 = BeautifulSoup(response, 'lxml')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["[<a> c </a>, <a> java </a>]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["languages = bs4.select('a')\n","languages"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["'c'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["languages[0].text.strip()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['c', 'java']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["[language.text.strip() for language in languages]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["<div>\n","<a> java </a>\n","<span> python </span>\n","</div>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('div')[2]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["[<a> c </a>, <a> java </a>]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('div div a')"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["[<a> c </a>, <a> java </a>]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('div > a')"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["response = '''\n","<div id=\"contents\"> \n","\t<div class=\"data1\"> \n","      <span class=\"language\"> c++ </span> \n","      <span class=\"language\"> java </span> \n","      <span class=\"language\"> python </span> \n","  </div> \n","    \n","  <div class=\"data2\"> \n","      <a class=\"framework\"> tensorflow </a> \n","      <a class=\"framework\"> pytorch </a> \n","      <a class=\"framework\"> spring </a> \n","  </div> \n","</div>\n","'''"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["bs4 = BeautifulSoup(response, 'lxml')"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["[<span class=\"language\"> c++ </span>,\n"," <span class=\"language\"> java </span>,\n"," <span class=\"language\"> python </span>]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('.language')"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["[<span class=\"language\"> c++ </span>,\n"," <span class=\"language\"> java </span>,\n"," <span class=\"language\"> python </span>]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('span.language')"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["[<a class=\"framework\"> tensorflow </a>,\n"," <a class=\"framework\"> pytorch </a>,\n"," <a class=\"framework\"> spring </a>]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('.data2')[0].select('a')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["[<div id=\"contents\">\n"," <div class=\"data1\">\n"," <span class=\"language\"> c++ </span>\n"," <span class=\"language\"> java </span>\n"," <span class=\"language\"> python </span>\n"," </div>\n"," <div class=\"data2\">\n"," <a class=\"framework\"> tensorflow </a>\n"," <a class=\"framework\"> pytorch </a>\n"," <a class=\"framework\"> spring </a>\n"," </div>\n"," </div>]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["bs4.select('div#contents')"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["['tensorflow', 'pytorch', 'spring']"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# ['tensorflow', 'pytorch', 'spring']\n","frameworks = bs4.select('a.framework')\n","frameworks = [framework.text.strip() for framework in frameworks]\n","frameworks"]},{"cell_type":"markdown","metadata":{"id":"KfuXXlLV7nSV"},"source":["#### 네이버 뉴스"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["response = requests.get('https://n.news.naver.com/mnews/article/009/0005351727')\n","if response.status_code == 200:\n","    bs = BeautifulSoup(response.text, 'lxml')"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["times = bs.select('div.media_end_head_info_datestamp_bunch > span')"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["['2024.08.18.', '오후', '6:05']"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["[time.text for time in times][0].split()"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["['2024.08.18.', '오후', '6:05']"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["times[0].text.split()"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def convert_to_datetime(date: str):\n","    date, am_pm, time = date.split()\n","\n","    hour, minute = time.split(':')\n","    hour = int(hour)\n","    \n","    if (am_pm == '오후') and (hour != 12):\n","        hour += 12\n","    time = f'{hour}:{minute}'\n","    datetime_ = f'{date}{time}'\n","\n","    return pd.to_datetime(datetime_, format='%Y.%m.%d.%H:%M')"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["posting_time, modifying_time = [convert_to_datetime(time.text) for time in times]"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["'인구 비상사태, 기업 대응 낙제점'"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["title = bs.select('h2#title_area > span')\n","title = title[0].text\n","title"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"data":{"text/plain":["'성승훈'"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["reporter = bs.select('em.media_end_head_journalist_name')\n","reporter = reporter[0].text\n","reporter = reporter.split()[0]\n","reporter"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["<article class=\"go_trans _article_content\" id=\"dic_area\">\n","<span style=\"border-left:4px solid #959595; padding-left: 20px; display: inline-block\"><strong>한미연 EPG 경영 평가, 300개 기업중 80점이상 5곳 그쳐<br/>육아 환경 여전히 팍팍 …\"출산장려 기업 인센티브 부족\"</strong></span><br/><br/>◆ 기업 인구대응 평가 ◆<br/><br/><span class=\"end_photo_org\"><div class=\"nbd_im_w _LAZY_LOADING_WRAP is_small\">\n","<div class=\"nbd_a _LAZY_LOADING_ERROR_HIDE\" id=\"img_a1\">\n","<img alt=\"\" class=\"_LAZY_LOADING _LAZY_LOADING_INIT_HIDE\" data-src=\"https://imgnews.pstatic.net/image/009/2024/08/18/0005351727_001_20240818204316565.jpg?type=w647\" id=\"img1\" style=\"display: none;\"/>\n","</div>\n","</div></span><br/><br/>제조업 분야 중견기업에서 근무 중인 30대 여성 직장인 A씨는 수년째 출산을 미루고 있다. 회사에서 자리가 사라질 것 같은 불안감 때문이다. 그는 \"주변 사례를 보면 육아휴직 후 돌아올 자리가 없을 것이라는 식으로 압박을 주는 일이 심심치 않게 있었다\"며 \"경력단절을 막기 위해 출산을 포기하든지 육아를 위해 퇴사하든지 선택지는 둘 중 하나\"라고 토로했다.<br/><br/>국내 기업들의 인구위기 대응은 '낙제점'을 면치 못한 것으로 나타났다. 인구절벽에 직면해 정부가 출산·육아 지원책을 잇달아 내놓고 있지만 현장에서 제대로 작동하지 않고 있다는 지적이다. <br/><br/>18일 매일경제와 한반도미래인구연구원은 국내 최초로 자산 규모 1조원 이상 기업 300곳을 대상으로 EPG(환경·인구·투명경영) 경영평가 점수를 발표했다. 올해 3월 매일경제와 한미연이 제34차 국민보고대회에서 EPG 경영을 제안한 데 따른 후속 조치다. EPG는 ESG에서 'S(책임·Social)'를 'P(인구·Population)'로 바꾼 용어다. <br/><br/>EPG 평가에서 300개 기업 평균 점수는 55.5점에 그쳤다. 최고점은 85.3점이었으며 최저점은 16.2점으로 나타났다. 기초 평가 17개 지표를 기준으로 삼성전기(85.3점)가 1위를 차지했으며 롯데정밀화학(83.8점)과 신한카드·KB국민카드·KT&amp;G(80.9점)가 뒤를 이었다.<br/><br/>대부분 기업은 다방면에서 취약점을 노출했다. 남성 육아휴직을 의무화한 기업은 5%인 15곳에 불과했다. 출산·육아휴직 복귀 지원 프로그램을 체계적으로 운영하는 곳도 27곳에 그쳤다.<br/><br/>산업계에서는 대부분 출산·양육 지원 제도가 사용자가 아닌 근로자에게만 초점이 맞춰진 점을 지적한다. 정만기 한국산업연합포럼 회장은 \"기업이 부담해야 하는 인력난이나 생산성 저하 등에 대한 사업주 지원책은 부족하다\"며 \"가령 출산을 장려하는 기업에 세금을 깎아주는 인적자본 투자세액공제 도입을 검토할 필요가 있다\"고 조언했다.<br/><br/>우수 기업에 대한 지원을 강화하기 위해 기업공시 항목에 이를 반영하자는 의견도 제시됐다. 포괄적으로만 공시했던 출산·육아휴직 항목을 상세히 밝히자는 것이다. 출산·육아휴직 사용률, 휴직기간, 복직자 규모, 복직기간 등을 공시할 필요가 있다는 취지다. 일본은 종업원 1000명 이상 기업에 대해 남성 육아휴직 현황을 공시하게 한 이후 남성 육아휴직 사용률이 30%대를 넘어섰다.<br/><br/>이인실 한미연 원장은 \"시대 변화에 따라 최고디지털책임자(CDO)나 최고인공지능책임자(CAIO) 같은 새 직책이 생겨나듯 기업에 최고인구책임자(CPO)를 만들고 인구위기 대응 전담 부서를 설치하는 방안도 생각할 수 있을 것\"이라며 \"근본적인 발상의 전환이 필요하다\"고 강조했다. <br/><br/>[성승훈 기자]<br/><br/><!-- r_start //--><!-- r_end //-->\n","</article>"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["contents = bs.select('article#dic_area')\n","contents[0]"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["press_id = '009'\n","article_id = 'https://n.news.naver.com/mnews/article/009/0005351727'.split('/')[-1]"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'str' object has no attribute 'text'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[149], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m psycopg2\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m      2\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     dbname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5432\u001b[39m,\n\u001b[0;32m      7\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[1;32m----> 9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mINSERT INTO article VALUES (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpress_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposting_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodifying_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreporter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m         cur\u001b[38;5;241m.\u001b[39mexecute()\n","\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"]}],"source":["with psycopg2.connect(\n","    host='localhost',\n","    dbname='postgres',\n","    user='postgres',\n","    password='1234',\n","    port=5432,\n",") as conn:\n","    with conn.cursor() as cur:\n","        f\"\"\"INSERT INTO article VALUES ('{press_id}', '{article_id}', '{posting_time}', '{modifying_time}', '{reporter}', '{title}', '{contents[0].text.replace('\\'', '').replace('\\\"', '')}')\"\"\"\n","        cur.execute()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 한 페이지 crawling"]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 24/24 [00:03<00:00,  6.84it/s]\n"]}],"source":["response = requests.get('https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid=009&date=20240819&page=1')\n","bs = BeautifulSoup(response.text, 'lxml')\n","\n","url_with_problem = []\n","press_id = '009'\n","for url in tqdm(list(set([tag.attrs.get('href') for tag in bs.select('li dl a')]))):\n","    article_id = url.split('/')[-1]\n","\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        bs = BeautifulSoup(response.text, 'lxml')\n","    else:\n","        url_with_problem.append(url)\n","        continue\n","\n","    times = bs.select('div.media_end_head_info_datestamp_bunch > span')\n","    if times and len(times) == 2:\n","        posting_time, modifying_time = [convert_to_datetime(time.text) for time in times]\n","    elif times and len(times) == 1:\n","        posting_time = [convert_to_datetime(time.text) for time in times][0]\n","        modifying_time = posting_time\n","    else:\n","        url_with_problem.append(url)\n","        continue\n","\n","    title = bs.select('h2#title_area > span')\n","    if title:\n","        title = title[0].text\n","    elif not title:\n","        title = bs.select('h2.NewsEndMain_article_title__kqEzS')\n","        title = title[0].text\n","    else:\n","        url_with_problem.append(url)\n","        continue\n","\n","    reporter = bs.select('em.media_end_head_journalist_name')\n","    if reporter:\n","        reporter = reporter[0].text\n","        reporter = reporter.split()[0]\n","    else:\n","        reporter = ''\n","    \n","    contents = bs.select('article#dic_area')\n","    if contents:\n","        contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","    elif not contents:\n","        contents = bs.select('article#comp_news_article')\n","        contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","    else:\n","        url_with_problem.append(url)\n","        continue\n","\n","    with psycopg2.connect(\n","        host='localhost',\n","        dbname='postgres',\n","        user='postgres',\n","        password='1234',\n","        port=5432,\n","    ) as conn:\n","        with conn.cursor() as cur:\n","            try:                \n","                cur.execute(f\"\"\"INSERT INTO article VALUES ('{press_id}', '{article_id}', '{posting_time}', '{modifying_time}', '{reporter}', '{title}', '{contents}')\"\"\")\n","            except Exception:\n","                url_with_problem.append(url)"]},{"cell_type":"markdown","metadata":{},"source":["## 전체 페이지 crawling"]},{"cell_type":"code","execution_count":237,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 24/24 [00:03<00:00,  6.69it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.92it/s]\n","100%|██████████| 20/20 [00:03<00:00,  6.61it/s]\n","100%|██████████| 20/20 [00:03<00:00,  6.04it/s]\n","100%|██████████| 20/20 [00:05<00:00,  3.58it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.92it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.47it/s]\n","100%|██████████| 20/20 [00:03<00:00,  6.03it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.27it/s]\n","100%|██████████| 20/20 [00:02<00:00,  6.70it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.85it/s]\n","100%|██████████| 20/20 [00:05<00:00,  3.61it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.32it/s]\n","100%|██████████| 20/20 [00:04<00:00,  4.99it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.43it/s]\n","100%|██████████| 20/20 [00:03<00:00,  5.37it/s]\n","100%|██████████| 6/6 [00:01<00:00,  5.55it/s]t]\n","100%|██████████| 17/17 [01:03<00:00,  3.73s/it]\n"]}],"source":["url = 'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid=009&date=20240819&page=100000'\n","response = requests.get(url)\n","bs = BeautifulSoup(response.text, 'lxml')\n","\n","last_page_num = bs.select('div.paging > strong')\n","last_page_num = int(last_page_num[0].text)\n","\n","url_with_problem = []\n","press_id = '009'\n","for page in tqdm(range(1, last_page_num+1)):\n","    response = requests.get(f'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid=009&date=20240819&page={page}')\n","    bs = BeautifulSoup(response.text, 'lxml')\n","\n","    for url in list(set([tag.attrs.get('href') for tag in bs.select('li dl a')])):\n","        article_id = url.split('/')[-1]\n","\n","        response = requests.get(url)\n","        if response.status_code == 200:\n","            bs = BeautifulSoup(response.text, 'lxml')\n","        else:\n","            url_with_problem.append(url)\n","            continue\n","\n","        times = bs.select('div.media_end_head_info_datestamp_bunch > span')\n","        if times and len(times) == 2:\n","            posting_time, modifying_time = [convert_to_datetime(time.text) for time in times]\n","        elif times and len(times) == 1:\n","            posting_time = [convert_to_datetime(time.text) for time in times][0]\n","            modifying_time = posting_time\n","        else:\n","            url_with_problem.append(url)\n","            continue\n","\n","        title = bs.select('h2#title_area > span')\n","        if title:\n","            title = title[0].text\n","        elif not title:\n","            title = bs.select('h2.NewsEndMain_article_title__kqEzS')\n","            title = title[0].text\n","        else:\n","            url_with_problem.append(url)\n","            continue\n","\n","        reporter = bs.select('em.media_end_head_journalist_name')\n","        if reporter:\n","            reporter = reporter[0].text\n","            reporter = reporter.split()[0]\n","        else:\n","            reporter = ''\n","        \n","        contents = bs.select('article#dic_area')\n","        if contents:\n","            contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","        elif not contents:\n","            contents = bs.select('article#comp_news_article')\n","            contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","        else:\n","            url_with_problem.append(url)\n","            continue\n","\n","        with psycopg2.connect(\n","            host='localhost',\n","            dbname='postgres',\n","            user='postgres',\n","            password='1234',\n","            port=5432,\n","        ) as conn:\n","            with conn.cursor() as cur:\n","                try:                \n","                    cur.execute(f\"\"\"INSERT INTO article VALUES ('{press_id}', '{article_id}', '{posting_time}', '{modifying_time}', '{reporter}', '{title}', '{contents}')\"\"\")\n","                except Exception:\n","                    url_with_problem.append(url)"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[],"source":["## 여러 날짜의 데이터 crawling"]},{"cell_type":"code","execution_count":243,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [03:03<00:00, 61.00s/it]\n"]}],"source":["url_with_problem = []\n","press_id = '009'\n","for date in tqdm(pd.date_range('2024-08-17', '2024-08-19')):\n","    date = date.strftime('%Y%m%d')\n","\n","    url = f'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid=009&date={date}&page=100000'\n","    response = requests.get(url)\n","    bs = BeautifulSoup(response.text, 'lxml')\n","\n","    last_page_num = bs.select('div.paging > strong')\n","    last_page_num = int(last_page_num[0].text)\n","\n","    for page in range(1, last_page_num+1):\n","        response = requests.get(f'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid=009&date={date}&page={page}')\n","        bs = BeautifulSoup(response.text, 'lxml')\n","\n","        for url in list(set([tag.attrs.get('href') for tag in bs.select('li dl a')])):\n","            article_id = url.split('/')[-1]\n","\n","            response = requests.get(url)\n","            if response.status_code == 200:\n","                bs = BeautifulSoup(response.text, 'lxml')\n","            else:\n","                url_with_problem.append(url)\n","                continue\n","\n","            times = bs.select('div.media_end_head_info_datestamp_bunch > span')\n","            if times and len(times) == 2:\n","                posting_time, modifying_time = [convert_to_datetime(time.text) for time in times]\n","            elif times and len(times) == 1:\n","                posting_time = [convert_to_datetime(time.text) for time in times][0]\n","                modifying_time = posting_time\n","            else:\n","                url_with_problem.append(url)\n","                continue\n","\n","            title = bs.select('h2#title_area > span')\n","            if title:\n","                title = title[0].text\n","            elif not title:\n","                title = bs.select('h2.NewsEndMain_article_title__kqEzS')\n","                title = title[0].text\n","            else:\n","                url_with_problem.append(url)\n","                continue\n","\n","            reporter = bs.select('em.media_end_head_journalist_name')\n","            if reporter:\n","                reporter = reporter[0].text\n","                reporter = reporter.split()[0]\n","            else:\n","                reporter = ''\n","            \n","            contents = bs.select('article#dic_area')\n","            if contents:\n","                contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","            elif not contents:\n","                contents = bs.select('article#comp_news_article')\n","                contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","            else:\n","                url_with_problem.append(url)\n","                continue\n","\n","            with psycopg2.connect(\n","                host='localhost',\n","                dbname='postgres',\n","                user='postgres',\n","                password='1234',\n","                port=5432,\n","            ) as conn:\n","                with conn.cursor() as cur:\n","                    try:                \n","                        cur.execute(f\"\"\"INSERT INTO article VALUES ('{press_id}', '{article_id}', '{posting_time}', '{modifying_time}', '{reporter}', '{title}', '{contents}')\"\"\")\n","                    except Exception:\n","                        url_with_problem.append(url)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 함수화"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[],"source":["def crawler(press_id: str, start_date: str, end_date: str, **kwargs) -> list:\n","    url_with_problem = []\n","    for date in tqdm(pd.date_range(start_date, end_date)):\n","        date = date.strftime('%Y%m%d')\n","\n","        url = f'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid={press_id}&date={date}&page=100000'\n","        response = requests.get(url)\n","        bs = BeautifulSoup(response.text, 'lxml')\n","\n","        last_page_num = bs.select('div.paging > strong')\n","        last_page_num = int(last_page_num[0].text)\n","\n","        for page in range(1, last_page_num+1):\n","            response = requests.get(f'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid={press_id}&date={date}&page={page}')\n","            bs = BeautifulSoup(response.text, 'lxml')\n","\n","            for url in list(set([tag.attrs.get('href') for tag in bs.select('li dl a')])):\n","                article_id = url.split('/')[-1]\n","\n","                response = requests.get(url)\n","                if response.status_code == 200:\n","                    bs = BeautifulSoup(response.text, 'lxml')\n","                else:\n","                    url_with_problem.append(url)\n","                    continue\n","\n","                times = bs.select('div.media_end_head_info_datestamp_bunch > span')\n","                if times and len(times) == 2:\n","                    posting_time, modifying_time = [convert_to_datetime(time.text) for time in times]\n","                elif times and len(times) == 1:\n","                    posting_time = [convert_to_datetime(time.text) for time in times][0]\n","                    modifying_time = posting_time\n","                else:\n","                    url_with_problem.append(url)\n","                    continue\n","\n","                title = bs.select('h2#title_area > span')\n","                if title:\n","                    title = title[0].text\n","                elif not title:\n","                    title = bs.select('h2.NewsEndMain_article_title__kqEzS')\n","                    title = title[0].text\n","                else:\n","                    url_with_problem.append(url)\n","                    continue\n","\n","                reporter = bs.select('em.media_end_head_journalist_name')\n","                if reporter:\n","                    reporter = reporter[0].text\n","                    reporter = reporter.split()[0]\n","                else:\n","                    reporter = ''\n","                \n","                contents = bs.select('article#dic_area')\n","                if contents:\n","                    contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","                elif not contents:\n","                    contents = bs.select('article#comp_news_article')\n","                    contents = contents[0].text.replace('\\'', '').replace('\\\"', '')\n","                else:\n","                    url_with_problem.append(url)\n","                    continue\n","\n","                with psycopg2.connect(\n","                    host=kwargs.get('host'),\n","                    dbname=kwargs.get('dbname'),\n","                    user=kwargs.get('user'),\n","                    password=kwargs.get('password'),\n","                    port=kwargs.get('port'),\n","                ) as conn:\n","                    with conn.cursor() as cur:\n","                        try:                \n","                            cur.execute(f\"\"\"INSERT INTO article VALUES ('{press_id}', '{article_id}', '{posting_time}', '{modifying_time}', '{reporter}', '{title}', '{contents}')\"\"\")\n","                        except Exception:\n","                            url_with_problem.append(url)\n","    \n","    return url_with_problem"]},{"cell_type":"code","execution_count":246,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [01:03<00:00, 31.58s/it]\n"]}],"source":["url_with_problem = crawler(\n","    '025', '2024-08-18', '2024-08-19',\n","    host='localhost', dbname='postgres', user='postgres', password='1234', port=5432,\n",")"]},{"cell_type":"markdown","metadata":{"id":"P4-hafLg7ogb"},"source":["#### 네이버 증권"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["requests.get(url, headers={'user-agent': 'Mozilla 5.0'})"]},{"cell_type":"markdown","metadata":{"id":"RtbLMtDR7qoS"},"source":["#### 다음 증권"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMh1vh/aoG6HuCRxj/hyhdn","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
