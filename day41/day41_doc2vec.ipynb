{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from kiwipiepy.utils import Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "\n",
    "paper: [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제상황\n",
    "\n",
    "기존의 방법들, 특히 bag-of-words와 bag-of-n-grams 모델은 아래의 문제를 가지고 있음 <br>\n",
    "\n",
    "1. 단어 순서 손실: bag-of-words는 단어의 순서를 고려하지 않아, 서로 다른 문장이 동일한 벡터 표현을 가질 수 있음 -> 의미의 구분을 어렵게 만듦\n",
    "\n",
    "2. 어휘의 의미적 거리 부족: 기존 방법들은 단어들 간의 의미적 유사성을 반영하지 않기 때문에, 의미적으로 가까운 단어와 먼 단어가 동일하게 취급"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "\n",
    "variable-length text(문장, 단락, 문서 등 다양한 길이의 텍스트)에 대해 고유한 벡터 표현을 생성할 수 있어, 기존의 bag-of-words나 bag-of-n-grams 방식보다 더 유연한 **Paragraph Vector** 제안\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec이란?\n",
    "\n",
    "<img src=\"https://i.sstatic.net/t7slV.png\" width=\"700\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Learning Vector Representation of Words\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcUAewV%2FbtqEp09vwMU%2Fw8XEQM8G0kDwcLyEjs6pSk%2Fimg.png)\n",
    "\n",
    "모든 단어는 고유한 벡터로 매핑되며, 이 벡터들은 예측을 위해 합쳐지거나 연결. <br>\n",
    "예측은 softmax를 사용하여 수행되며, 효율적인 학습을 위해 계층적 softmax가 선호 <br>\n",
    "-> 비슷한 의미의 단어들은 벡터 공간에서 가까운 위치에 매핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PV-DM (the Distributed Memory Model of Paragraph Vectors)\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F30I47%2FbtqEq62upsp%2FuKtE1W4eSoJ6jBs41TTRfk%2Fimg.png)\n",
    "\n",
    "1의 방법에서 paragraph id가 추가된 방법. <br>\n",
    "각 document에 고유한 벡터 매핑. <br>\n",
    "sliding window를 통해 현재 단어 예측에 사용할 전후의 단어 개수 설정. <br>\n",
    "document vector와 선택한 단어 벡터와의 concat 또는 average 후 다음 단어를 예측. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; document vector는 해당 document의 모든 문맥에서 공유되지만, 각 문단마다 고유. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 이 과정에서 **단어의 순서**를 고려 (ex) [$\\text{word}_{t-1}$, $\\text{word}_{t+1}$, document_vector]와 [$\\text{word}_{t-1}$, $\\text{word}_{t+1}$, document_vector]가 layer를 통과했을 때 학습되는 결과가 달라짐 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; concat 방식을 사용하면, 문맥 내 단어의 순서가 명확하게 나타나므로, 모델이 더 풍부한 의미 정보를 학습할 수 있음 <br>\n",
    "모델 학습 완료된 후 document vector를 feature로 사용하여 기존의 기계 학습 기법에 적용 가능. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.PV-DBOW (the Distributed Bag of Words version of Paragraph Vector)\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFnx9w%2FbtqEsDLQw44%2FIW4vfgoc9dad5vozNhxAG1%2Fimg.png)\n",
    "\n",
    "word2vec의 skip-gram과 유사한 방법론. <br>\n",
    "document vector를 사용하여 랜덤으로 선택된 단어를 예측하는 방식으로 학습. <br>\n",
    "이 과정에서 단어 순서는 고려되지 않지만, 단락의 의미를 잘 반영하는 벡터 생성. <br>\n",
    "**메모리 효율성을 제공하는 장점**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 방법\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> model = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.025, workers=8, window=8)\n",
    "> \n",
    "> model.build_vocab(tagged_corpus_list) # vocabulary build\n",
    "> \n",
    "> model.train(tagged_corpus_list, total_examples=model.corpus_count, epochs=20) # train\n",
    "> \n",
    "> model.dv.most_similar('[document_name]')  # 유사 문서 검색\n",
    ">\n",
    "> model.save('dart.doc2vec')    # save\n",
    "> model = Doc2Vec.load('/tmp/my_model.doc2vec') # load\n",
    "> ```\n",
    "\n",
    "<br>\n",
    "\n",
    "parameters\n",
    "- vector_size: vector dimension (몇 차원 벡터로 학습시킬 것인가)\n",
    "- alpha: learning rate\n",
    "- min_alpha: 최저 learning rate\n",
    "- window: window size\n",
    "- min_count: 학습 시 학습할 단어가 최소 몇 개 등장해야 학습할 것인가\n",
    "- workers: 학습에 사용할 cpu 수\n",
    "- dm\n",
    "    - 0: PV-DBOW\n",
    "    - 1: PV-DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 13747.62it/s]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "file_paths = glob.glob('./data/news_data/**/*.txt', recursive=True)\n",
    "for file_path in tqdm(file_paths):\n",
    "    with open(file_path) as file:\n",
    "        temp = file.read()\n",
    "        news.append(temp)\n",
    "\n",
    "\n",
    "def extract_tokens(string: str, tokenizer: Kiwi, stopwords: Stopwords, tags={'NNP', 'NNG'}):\n",
    "    # 주어진 문자열(string)을 입력으로 받아, 지정된 품사 태그와 길이 조건을 만족하는 토큰들을 추출하는 함수\n",
    "\n",
    "    # Kiwi 객체를 사용하여 문자열을 토크나이즈(tokenize)하고, 불용어(stopwords)를 적용\n",
    "    tokens = tokenizer.tokenize(string, stopwords=stopwords)\n",
    "    \n",
    "    # 토크나이즈된 토큰 중에서, 지정된 태그 집합(tags)에 포함되며 길이가 2 이상인 토큰들의 형태소를 추출하여 리스트에 저장\n",
    "    target_tokens = [token.form for token in tokens if token.tag in tags and len(token.form) >= 2]\n",
    "\n",
    "    # 조건을 만족하는 토큰들의 리스트를 반환\n",
    "    return target_tokens\n",
    "\n",
    "\n",
    "kiwi = Kiwi()\n",
    "stopwords = Stopwords()\n",
    "\n",
    "news = pd.DataFrame(news, columns=['contents'])\n",
    "news = news.drop_duplicates()\n",
    "news.contents = news.contents.str.replace('\\s{1,}', ' ', regex=True)\n",
    "news['tokens'] = news.contents.apply(lambda x: extract_tokens(x, kiwi, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = []\n",
    "for i, token_list in enumerate(news.tokens):\n",
    "    tagged_documents.append(\n",
    "        TaggedDocument(tags=[i], words=token_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    vector_size=128,\n",
    "    alpha=1e-3,\n",
    "    min_alpha=1e-4,\n",
    "    workers=8,\n",
    "    window=5,\n",
    "    dm=0,\n",
    "    )\n",
    "model.build_vocab(tagged_documents)\n",
    "model.train(\n",
    "    tagged_documents,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(838, 0.3954414427280426),\n",
       " (1416, 0.3784639835357666),\n",
       " (947, 0.3239007890224457),\n",
       " (1330, 0.322730153799057),\n",
       " (653, 0.3110679090023041),\n",
       " (1549, 0.31089287996292114),\n",
       " (793, 0.30750396847724915),\n",
       " (869, 0.3064292371273041),\n",
       " (55, 0.2939872741699219),\n",
       " (1552, 0.2923944592475891)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv.most_similar(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>징검다리 연휴 나들이…전국 고속道 심한 정체 서울~부산 5시간30분 오후 9~10시...</td>\n",
       "      <td>[징검다리, 연휴, 나들이, 전국, 고속, 정체, 서울, 부산, 오후, 해소, 서울...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>靑, NSC 개최…\"북·미 사이에 입장 차이…중재자 역할할 것\"(종합) [아시아경제...</td>\n",
       "      <td>[개최, 사이, 입장, 차이, 중재자, 역할, 종합, 아시아, 경제, 진영, 기자,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              contents  \\\n",
       "15   징검다리 연휴 나들이…전국 고속道 심한 정체 서울~부산 5시간30분 오후 9~10시...   \n",
       "854  靑, NSC 개최…\"북·미 사이에 입장 차이…중재자 역할할 것\"(종합) [아시아경제...   \n",
       "\n",
       "                                                tokens  \n",
       "15   [징검다리, 연휴, 나들이, 전국, 고속, 정체, 서울, 부산, 오후, 해소, 서울...  \n",
       "854  [개최, 사이, 입장, 차이, 중재자, 역할, 종합, 아시아, 경제, 진영, 기자,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.iloc[[15, 838]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사업의 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/crawl_data.pickle')\n",
    "data = data.query('dates == \"2022.03\"')\n",
    "\n",
    "data['business_info'] = data['사업의 개요'].apply(lambda x: BeautifulSoup(x, 'lxml').text)\n",
    "data.business_info = data.business_info.str.replace('\\s+', ' ', regex=True)\n",
    "data.business_info = data.business_info.str.replace('\\d?\\.? ? 사업의 개요 ?', '', regex=True)\n",
    "\n",
    "def extract_tokens(string: str, tokenizer: Kiwi, stopwords: Stopwords, tags={'NNP', 'NNG'}):\n",
    "    # 주어진 문자열(string)을 입력으로 받아, 지정된 품사 태그와 길이 조건을 만족하는 토큰들을 추출하는 함수\n",
    "\n",
    "    # Kiwi 객체를 사용하여 문자열을 토크나이즈(tokenize)하고, 불용어(stopwords)를 적용\n",
    "    tokens = tokenizer.tokenize(string, stopwords=stopwords)\n",
    "    \n",
    "    # 토크나이즈된 토큰 중에서, 지정된 태그 집합(tags)에 포함되며 길이가 2 이상인 토큰들의 형태소를 추출하여 리스트에 저장\n",
    "    target_tokens = [token.form for token in tokens if token.tag in tags and len(token.form) >= 2]\n",
    "\n",
    "    # 조건을 만족하는 토큰들의 리스트를 반환\n",
    "    return target_tokens\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word('바이오시밀러', 'NNG')\n",
    "kiwi.add_user_word('웅진씽크빅', 'NNP')\n",
    "\n",
    "stopwords = Stopwords()\n",
    "stopwords.add(('당사', 'NNG'))\n",
    "stopwords.add(('산업', 'NNG'))\n",
    "stopwords.add(('특성', 'NNG'))\n",
    "\n",
    "data['tokens'] = data.business_info.apply(lambda x: extract_tokens(x, kiwi, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_document = []\n",
    "for row in data.itertuples():\n",
    "    tagged_documents.append(TaggedDocument(tags=[row.corp_name], words=row.tokens))\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size=128,\n",
    "    alpha=1e-2,\n",
    "    min_alpha=1e-3,\n",
    "    workers=8,\n",
    "    window=8,\n",
    "    dm=0,\n",
    "    )\n",
    "model.build_vocab(tagged_documents)\n",
    "model.train(\n",
    "    tagged_documents,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('금호타이어', 0.7989670634269714),\n",
       " ('휴니드테크놀러지스', 0.7830410599708557),\n",
       " ('삼성전기', 0.7797526121139526),\n",
       " ('삼영전자공업', 0.7634993195533752),\n",
       " ('SK하이닉스', 0.762822687625885),\n",
       " ('화천기공', 0.7555910348892212),\n",
       " ('KPX케미칼', 0.7461060881614685),\n",
       " ('LX세미콘', 0.7452780604362488),\n",
       " ('솔루엠', 0.7375050783157349),\n",
       " ('삼익악기', 0.7337893843650818)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv.most_similar('삼성전자')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특허"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "file_paths = glob.glob(r'.\\data\\patent\\A_농업_임업및어업_01_03\\01_농업\\**\\*.json')\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path) as file:\n",
    "        datum = json.load(file).get('dataset')\n",
    "    data.extend(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1: 곡물및기타식량작물재배업\n",
    "## tag: agriculture1, agriculture2, ...\n",
    "# task2: 농업 전체\n",
    "\n",
    "claims = [datum.get('claims') for datum in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()\n",
    "stopwords = Stopwords()\n",
    "\n",
    "\n",
    "def extract_tokens(string: str, tokenizer: Kiwi, stopwords: Stopwords, tags={'NNP', 'NNG'}):\n",
    "    # string이 빈 값이면 early return\n",
    "    if not string:\n",
    "        return []\n",
    "\n",
    "    # 주어진 문자열(string)을 입력으로 받아, 지정된 품사 태그와 길이 조건을 만족하는 토큰들을 추출하는 함수\n",
    "\n",
    "    # Kiwi 객체를 사용하여 문자열을 토크나이즈(tokenize)하고, 불용어(stopwords)를 적용\n",
    "    tokens = tokenizer.tokenize(string, stopwords=stopwords)\n",
    "    \n",
    "    # 토크나이즈된 토큰 중에서, 지정된 태그 집합(tags)에 포함되며 길이가 2 이상인 토큰들의 형태소를 추출하여 리스트에 저장\n",
    "    target_tokens = [token.form for token in tokens if token.tag in tags and len(token.form) >= 2]\n",
    "\n",
    "    # 조건을 만족하는 토큰들의 리스트를 반환\n",
    "    return target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.Series(claims).apply(lambda x: extract_tokens(x, kiwi, stopwords))\n",
    "\n",
    "tagged_documents = []\n",
    "for i, token in enumerate(tokens):\n",
    "    tagged_documents.append(TaggedDocument(tags=[f'agriculture{i+1}'], words=token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    vector_size=128,\n",
    "    alpha=1e-3,\n",
    "    min_alpha=1e-3,\n",
    "    workers=8,\n",
    "    window=5,\n",
    "    dm_concat=1,\n",
    "    dm=0,\n",
    "    )\n",
    "model.build_vocab(tagged_documents)\n",
    "model.train(\n",
    "    tagged_documents,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('agriculture766', 0.625105082988739),\n",
       " ('agriculture219', 0.6058903336524963),\n",
       " ('agriculture580', 0.6004685163497925),\n",
       " ('agriculture237', 0.592998206615448),\n",
       " ('agriculture716', 0.5876607298851013),\n",
       " ('agriculture494', 0.587647557258606),\n",
       " ('agriculture496', 0.5862235426902771),\n",
       " ('agriculture492', 0.5852714776992798),\n",
       " ('agriculture498', 0.5843524932861328),\n",
       " ('agriculture300', 0.583195686340332)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv.most_similar('agriculture1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
