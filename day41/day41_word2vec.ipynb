{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "from kiwipiepy.utils import Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "paper: [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781) <br>\n",
    "reference: [Word2Vec](https://wikidocs.net/22660) <br>\n",
    "site: https://word2vec.kr/ <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 사용 방법 </font> <p>\n",
    "\n",
    "> ```python\n",
    "> model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=5, workers=8, sg=0)   # train\n",
    "> model.wv.most_similar('[word]')   # 입력 단어와 가장 유사한 단어 출력\n",
    ">\n",
    "> model.wv.save_word2vec_format('[filename]')   # save\n",
    "> model = KeyedVectors.load_word2vec_format('[filename]')   # load\n",
    "> ```\n",
    "\n",
    "<br>\n",
    "\n",
    "parameters\n",
    "- vector_size: vector dimension (몇 차원 벡터로 학습시킬 것인가)\n",
    "- window: window size\n",
    "- min_count: 학습 시 학습할 단어가 최소 몇 개 등장해야 학습할 것인가\n",
    "- workers: 학습에 사용할 cpu 수\n",
    "- sg\n",
    "    - 0: CBOW\n",
    "    - 1: Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.output = nn.Linear(self.embedding_dim, self.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.sum(axis=1)\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.zeros((5, 5), dtype=torch.long)\n",
    "# for i in range(len(x)):\n",
    "#     x[i, i] = 1\n",
    "# input_ = torch.cat([x[:2], x[3:]])\n",
    "input_ = torch.Tensor([[0, 1, 3, 4]]).long()\n",
    "word2vec = CBOWWord2Vec(5, 16)\n",
    "logit = word2vec(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5080, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logit, torch.Tensor([2000]).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Projector\n",
    "\n",
    "paper: [Embedding Projector: Interactive Visualization and\n",
    "Interpretation of Embeddings](https://arxiv.org/pdf/1611.05469v1.pdf) <br>\n",
    "site: https://projector.tensorflow.org/ <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 사용 방법 </font> <p>\n",
    "\n",
    "> ```cmd\n",
    "> !python -m gensim.scripts.word2vec2tensor --input [model_name] --output [output_model_name]\n",
    "> ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## english news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/abcnews-date-text.csv')\n",
    "\n",
    "# 각 기사 제목(headline_text)에 대해 단어를 토큰화하여 'tokens' 컬럼에 저장\n",
    "data['tokens'] = data.headline_text.apply(lambda x: nltk.wordpunct_tokenize(x))\n",
    "\n",
    "# NLTK의 영어 불용어(stop words) 리스트를 로드\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 토큰 중 불용어를 제거하여 다시 'tokens' 컬럼에 저장\n",
    "data.tokens = data.tokens.apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# 토큰들을 명사 형태로 표제어 추출(lemmatization)한 후, 다시 'tokens' 컬럼에 저장\n",
    "data.tokens = data.tokens.apply(lambda tokens: [\n",
    "    WordNetLemmatizer().lemmatize(token) for token in tokens if token not in stop_words\n",
    "])\n",
    "\n",
    "# 토큰들을 동사 형태로 표제어 추출(lemmatization)한 후, 다시 'tokens' 컬럼에 저장\n",
    "data.tokens = data.tokens.apply(lambda tokens: [\n",
    "    WordNetLemmatizer().lemmatize(token, pos='v') for token in tokens if token not in stop_words\n",
    "])\n",
    "\n",
    "# 길이가 3자 이상인 토큰만 남기고 다시 'tokens' 컬럼에 저장\n",
    "data.tokens = data.tokens.apply(lambda tokens: [token for token in tokens if len(token) >= 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=data.tokens.tolist(),\n",
    "    vector_size=128,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('./word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.8843256831169128),\n",
       " ('obama', 0.7616173028945923),\n",
       " ('clinton', 0.7524031400680542),\n",
       " ('obamas', 0.6814157366752625),\n",
       " ('romney', 0.666195809841156),\n",
       " ('merkel', 0.6616398096084595),\n",
       " ('republican', 0.6558955907821655),\n",
       " ('barack', 0.6552227735519409),\n",
       " ('melania', 0.6508939266204834),\n",
       " ('gop', 0.6417585611343384)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trump와 유사한 단어 탐색\n",
    "\n",
    "model.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'pneumonoultramicroscopicilisocovlcanocniosis' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpneumonoultramicroscopicilisocovlcanocniosis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'pneumonoultramicroscopicilisocovlcanocniosis' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 학습되지 않은 token은 vocab에 없어 key error 발생\n",
    "\n",
    "model.wv.most_similar('pneumonoultramicroscopicilisocovlcanocniosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기자회견"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_pickle('./data/comments_minheejin.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()\n",
    "# 단어 사전에 단어 추가\n",
    "kiwi.add_user_word('어도어', 'NNP')\n",
    "kiwi.add_user_word('빌리프랩', 'NNP')\n",
    "kiwi.add_user_word('빌리프렙', 'NNP')\n",
    "kiwi.add_user_word('아일리스', 'NNP')\n",
    "kiwi.add_user_word('르세라핌', 'NNP')\n",
    "kiwi.add_user_word('피프티피프티', 'NNP')\n",
    "kiwi.add_user_word('뉴진스', 'NNP')\n",
    "kiwi.add_user_word('아일릿', 'NNP')\n",
    "kiwi.add_user_word('하이브', 'NNP')\n",
    "kiwi.add_user_word('방시혁', 'NNP')\n",
    "kiwi.add_user_word('힛뱅맨', 'NNP')\n",
    "kiwi.add_user_word('힛맨뱅', 'NNP')\n",
    "kiwi.add_user_word('민희진', 'NNP')\n",
    "kiwi.add_user_word('미니진', 'NNP')\n",
    "kiwi.add_user_word('희진', 'NNP')\n",
    "kiwi.add_user_word('진짜사나이이', 'NNP')\n",
    "kiwi.add_user_word('레퍼런스', 'NNG')\n",
    "kiwi.add_user_word('언플', 'NNG')\n",
    "kiwi.add_user_word('대퓨', 'NNG')\n",
    "kiwi.add_user_word('대퓨님', 'NNG')\n",
    "kiwi.add_user_word('개저씨', 'NNG')\n",
    "kiwi.add_user_word('댓글부대', 'NNG')\n",
    "\n",
    "# 불용어 사전에 불용어 추가\n",
    "stopwords = Stopwords()\n",
    "stopwords.add(('웩퉥', 'NNG'))\n",
    "stopwords.add(('결국', 'NNG'))\n",
    "\n",
    "def extract_tokens(string: str, tokenizer: Kiwi, stopwords: Stopwords, tags={'NNP', 'NNG'}):\n",
    "    # 주어진 문자열(string)을 입력으로 받아, 지정된 품사 태그와 길이 조건을 만족하는 토큰들을 추출하는 함수\n",
    "\n",
    "    # Kiwi 객체를 사용하여 문자열을 토크나이즈(tokenize)하고, 불용어(stopwords)를 적용\n",
    "    tokens = tokenizer.tokenize(string, stopwords=stopwords)\n",
    "    \n",
    "    # 토크나이즈된 토큰 중에서, 지정된 태그 집합(tags)에 포함되며 길이가 2 이상인 토큰들의 형태소를 추출하여 리스트에 저장\n",
    "    target_tokens = [token.form for token in tokens if token.tag in tags and len(token.form) >= 2]\n",
    "\n",
    "    # 조건을 만족하는 토큰들의 리스트를 반환\n",
    "    return target_tokens\n",
    "\n",
    "# comments 데이터프레임의 'textOriginal' 열에 있는 텍스트에 대해\n",
    "# extract_tokens 함수를 적용하여 추출된 토큰들을 'tokens' 열에 저장\n",
    "comments['tokens'] = comments.textOriginal.apply(lambda x: extract_tokens(x, kiwi, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=comments.tokens.tolist(),\n",
    "    vector_size=16,\n",
    "    window=1,\n",
    "    min_count=3,\n",
    "    sg=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('레이블', 0.9915684461593628),\n",
       " ('방시혁', 0.9895018935203552),\n",
       " ('상장', 0.988251805305481),\n",
       " ('주식', 0.9861562848091125),\n",
       " ('몰이', 0.9855770468711853),\n",
       " ('자본', 0.9853259921073914),\n",
       " ('박지원', 0.9847903251647949),\n",
       " ('이단', 0.9847900867462158),\n",
       " ('소속사', 0.9844740629196167),\n",
       " ('변호사', 0.9844532608985901)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('민희진')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['tokens'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19295599, -0.5011149 ,  0.18954143,  0.4211095 ,  0.41131747,\n",
       "        0.22440618,  0.62413543, -0.11859904, -0.13695215,  0.05069742,\n",
       "       -0.03767992, -0.34668222,  0.02807992, -0.4721375 , -0.06153082,\n",
       "        0.08143295], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['인기']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "comments['vector_mean'] = comments.tokens.apply(\n",
    "    lambda x: np.mean(\n",
    "        [model.wv[word] for word in x if len(x) >= 1 and word in model.wv.index_to_key]\n",
    "    , axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 13051.32it/s]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "file_paths = glob.glob('./data/news_data/**/*.txt', recursive=True)\n",
    "for file_path in tqdm(file_paths):\n",
    "    with open(file_path) as file:\n",
    "        temp = file.read()\n",
    "        news.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(string: str, tokenizer: Kiwi, stopwords: Stopwords, tags={'NNP', 'NNG'}):\n",
    "    # 주어진 문자열(string)을 입력으로 받아, 지정된 품사 태그와 길이 조건을 만족하는 토큰들을 추출하는 함수\n",
    "\n",
    "    # Kiwi 객체를 사용하여 문자열을 토크나이즈(tokenize)하고, 불용어(stopwords)를 적용\n",
    "    tokens = tokenizer.tokenize(string, stopwords=stopwords)\n",
    "    \n",
    "    # 토크나이즈된 토큰 중에서, 지정된 태그 집합(tags)에 포함되며 길이가 2 이상인 토큰들의 형태소를 추출하여 리스트에 저장\n",
    "    target_tokens = [token.form for token in tokens if token.tag in tags and len(token.form) >= 2]\n",
    "\n",
    "    # 조건을 만족하는 토큰들의 리스트를 반환\n",
    "    return target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()\n",
    "stopwords = Stopwords()\n",
    "\n",
    "news = pd.DataFrame(news, columns=['contents'])\n",
    "news = news.drop_duplicates()\n",
    "news.contents = news.contents.str.replace('\\s{1,}', ' ', regex=True)\n",
    "news['tokens'] = news.contents.apply(lambda x: extract_tokens(x, kiwi, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=news.tokens.tolist(),\n",
    "    vector_size=64,\n",
    "    window=3,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LG전자', 0.9372370839118958),\n",
       " ('애플', 0.917360246181488),\n",
       " ('갤럭시S', 0.9171818494796753),\n",
       " ('갤럭시', 0.9132965207099915),\n",
       " ('출고', 0.9128235578536987),\n",
       " ('씽큐', 0.9080085754394531),\n",
       " ('스마트폰', 0.8961372971534729),\n",
       " ('인하', 0.8925694823265076),\n",
       " ('하반기', 0.8884367346763611),\n",
       " ('수요', 0.8881014585494995)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('삼성전자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('./word2vec.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
