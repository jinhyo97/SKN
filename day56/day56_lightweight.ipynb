{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft ipywidgets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    AutoPeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    LoftQConfig,\n",
    "    TaskType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight\n",
    "\n",
    "기존 학습된 모델의 정확도를 유지하면서 모델 크기를 줄이고, 연산을 간소화하는 기법.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:18px\"> 종류 </font>\n",
    "\n",
    "- Pruning\n",
    "- Quantization\n",
    "- Knowledge Distillation\n",
    "- PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb76aDK%2FbtsHuMdZpAK%2FWCBvKZOfiEwhWsJlpOhKQ1%2Fimg.jpg)\n",
    "\n",
    "reference: https://huggingface.co/docs/peft/v0.6.0/index\n",
    "\n",
    "<br>\n",
    "\n",
    "PEFT (Parameterized Efficient Fine-Tuning): 모델을 보다 효율적으로 fine-tuning하기 위한 기술. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapter Tuning\n",
    "\n",
    "기존 모델의 특정 레이어에 작은 네트워크(adapter)를 삽입하여 fine-tuning. <br>\n",
    "모델의 기본 parameter는 고정하고 adapter만 새로운 task에 적합하도록 함. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> class AdapterLayer(nn.Module):\n",
    ">     def __init__(self, input_dim, output_dim, reduction_factor=2):\n",
    ">         super().__init__()\n",
    "> \n",
    ">         self.downsample = nn.Linear(input_dim, input_dim//reduction_factor)\n",
    ">         self.activation = nn.ReLU()\n",
    ">         self.upsample = nn.Linear(input_dim//reduction_factor, output_dim)\n",
    "> \n",
    ">     def forward(self, x):\n",
    ">         x = self.downsample(x)\n",
    ">         x = self.activation(x)\n",
    ">         x = self.upsample(x)\n",
    "> \n",
    ">         return x\n",
    "> \n",
    "> class BertWithAdapter(nn.Module):\n",
    ">     def __init__(self, model_name, reduction_factor):\n",
    ">         super().__init__()\n",
    "> \n",
    ">         self.bert = BertModel.from_pretrained(model_name)\n",
    ">         self.adapters = nn.ModuleList([\n",
    ">             AdapterLayer(self.bert.config.hidden_size, self.bert.config.hidden_size, reduction_factor)\n",
    ">             for _ in range(self.bert.config.num_hidden_layers)\n",
    ">         ])\n",
    "> \n",
    ">     def forward(self, inputs):\n",
    ">         outputs = self.bert(**inputs)\n",
    ">         hidden_states = outputs.last_hidden_state\n",
    ">         \n",
    ">         for i, adapter in enumerate(self.adapters):\n",
    ">             hidden_states = adapter(hidden_states)\n",
    ">         \n",
    ">         return hidden_states\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "# 뉴스에 대한 감성분석 모델\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscussionReviewLayer(nn.Module):  # nn.Module을 상속하여 커스텀 레이어 클래스 생성\n",
    "    def __init__(self, in_feature, out_feature, num_labels):\n",
    "        super().__init__()  # 부모 클래스의 초기화 함수 호출\n",
    "\n",
    "        # 첫 번째 선형 변환: 입력 차원(in_feature)에서 중간 출력 차원(out_feature)으로 매핑\n",
    "        self.linear = nn.Linear(in_feature, out_feature)\n",
    "        \n",
    "        # 비선형 활성화 함수 ReLU 정의\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 최종 출력 선형 변환: 중간 출력(out_feature)에서 레이블 수(num_labels)로 매핑\n",
    "        self.out = nn.Linear(out_feature, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 입력 x에 대해 첫 번째 선형 변환 수행\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # ReLU 활성화 함수 적용\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # 두 번째 선형 변환으로 최종 예측 출력 생성\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x  # 최종 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperReviewLayer(nn.Module):  # nn.Module을 상속하여 PaperReviewLayer 클래스 생성\n",
    "    def __init__(self, in_feature, out_feature, num_labels):\n",
    "        super().__init__()  # nn.Module의 초기화 함수 호출\n",
    "\n",
    "        # 첫 번째 선형 변환: 입력 차원(in_feature)에서 중간 출력 차원(out_feature)으로 매핑\n",
    "        self.linear = nn.Linear(in_feature, out_feature)\n",
    "        \n",
    "        # 비선형 활성화 함수 ReLU 정의\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 두 번째 선형 변환: 중간 출력 차원(out_feature)에서 레이블 수(num_labels)로 매핑\n",
    "        self.out = nn.Linear(out_feature, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 입력 텐서 x를 첫 번째 선형 변환에 통과\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # 활성화 함수 ReLU 적용\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # 두 번째 선형 변환 적용\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x  # 최종 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaperReviewLayer 클래스를 사용하여 모델의 분류기(classifier) 레이어를 새로운 레이어로 교체하는 코드입니다.\n",
    "\n",
    "# 입력 특징 수 768, 중간 출력 차원 768, 레이블 수 3으로 PaperReviewLayer 클래스의 인스턴스를 생성\n",
    "paper_review_layer = PaperReviewLayer(768, 768, 3)\n",
    "\n",
    "# 생성한 PaperReviewLayer 인스턴스를 모델의 classifier 레이어에 할당하여 교체\n",
    "# 이를 통해 모델의 분류기 부분이 새로 정의된 PaperReviewLayer 구조로 대체됩니다.\n",
    "model.classifier = paper_review_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.classifier.state_dict(), 'paper_review_layer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23548\\3416307944.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  paper_review_layer.load_state_dict(torch.load('paper_review_layer.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_review_layer.load_state_dict(torch.load('paper_review_layer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperReviewBert(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_feature,\n",
    "            out_feature,\n",
    "            num_labels,\n",
    "            model_name='bert-base-uncased',\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # 사전 학습된 BERT 모델을 불러옴 (분류를 위한 BERT 모델)\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name).bert\n",
    "        # PaperReviewLayer를 초기화하여 최종 출력 레이어 설정\n",
    "        self.paper_review_layer = PaperReviewLayer(in_feature, out_feature, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BERT 모델을 통해 입력을 전파하고, [CLS] 토큰의 출력만 가져옴\n",
    "        x = self.bert(x).last_hidden_state[:, 0, :]\n",
    "        # PaperReviewLayer를 통해 출력 변환\n",
    "        x = self.paper_review_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice (Colab, L4)\n",
    "# 데이터 출처\n",
    "## 네이버 영화: https://github.com/e9t/nsmc/\n",
    "## 네이버 쇼핑: https://github.com/bab2min/corpus/tree/master/sentiment\n",
    "\n",
    "# 모델: bert-base-multilingual-uncased, llama\n",
    "# 네이버 영화와 네이버 쇼핑에 대한 감성 분류 모델을 각각 학습합니다.\n",
    "\n",
    "# 영화 데이터 로드\n",
    "movie_data = pd.read_csv(\n",
    "    './data/naver_movie.txt',\n",
    "    sep='\\t',\n",
    "    usecols=['document', 'label']  # 문서와 레이블 열만 사용\n",
    ")\n",
    "\n",
    "# 쇼핑 데이터 로드\n",
    "shopping_data = pd.read_csv(\n",
    "    './data/naver_shopping.txt',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['label', 'review']  # 열 이름 지정\n",
    ")\n",
    "\n",
    "# 쇼핑 데이터 레이블 값 확인\n",
    "shopping_data.label.value_counts()\n",
    "\n",
    "# 레이블 값 변경: 1, 2, 4, 5를 0, 1, 2, 3으로 변환\n",
    "shopping_data.label = shopping_data.label.replace({1: 0, 2: 1, 4: 2, 5: 3})\n",
    "\n",
    "# NaverMoviewClassifier 클래스 정의\n",
    "class NaverMoviewClassifer(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, num_labels):\n",
    "        super().__init__()\n",
    "        # 선형 레이어, ReLU 활성화 함수 및 최종 출력 레이어 정의\n",
    "        self.linear = nn.Linear(in_feature, out_feature)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(out_feature, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력을 선형 레이어에 통과시키고 활성화\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        # 최종 출력\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# NaverMovieBert 클래스 정의\n",
    "class NaverMovieBert(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_feature,\n",
    "            out_feature,\n",
    "            num_labels,\n",
    "            model_name='bert-base-uncased',\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # BERT 모델 로드 (분류를 위한 BERT)\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name).bert\n",
    "        # 영화 분류기 레이어 정의\n",
    "        self.naver_movie_classifier = NaverMoviewClassifer(in_feature, out_feature, num_labels)\n",
    "\n",
    "        # BERT의 모든 파라미터를 고정(freeze)\n",
    "        for parameter in self.bert.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        # BERT 모델에 입력 전달\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # BERT의 [CLS] 토큰 출력으로 분류 수행\n",
    "        logits = self.naver_movie_classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        # 손실을 계산하고 반환\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# NaverShoppingClassifier 클래스 정의\n",
    "class NaverShoppingClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, num_labels):\n",
    "        super().__init__()\n",
    "        # 선형 레이어, ReLU 활성화 함수 및 최종 출력 레이어 정의\n",
    "        self.linear = nn.Linear(in_feature, out_feature)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(out_feature, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력을 선형 레이어에 통과시키고 활성화\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        # 최종 출력\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# NaverShoppingBert 클래스 정의\n",
    "class NaverShoppingBert(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_feature,\n",
    "            out_feature,\n",
    "            num_labels,\n",
    "            model_name='bert-base-uncased',\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # BERT 모델 로드\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        # 쇼핑 분류기 레이어 정의\n",
    "        self.naver_shopping_classifier = NaverShoppingClassifier(in_feature, out_feature, num_labels)\n",
    "\n",
    "        # BERT의 모든 파라미터를 고정(freeze)\n",
    "        for parameter in self.bert.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        # BERT 모델에 입력 전달\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # BERT의 [CLS] 토큰 출력으로 분류 수행\n",
    "        logits = self.naver_shopping_classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        # 손실을 계산하고 반환\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# 데이터 샘플 수를 2000으로 제한\n",
    "movie_data = movie_data.iloc[:2000]\n",
    "shopping_data = shopping_data.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델 이름 정의 (Google BERT의 다국어 버전)\n",
    "model_name = 'google-bert/bert-base-multilingual-uncased'\n",
    "\n",
    "# AutoTokenizer를 사용하여 지정한 모델에 맞는 토크나이저를 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 데이터에 대해 토큰화 및 필요한 처리를 적용\n",
    "movie_data['tokenized'] = movie_data.document.apply(lambda x: tokenizer(\n",
    "    x,                                # 입력 텍스트\n",
    "    padding='max_length',            # 최대 길이에 맞춰 패딩 추가\n",
    "    max_length=128,                  # 최대 토큰 수 (128로 설정)\n",
    "    truncation=True,                 # 길이가 초과하는 경우 잘림 처리\n",
    "    return_tensors='pt',            # PyTorch 텐서 형식으로 반환\n",
    "))\n",
    "\n",
    "# 쇼핑 리뷰 데이터에 대해 토큰화 및 필요한 처리를 적용\n",
    "shopping_data['tokenized'] = shopping_data.review.apply(lambda x: tokenizer(\n",
    "    x,                                # 입력 텍스트\n",
    "    padding='max_length',            # 최대 길이에 맞춰 패딩 추가\n",
    "    max_length=128,                  # 최대 토큰 수 (128로 설정)\n",
    "    truncation=True,                 # 길이가 초과하는 경우 잘림 처리\n",
    "    return_tensors='pt',            # PyTorch 텐서 형식으로 반환\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NaverDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        데이터셋 초기화\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): 입력 데이터프레임\n",
    "        \"\"\"\n",
    "        self.data = data  # 데이터프레임을 인스턴스 변수로 저장\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 길이를 반환\n",
    "\n",
    "        Returns:\n",
    "            int: 데이터셋의 샘플 수\n",
    "        \"\"\"\n",
    "        return len(self.data)  # 데이터프레임의 길이 반환\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 샘플을 반환\n",
    "\n",
    "        Args:\n",
    "            idx (int): 샘플의 인덱스\n",
    "\n",
    "        Returns:\n",
    "            dict: 입력 데이터와 레이블을 포함하는 딕셔너리\n",
    "        \"\"\"\n",
    "        temp = self.data.iloc[idx]  # 주어진 인덱스의 데이터를 가져옴\n",
    "\n",
    "        return {\n",
    "            'input_ids': temp.tokenized.get('input_ids')[0],         # 토큰화된 입력 ID\n",
    "            'token_type_ids': temp.tokenized.get('token_type_ids')[0], # 토큰 유형 ID\n",
    "            'attention_mask': temp.tokenized.get('attention_mask')[0], # 어텐션 마스크\n",
    "            'labels': temp.label,                                       # 레이블\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쇼핑 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "train_shopping, temp = train_test_split(shopping_data, test_size=0.5, random_state=0)\n",
    "valid_shopping, test_shopping = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# 영화 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "train_movie, temp = train_test_split(movie_data, test_size=0.5, random_state=0)\n",
    "valid_movie, test_movie = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# NaverDataset 클래스를 사용하여 데이터셋 객체 생성\n",
    "train_dataset_shopping = NaverDataset(train_shopping)  # 훈련 쇼핑 데이터셋\n",
    "valid_dataset_shopping = NaverDataset(valid_shopping)  # 검증 쇼핑 데이터셋\n",
    "test_dataset_shopping = NaverDataset(test_shopping)    # 테스트 쇼핑 데이터셋\n",
    "\n",
    "train_dataset_movie = NaverDataset(train_movie)      # 훈련 영화 데이터셋\n",
    "valid_dataset_movie = NaverDataset(valid_movie)      # 검증 영화 데이터셋\n",
    "test_dataset_movie = NaverDataset(test_movie)        # 테스트 영화 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 데이터로 adapter 학습 후 weight 저장\n",
    "\n",
    "# 훈련 매개변수를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                    # 평가 전략: 에포크마다 평가\n",
    "    learning_rate=2e-5,                       # 학습률\n",
    "    per_device_train_batch_size=2,           # 장치당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=2,            # 장치당 평가 배치 크기\n",
    "    num_train_epochs=2,                       # 훈련할 에포크 수\n",
    "    weight_decay=0.01,                        # 가중치 감쇠\n",
    ")\n",
    "\n",
    "# 미리 학습된 BERT 모델을 불러옵니다 (분류용).\n",
    "model = NaverShoppingBert(768, 768, 4, model_name)\n",
    "\n",
    "# Trainer 객체를 초기화합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,                              # 사용할 모델\n",
    "    args=training_args,                       # 훈련 매개변수\n",
    "    train_dataset=train_dataset_shopping,              # 훈련 데이터셋\n",
    "    eval_dataset=valid_dataset_shopping,                # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaverShoppingClassifier의 가중치를 저장\n",
    "torch.save(model.naver_shopping_classifier.state_dict(), 'shopping_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 새롭게 model 생성\n",
    "model = NaverShoppingBert(768, 768, 4, 'bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23548\\2738794209.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('shopping_classifier.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존에 학습한 weight를 load\n",
    "model.naver_shooping_classifier.load_state_dict(\n",
    "    torch.load('shopping_classifier.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6610e653a08047f9b5beb143d3ddf877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.04252329,  0.1618462 , -0.2530684 ,  0.18095714],\n",
       "       [-0.03171977,  0.15301907, -0.25044414,  0.19558582],\n",
       "       [-0.03076731,  0.14553234, -0.26409313,  0.20010293],\n",
       "       ...,\n",
       "       [-0.03610814,  0.15791234, -0.24591534,  0.18981424],\n",
       "       [-0.04775867,  0.13160829, -0.25120008,  0.19475177],\n",
       "       [-0.03156469,  0.14278744, -0.2520712 ,  0.19009817]],\n",
       "      dtype=float32), label_ids=array([1, 3, 3, 0, 3, 1, 3, 3, 0, 0, 1, 0, 3, 2, 1, 0, 0, 3, 0, 3, 3, 0,\n",
       "       2, 3, 3, 3, 1, 3, 1, 2, 3, 2, 0, 3, 3, 1, 1, 1, 3, 3, 3, 0, 1, 1,\n",
       "       0, 3, 3, 3, 2, 3, 2, 3, 3, 3, 0, 3, 0, 2, 3, 3, 1, 3, 0, 1, 3, 0,\n",
       "       3, 2, 3, 1, 3, 1, 0, 1, 3, 1, 3, 1, 1, 3, 3, 1, 1, 2, 0, 3, 2, 3,\n",
       "       1, 3, 3, 0, 1, 3, 1, 3, 0, 3, 1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 3, 0,\n",
       "       3, 2, 3, 1, 1, 0, 2, 1, 0, 3, 3, 0, 1, 1, 3, 3, 3, 1, 3, 1, 3, 1,\n",
       "       0, 3, 1, 1, 3, 1, 3, 3, 3, 3, 2, 1, 3, 1, 2, 3, 2, 1, 3, 3, 2, 3,\n",
       "       3, 3, 3, 3, 0, 3, 0, 0, 0, 1, 0, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3,\n",
       "       3, 1, 0, 3, 3, 3, 0, 1, 2, 2, 1, 3, 1, 0, 3, 2, 3, 3, 1, 0, 1, 3,\n",
       "       1, 3, 3, 1, 3, 1, 2, 1, 3, 2, 3, 1, 0, 3, 3, 1, 1, 0, 2, 1, 0, 3,\n",
       "       1, 0, 0, 0, 1, 3, 2, 2, 3, 1, 3, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 2,\n",
       "       1, 3, 1, 1, 1, 3, 0, 0, 3, 3, 0, 1, 0, 2, 1, 1, 2, 0, 0, 3, 3, 2,\n",
       "       3, 0, 0, 3, 1, 1, 3, 1, 3, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 3, 3, 1,\n",
       "       0, 0, 1, 3, 1, 1, 0, 3, 3, 1, 0, 1, 1, 0, 3, 0, 1, 0, 1, 3, 0, 1,\n",
       "       3, 1, 1, 1, 0, 0, 3, 3, 3, 3, 3, 3, 1, 0, 1, 1, 3, 0, 3, 3, 2, 2,\n",
       "       0, 0, 0, 3, 3, 3, 0, 1, 1, 1, 3, 1, 3, 1, 0, 2, 0, 0, 1, 1, 3, 3,\n",
       "       1, 1, 3, 1, 3, 3, 3, 1, 1, 0, 3, 1, 3, 1, 2, 3, 0, 1, 1, 1, 3, 1,\n",
       "       2, 3, 3, 0, 3, 1, 1, 3, 1, 1, 2, 3, 1, 3, 3, 3, 0, 1, 1, 0, 3, 3,\n",
       "       1, 2, 1, 1, 1, 2, 3, 1, 1, 0, 1, 1, 1, 3, 1, 0, 0, 0, 1, 2, 1, 3,\n",
       "       3, 3, 3, 1, 1, 3, 1, 3, 3, 1, 0, 1, 2, 3, 1, 2, 3, 3, 3, 3, 3, 2,\n",
       "       1, 1, 0, 2, 1, 1, 1, 3, 0, 1, 1, 1, 3, 0, 3, 1, 0, 0, 1, 3, 3, 1,\n",
       "       0, 3, 1, 0, 3, 3, 3, 3, 3, 1, 3, 2, 1, 2, 1, 3, 3, 0, 2, 3, 3, 1,\n",
       "       3, 3, 2, 2, 1, 3, 3, 0, 1, 1, 3, 3, 3, 0, 1, 3], dtype=int64), metrics={'test_loss': 1.3231526613235474, 'test_runtime': 46.1682, 'test_samples_per_second': 10.83, 'test_steps_per_second': 5.415})"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 weight를 load하여 추론\n",
    "trainer.predict(test_dataset_shopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 adapter 생성\n",
    "naver_movie_classifier = NaverMoviewClassifer(768, 768, 2, 'bert-base-multilingual-uncased')\n",
    "naver_movie_classifier.load_state_dict(torch.load())\n",
    "\n",
    "# 새로운 adapter 연결\n",
    "model.naver_shooping_classifier = naver_movie_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "\n",
    "paper: https://arxiv.org/pdf/2106.09685\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYMCCt%2FbtsIYv4efkd%2FGVwpQOnQhc0c6AGNqwAI20%2Fimg.png\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"><b> 기존 fine-tuning의 문제점 </b></font>\n",
    "\n",
    "1. 모델 크기 문제: fine-tuning 시 최종 모델도 원래 모델 만큼 많은 파라미터를 가짐 <br>\n",
    "    -> 배포 문제: 특히 대규모 모델의 경우, 파라미터 수가 많아 배포가 어려워짐\n",
    "2. Inference latency 증가: 모델 깊이 확장, 또는 시퀀스 길이를 줄이는 방식으로 인해 야기\n",
    "    - 효율성과 품질 간의 trade-off\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"><b> Contribution </b></font>\n",
    "\n",
    "1. 모델 공유와 모듈화: 사전 학습된 모델을 공유하여 다양한 작업을 위한 작은 LoRA 모듈을 구축 가능. <br>\n",
    "    -> 공유 모델을 고정하고, 작업 전환 시 그림과 같이 행렬 A와 B를 교체함으로써 저장 요구 사항과 작업 전환 오버헤드를 크게 줄일 수 있음.\n",
    "2. 효율적인 훈련: LoRA는 훈련을 더 효율적으로 만들고 하드웨어 진입 장벽을 최대 3배 낮춤. <br>\n",
    "    <- 대부분의 파라미터에 대해 기울기를 계산하거나 옵티마이저 상태를 유지할 필요가 없기 때문. <br>\n",
    "    -> 훨씬 더 작은 저차 행렬만 최적화.\n",
    "3. 지연 없는 배포: 배포 시 학습 가능한 행렬을 고정된 가중치와 병합할 수 있어, 완전히 파인튜닝된 모델과 비교해도 지연이 발생하지 않음.\n",
    "4. 다양한 방법과의 호환성: LoRA는 많은 이전 방법들과 독립적이며, prefix-tuning과 같은 여러 방법과 결합 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "<font style=\"font-size:18px\"><b> LOW-RANK-PARAMETRIZED UPDATE MATRICES </b></font>\n",
    "\n",
    "특정 task에 fine-tuning할 때 사전 학습된 언어 모델은 더 작은 subspace로의 랜덤 projection에도 불구하고 여전히 효율적으로 학습할 수 있고, 낮은 instrisic dimension을 가짐 (https://arxiv.org/pdf/2012.13255) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; over-parametrized model은 실제로 low intrinsic dimension에 핵심이 존재 <br>\n",
    "이에 영감받아 가중치에 대한 업데이트도 adaptation 중에 낮은 instrisic rank를 갖는다고 가정\n",
    "\n",
    "$ W_0 + \\Delta W = W_0 + B A $ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; where $ B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$, $r \\ll \\min(d, k) $, r: rank, $W_0$: pre-trained weight <br>\n",
    "\n",
    "$ h = w_0x + \\Delta x = W_0x + BAx $\n",
    "- $ \\Delta Wx$: $ \\alpha / r $로 스케일링\n",
    "- $ \\alpha $: r의 상수\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:18px\"><b> APPLYING LORA TO TRANSFORMER </b></font>\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Focf8S%2FbtsIYWNUe7c%2Fv60Vk4pQj3AnpwrjGTPb6k%2Fimg.png)\n",
    "\n",
    "weight 행렬의 모든 부분집합에 LoRA 적용 가능. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;단순성 및 파라미터 효율성을 위해 task에 대한 attention 내 weight만 튜닝하고 mlp 모듈은 고정. <br>\n",
    "-> r << $d_{\\text{model}}$의 경우 VARM을 최대 2/3까지 줄일 수 있음 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;    ex) 1.2T -> 350G (GPT3) <br>\n",
    "-> $r=4$이고, q, k만 튜닝되면 모델 사이즈가 350GB -> 35MB로 감소 <br>\n",
    "모든 parameter가 아닌 LoRA 가중치면 교환하기에 더 저렴한 비용으로 배포 중 task 전환이 용이. <br>\n",
    "대다수의 파라미터에서 기울기 재계산이 필요없기에 전체 fine-tuning대비 속도 향상. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; ex) GPT3 - 25%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용 방법\n",
    "\n",
    "> ```python\n",
    "> model_name = 'bert-base-uncased'\n",
    "> model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "> tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "> \n",
    "> lora_config = LoraConfig(\n",
    ">     r=8,  \n",
    ">     lora_alpha=16,\n",
    ">     lora_dropout=0.1,\n",
    ">     task_type=TaskType.SEQ_CLS,\n",
    ">     target_modules=[<layer_name>],\n",
    "> )\n",
    "> \n",
    "> lora_model = get_peft_model(model, lora_config)\n",
    "> lora_model.print_trainable_parameters()\n",
    "> ```\n",
    "- r: Low-rank\n",
    "- lora_alpha: Scaling factor\n",
    "- lora_dropout: Dropout rate\n",
    "- task_type: task type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# 사전 훈련된 BERT 모델을 로드\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# LoRA 구성 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Low-rank 매트릭스의 차원\n",
    "    lora_alpha=16,  # LoRA에서 사용되는 스케일링 계수\n",
    "    lora_dropout=0.1,  # Dropout 비율\n",
    "    task_type=TaskType.SEQ_CLS,  # 작업 유형: 시퀀스 분류\n",
    "    target_modules=['query', 'key'],  # LoRA 적용할 모듈\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForSequenceClassification(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\peft\\tuners\\lora\\layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 사전 훈련된 OpenAI GPT 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained('openai-community/openai-gpt')\n",
    "\n",
    "# LoRA 구성 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Low-rank 매트릭스의 차원\n",
    "    lora_alpha=16,  # LoRA에서 사용되는 스케일링 계수\n",
    "    lora_dropout=0.1,  # Dropout 비율\n",
    "    task_type=TaskType.CAUSAL_LM,  # 작업 유형: 인과 언어 모델링\n",
    "    target_modules=['c_attn', 'c_proj'],  # LoRA 적용할 모듈\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,504 || all params: 116,940,288 || trainable%: 0.3468\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "PeftModelForCausalLM                                              --\n",
       "├─LoraModel: 1-1                                                  --\n",
       "│    └─OpenAIGPTLMHeadModel: 2-1                                  --\n",
       "│    │    └─OpenAIGPTModel: 3-1                                   116,940,288\n",
       "│    │    └─Linear: 3-2                                           (31,087,104)\n",
       "==========================================================================================\n",
       "Total params: 148,027,392\n",
       "Trainable params: 405,504\n",
       "Non-trainable params: 147,621,888\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## day46 bert NLI \n",
    "## bert 모델에 LoRA (q, k) 적용해서 결과 보기\n",
    "## Colab L4\n",
    "\n",
    "def preprocessing(row):\n",
    "    # 주어진 행(row)에서 전제(premise)와 가설(hypothesis)을 토크나이즈합니다.\n",
    "    return tokenizer(\n",
    "        row['premise'],                          # 전제 문장\n",
    "        row['hypothesis'],                       # 가설 문장\n",
    "        truncation=True,                        # 길이가 max_length를 초과하는 경우 잘라냄\n",
    "        padding='max_length',                           # 최대 길이에 맞춰 패딩 추가\n",
    "        return_tensors='pt',                   # 파이토치 텐서로 반환\n",
    "        max_length=128,                        # 최대 길이 설정\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('snli')\n",
    "\n",
    "# 훈련 데이터셋에서 처음 1000개의 샘플을 선택합니다.\n",
    "train_dataset = dataset['train'].select(range(2000))\n",
    "\n",
    "# 검증 데이터셋에서 처음 1000개의 샘플을 선택합니다.\n",
    "valid_dataset = dataset['validation'].select(range(2000))\n",
    "\n",
    "# BERT 모델의 이름을 지정합니다. 여기서는 소문자 처리가 된 BERT 모델을 사용합니다.\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 미리 학습된 BERT 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 훈련 데이터셋에 대해 전처리 함수를 적용하여 토크나이즈된 데이터셋을 생성합니다.\n",
    "tokenized_train_dataset = train_dataset.map(preprocessing, batched=True)\n",
    "\n",
    "# 검증 데이터셋에 대해 전처리 함수를 적용하여 토크나이즈된 데이터셋을 생성합니다.\n",
    "tokenized_valid_dataset = valid_dataset.map(preprocessing, batched=True)\n",
    "\n",
    "# 토크나이즈된 훈련 데이터셋을 리스트로 변환하며, 레이블이 -1이 아닌 데이터만 포함합니다.\n",
    "tokenized_train_dataset = [\n",
    "    {'input_ids': row.get('input_ids'),                # 입력 ID\n",
    "     'token_type_ids': row.get('token_type_ids'),      # 토큰 타입 ID\n",
    "     'attention_mask': row.get('attention_mask'),       # 어텐션 마스크\n",
    "     'label': row.get('label')}                          # 레이블\n",
    "    for row in tokenized_train_dataset\n",
    "    if row.get('label') != -1                          # 레이블이 -1이 아닌 경우만 포함\n",
    "]\n",
    "\n",
    "# 토크나이즈된 검증 데이터셋을 리스트로 변환하며, 레이블이 -1이 아닌 데이터만 포함합니다.\n",
    "tokenized_valid_dataset = [\n",
    "    {'input_ids': row.get('input_ids'),                # 입력 ID\n",
    "     'token_type_ids': row.get('token_type_ids'),      # 토큰 타입 ID\n",
    "     'attention_mask': row.get('attention_mask'),       # 어텐션 마스크\n",
    "     'label': row.get('label')}                          # 레이블\n",
    "    for row in tokenized_valid_dataset\n",
    "    if row.get('label') != -1                          # 레이블이 -1이 아닌 경우만 포함\n",
    "]\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터셋 초기화: 주어진 데이터를 저장합니다.\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환합니다.\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터 항목을 반환합니다.\n",
    "        return {\n",
    "            'input_ids': self.data[idx].get('input_ids'),          # 입력 ID\n",
    "            'token_type_ids': self.data[idx].get('token_type_ids'),# 토큰 타입 ID\n",
    "            'attention_mask': self.data[idx].get('attention_mask'), # 어텐션 마스크\n",
    "            'labels': self.data[idx].get('label'),                  # 레이블\n",
    "        }\n",
    "\n",
    "# 토크나이즈된 훈련 데이터셋을 NLIDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = NLIDataset(tokenized_train_dataset)\n",
    "\n",
    "# 토크나이즈된 검증 데이터셋을 NLIDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "valid_dataset = NLIDataset(tokenized_valid_dataset)\n",
    "\n",
    "# 학습을 위한 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/NLI',                   # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                         # 평가 전략: 매 에폭마다 평가\n",
    "    learning_rate=2e-5,                           # 학습률\n",
    "    warmup_steps=50,                              # 웜업 단계 수\n",
    "    per_device_train_batch_size=256,               # 디바이스당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=256,                # 디바이스당 평가 배치 크기\n",
    "    num_train_epochs=1,                           # 훈련 에폭 수\n",
    "    weight_decay=0.01,                            # 가중치 감소\n",
    "    logging_dir='./logs',                         # 로그 저장 디렉토리\n",
    ")\n",
    "\n",
    "# 미리 학습된 BERT 모델을 로드하고, 레이블 수를 설정합니다. 여기서는 3개의 레이블이 있습니다.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                       # LoRA의 차원 수\n",
    "    lora_alpha=2e-5,          # LoRA의 학습률 조정값\n",
    "    lora_dropout=0.1,          # 드롭아웃 비율\n",
    "    task_type=TaskType.SEQ_CLS, # 작업 유형: 시퀀스 분류\n",
    "    target_modules=['query', 'key'] # LoRA를 적용할 모듈\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config) # LoRA 설정이 적용된 모델을 생성\n",
    "\n",
    "# Trainer 객체를 생성하여 모델, 학습 인자, 훈련 데이터셋, 평가 데이터셋을 지정합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,                  # 훈련 데이터셋\n",
    "    eval_dataset=valid_dataset,                   # 검증 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 훈련된 모델을 저장합니다.\n",
    "model.save_pretrained('./bert_nli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbXst46%2FbtrLgq8tGwo%2FwMWbieyztBgOz4RvkEW9C1%2Fimg.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FO8PQn%2FbtrLgN9Y8gp%2FhnXOuZ19p7KOnG023WbATk%2Fimg.png\" width=\"400\">\n",
    "\n",
    "\n",
    "paper: https://arxiv.org/pdf/1712.05877\n",
    "\n",
    "<br>\n",
    "\n",
    "모델 parameter를 low bit로 표현하여 모델 사이즈를 줄이는 경량화 기법 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; fp32 -> int8\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:18px\"> 분류 </font>\n",
    "\n",
    "- fixed point dnn quantization: inference에 사용할 model을 만드는 것을 목적으로 함 <br>\n",
    "-> forward에 대해서만 quantization\n",
    "\n",
    "\n",
    "<center><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbNUGim%2FbtsG2VXn5AC%2F2JcO58bbR7xz5KYsB8qODK%2Fimg.png\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "- floating point dnn quantization: backward에 대해서도 quantization \n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:18px\"> 사용 방법 </font>\n",
    "\n",
    "> ```python\n",
    "> quantization_config = BitsAndBytesConfig(\n",
    ">     load_in_8bit=True,  # 8 bit 양자화\n",
    ">     load_in_4bit=True,  # 4 bit 양자화\n",
    "> )\n",
    "> \n",
    "> model = AutoModelForCausalLM.from_pretrained(\n",
    ">     <model_name>, \n",
    ">     quantization_config=quantization_config,\n",
    "> )\n",
    "> \n",
    "> model.get_memory_footprint()  # 용량 확인 (byte 단위)\n",
    ">\n",
    "> model.dequantize()  # 비양자화\n",
    ">\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 양자화 설정\n",
    "# 모델을 4비트로 로드할 수 있도록 BitsAndBytesConfig 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4비트로 모델을 로드\n",
    ")\n",
    "\n",
    "# 2. 기본 모델 로드\n",
    "# 양자화 설정을 적용하여 LLaMA 모델을 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-3.2-3B-Instruct',  # 사전 훈련된 LLaMA 모델\n",
    "    quantization_config=quantization_config,  # 위에서 설정한 양자화 설정 적용\n",
    ")\n",
    "\n",
    "# 3. LoRA 설정\n",
    "# LoRA를 적용하기 위한 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=2,                   # LoRA의 차원 수 (저차원 표현)\n",
    "    lora_alpha=1e-4,      # LoRA의 학습률 조정값 (작은 값으로 설정)\n",
    "    lora_dropout=0.1,      # 드롭아웃 비율 (과적합 방지를 위해 설정)\n",
    "    task_type=TaskType.CAUSAL_LM, # 작업 유형: 언어 생성\n",
    "    target_modules=['q_proj', 'p_proj'],  # LoRA를 적용할 모듈 지정 (쿼리와 프로젝션 모듈)\n",
    ")\n",
    "\n",
    "# 4. LoRA 모델 생성\n",
    "# 기본 모델과 LoRA 설정을 통합하여 LoRA 모델 생성\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
