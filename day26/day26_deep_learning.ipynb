{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V6g7lqozJEr"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1SZSOJe1gnd"
      },
      "source": [
        "## 딥러닝이란?\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/talend/image/upload/w_1600/q_auto/qlik/glossary/augmented-analytics/seo-hero-machine-learning-vs-ai_kls4c0.png\" width=\"700\" height=\"400\"/>\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png\" width=\"700\" height=\"400\"/>\n",
        "\n",
        "- AI의 한 분야\n",
        "- 인간의 두뇌에서 영감을 얻은 방식으로 데이터를 처리하도록 컴퓨터를 가르치는 학습 방식\n",
        "- 딥러닝 이전에 불가하였던 그림, 텍스트, 사운드 및 기타 데이터의 복잡한 패턴을 인식하여 보다 정확한 모델 생성 가능\n",
        "- 이는 **컴퓨팅 파워의 증가**와도 연관이 깊음\n",
        "- 딥러닝의 핵심은 **복잡한 패턴**을 찾는 것으로 ***데이터의 패턴이 단순한 경우 딥러닝을 사용하는 것이 오히려 독이 될 수 있음***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViaUVZOY0vA2"
      },
      "source": [
        "## 딥러닝과 머신러닝과의 차이\n",
        "\n",
        "<img src=\"https://cdn.gttkorea.com/news/photo/202306/5448_6001_923.jpg\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "- 딥러닝과 머신러닝의 가장 큰 차이는 feature engineering에 있음\n",
        "- 전통적인 통계 기법이나 머신러닝 등에서는 모델링 전 설명변수(종속변수)와 관계성이 <br>\n",
        "  높은 변수를 직접 선별하거나 생성하는 작업을 거쳐야만 했음\n",
        "  - 이는 모델링 작업에서 많은 리소스 소비\n",
        "- 딥러닝에서는 데이터를 모델에 넣으면, 모델에서 유의한 설명변수를 찾아낼 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJLFBT-t2AhS"
      },
      "source": [
        "## 딥러닝의 구성요소\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1156/format:webp/1*ToPT8jnb5mtnikmiB42hpQ.png\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "딥러닝은 크게 아래의 요소로 구분되며 **두 개 이상의 hidden layer**를 가지는 경우를 딥러닝이라고 부름\n",
        "\n",
        "<br>\n",
        "\n",
        "**입력 계층 (Input Layer)** <br>\n",
        "- 데이터를 입력하는 노드\n",
        "- 노드 하나가 특성(feature)의 수\n",
        "\n",
        "<br>\n",
        "\n",
        "**은닉 계층 (Hidden Layer)**\n",
        "- 입력 계층에서 데이터를 처리하여 신경망의 hidden layer로 전달\n",
        "- 은닉 계층은 서로 다른 수준에서 정보를 처리하고 새 정보를 수신할 때마다 동작을 조정(update)\n",
        "- 데이터의 복잡도나 모델 구성에 따라 hidden layer의 수는 천차만별\n",
        "\n",
        "<br>\n",
        "\n",
        "**출력 계층 (Output Layer)**\n",
        "- 데이터를 출력하는 노드\n",
        "- 분류 문제에서는 정답의 갯수 만큼 노드 존재 <br>\n",
        "    ex) yes 또는 no라는 답을 출력하는 딥 러닝 모델은 출력 계층에 2개 노드 존재\n",
        "- 회귀 문제에서는\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm7LHe2314tz"
      },
      "source": [
        "## 딥러닝의 단점\n",
        "\n",
        "1. **Black Box**\n",
        "- 모델 학습 결과를 설명하거나 이해할 수 없음 (그냥 받아들여야 함)\n",
        "- 설명이 요구되는 분야에서는 치명적인 약점 <br>\n",
        "  ex) 대출 승인 거절 시, 거절에 대한 이유를 알 수 없음\n",
        "\n",
        "<br>\n",
        "\n",
        "2. Data Requirements\n",
        "- 모델 학습을 위해 **다량의 고품질 데이터** 필요 <br>\n",
        "- 모델의 복잡도 대비 데이터 수가 부족하면 과적합 발생\n",
        "\n",
        "<br>\n",
        "\n",
        "3. Computation\n",
        "- 모델 학습을 위해서 gpu 등 사용\n",
        "- 모델이 복잡해져감에 따라 더 많은 리소스를 사용\n",
        "- 일부 모델은 수 주를 학습하는 경우도 존재\n",
        "\n",
        "<br>\n",
        "\n",
        "4. Architecture Design\n",
        "- 딥 러닝은 아키텍처를 정의하는 데 많은 시간을 필요 <br>\n",
        "  ex) 모델이 몇 개의 레이어를 가질지, 어떤 유형의 레이어를 가져야 하는지, 어떤 활성화 함수를 사용해야 하는지 등\n",
        "- 데이터의 특징을 적절하게 반영하지 못하는 아키텍처에서 좋은 퍼포먼스가 나오지 못함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAGOdG3NUOU_"
      },
      "source": [
        "## 딥러닝 프레임워크\n",
        "파이썬에서는 딥러닝을 사용할 수 있는 다양한 프레임워크 존재\n",
        "  - Tensorflow (Google)\n",
        "  - Pytorch (Facebook)\n",
        "  - MXNet (Apache)\n",
        "\n",
        "<br>\n",
        "\n",
        "일반적으로는 Tensorflow와 Pytorch를 사용\n",
        "  - Tensorflow: 배포 관점에서 우위, Customize가 일부 불편\n",
        "  - Pytorch: 연구 관점에서 우위, Customize가 용이"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.6286, 0.8263, 0.0284,  ..., 0.1953, 0.7698, 0.5585],\n",
              "         [0.6597, 0.8929, 0.5985,  ..., 0.3256, 0.2965, 0.7933],\n",
              "         [0.5645, 0.3219, 0.4595,  ..., 0.8822, 0.4023, 0.2130],\n",
              "         ...,\n",
              "         [0.2068, 0.5485, 0.9253,  ..., 0.4670, 0.1534, 0.0187],\n",
              "         [0.9657, 0.1855, 0.7336,  ..., 0.5491, 0.7857, 0.6152],\n",
              "         [0.5420, 0.2534, 0.6782,  ..., 0.5159, 0.4720, 0.6904]],\n",
              "\n",
              "        [[0.5814, 0.0814, 0.9047,  ..., 0.7644, 0.9424, 0.4151],\n",
              "         [0.0491, 0.6602, 0.7032,  ..., 0.1185, 0.5430, 0.5576],\n",
              "         [0.3511, 0.5721, 0.3816,  ..., 0.7435, 0.8418, 0.8337],\n",
              "         ...,\n",
              "         [0.3323, 0.4192, 0.8478,  ..., 0.1424, 0.5160, 0.2799],\n",
              "         [0.4443, 0.6993, 0.1452,  ..., 0.5368, 0.8824, 0.4310],\n",
              "         [0.8468, 0.1164, 0.8002,  ..., 0.6162, 0.0048, 0.8556]],\n",
              "\n",
              "        [[0.9442, 0.2972, 0.3832,  ..., 0.3010, 0.5388, 0.4950],\n",
              "         [0.1683, 0.6623, 0.8077,  ..., 0.3827, 0.2251, 0.6651],\n",
              "         [0.7937, 0.0934, 0.3754,  ..., 0.4982, 0.5126, 0.9278],\n",
              "         ...,\n",
              "         [0.2181, 0.5459, 0.6802,  ..., 0.6805, 0.0915, 0.6902],\n",
              "         [0.1836, 0.3322, 0.8007,  ..., 0.9635, 0.6877, 0.6320],\n",
              "         [0.9727, 0.3808, 0.6572,  ..., 0.1399, 0.6732, 0.7008]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.4437, 0.6403, 0.6589,  ..., 0.3283, 0.7243, 0.3878],\n",
              "         [0.8167, 0.5248, 0.3474,  ..., 0.5706, 0.9536, 0.2415],\n",
              "         [0.6920, 0.5114, 0.7445,  ..., 0.3135, 0.0100, 0.0938],\n",
              "         ...,\n",
              "         [0.4536, 0.6184, 0.2099,  ..., 0.7016, 0.3827, 0.0995],\n",
              "         [0.0579, 0.4182, 0.6475,  ..., 0.8143, 0.9099, 0.2866],\n",
              "         [0.0173, 0.4923, 0.4612,  ..., 0.6143, 0.0891, 0.1307]],\n",
              "\n",
              "        [[0.1371, 0.1938, 0.2760,  ..., 0.5530, 0.0059, 0.4465],\n",
              "         [0.7244, 0.5160, 0.9659,  ..., 0.3873, 0.7579, 0.2651],\n",
              "         [0.6677, 0.0690, 0.3309,  ..., 0.5369, 0.3222, 0.9062],\n",
              "         ...,\n",
              "         [0.6695, 0.2591, 0.6689,  ..., 0.8182, 0.0276, 0.9255],\n",
              "         [0.8746, 0.0077, 0.9978,  ..., 0.1937, 0.7406, 0.9606],\n",
              "         [0.0684, 0.9124, 0.3213,  ..., 0.7992, 0.5159, 0.1118]],\n",
              "\n",
              "        [[0.3105, 0.8681, 0.6013,  ..., 0.6066, 0.5056, 0.1090],\n",
              "         [0.7778, 0.1536, 0.5027,  ..., 0.8523, 0.2395, 0.5642],\n",
              "         [0.3255, 0.1315, 0.4015,  ..., 0.5147, 0.5814, 0.6765],\n",
              "         ...,\n",
              "         [0.5484, 0.4763, 0.2169,  ..., 0.3857, 0.5712, 0.1784],\n",
              "         [0.0561, 0.3600, 0.4560,  ..., 0.3492, 0.6297, 0.5058],\n",
              "         [0.9329, 0.6100, 0.9567,  ..., 0.7614, 0.1042, 0.7839]]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.rand(32, 20, 128)  # batch_size, seq_len, dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 딥러닝의 학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### Loss Function (손실함수, 목적함수)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*199FRpu1Q4isjnpCbs6hQQ.jpeg\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "모델 예측 값과 실제 정답의 차이를 비교하기 위한 함수로 학습 중에 모델의 성능을 확인하기 위한 함수로써 최적화(Optimization)를 위해 **최소화**하는 것이 목적인 함수\n",
        "\n",
        "목적에 따라 다양한 종류의 loss 존재\n",
        "\n",
        "- 회귀\n",
        "    - MAE (Mean Absolute Error) $$ \\frac {1}{n} \\Sigma_{i=1}^n|\\hat{y_i} - y_i| $$\n",
        "        - 이상치에 덜 예민\n",
        "    - MSE (Mean Squared Error) $$ \\frac {1}{n} \\Sigma_{i=1}^n(\\hat{y_i} - y_i)^2 $$\n",
        "        - 오차가 커질수록 loss가 빠르게 증가\n",
        "        - 이상치에 예민\n",
        "    - RMSE (Root Mean Squared Error) $$ \\sqrt{\\frac {1}{n} \\Sigma_{i=1}^n(\\hat{y_i} - y_i)^2} $$\n",
        "\n",
        "- 분류\n",
        "    - BCE (Binary Cross-Entropy) $$ -\\frac{1}{n} \\Sigma^n_{i=1}{y_i*log(\\hat{y_i}) + (1-y_i)*log(1-\\hat{y_i})} $$\n",
        "        - 이진 분류 문제에서 사용\n",
        "        - $ \\hat{y} $의 값은 0과 1사이의 확률 값 (logit)\n",
        "        - $ \\hat{y} $이 1에 가까우면 True일 확률이 크고, 0에 가까우면 False 확률이 큼\n",
        "    - CE (Categorical Cross_Entropy) $$ -\\frac{1}{n} \\Sigma^n_{i=1}\\Sigma^C_{i=1} y_{ic} * log(\\hat{y_{ic}})\\ \\ \\ \\text{where c: num of class} $$ \n",
        "        - class의 수가 3 개 이상일 때 사용\n",
        "        - $ \\hat{y} $의 값은 0과 1사이의 확률 값 (logit)\n",
        "        - $ y $는 one-hot encoding된 상태\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Descent (경사하강법)\n",
        "\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "**비용 함수(Cost Function)의 비용을 최소화** 하는 파라미터를 찾는 알고리즘 <br>\n",
        "위의 그림에서 최솟값을 찾아갈 때 어떻게 찾아갈지에 대한 방법 <br>\n",
        "학습 시 얼마 만큼 이동할 것인가를 **learning rate ($\\alpha$), 학습률** 라고 부름 <br>\n",
        "\n",
        "    너무 작은은 learning rate는 느린 수렴 유발 및 local minia 문제 야기\n",
        "    너무 큰 learning rate는 gradient explode 유발\n",
        "\n",
        "<br>\n",
        "\n",
        "**Local Minima** <br>\n",
        "\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2020/09/local-minima-vs-global-minima-1.png\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "우리가 최소화하고자 하는 목적함수에서 국소적 최솟값을 갖는 것을 local minima라고 부름 <br>\n",
        "local minima를 global minima로 인식하여 학습이 종료되는 경우가 발생할 수 있음 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer (최적화 알고리즘)\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAH58V%2FbtqDyD7qEj4%2FK7hDnh5zzjfgwkI9wWTmeK%2Fimg.png\" width=\"700\" height=\"300\"/>\n",
        "\n",
        "목적함수를 최소화시키기 위한 알고리즘 <br>\n",
        "위에 기술된 내용 외에도 다양한 알고리즘이 존재하나 대다수 **Adam** 사용 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Batch Gradient Descent (GD)\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images%2Fyelim421%2Fpost%2Ff661eaa4-f2ea-4df6-88b2-9234f065a8d6%2Fimage.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "전체 데이터셋을 사용하여 비용 함수의 기울기를 계산\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>:\n",
        "\n",
        "$\\theta_j := \\theta_j - \\eta \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "#### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images%2Fyelim421%2Fpost%2F0c873df1-a8d5-451d-94b4-dc11d9c8f3d6%2Fimage.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "한 번에 하나 또는 일부의 훈련 샘플을 사용하여 그라디언트를 계산하고 매개 변수를 업데이트 <br>\n",
        "계산 비용을 절감하며, 비선형 최적화 문제에 대한 솔루션을 빠르게 근사 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### mini-Batch Gradient Descent\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images%2Fyelim421%2Fpost%2F372d0704-d5b8-45b4-a7c9-61e9c060a37c%2Fimage.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "\n",
        "전체 학습 데이터를 mini-batch로 나누어 gradient descent 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Momentum\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images%2Fyelim421%2Fpost%2F366fa59a-cd67-4545-b56c-69f4aa0c1ac4%2Fimage.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "\n",
        "물리의 속도 개념을 도입하여 기울기 방향으로 가속화된 움직임을 보임\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>:\n",
        "\n",
        "$\\nu_j := \\alpha\\nu_j - \\eta \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $, where $\\alpha$: momentum factor <br>\n",
        "$\\theta_j = \\theta_j + \\nu_j$ <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AdaGrad\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBoGBQ%2Fbtq0qAPFDtU%2FdP4jbshcOtjK2SgBIWlQlK%2Fimg.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "학습 진행 중 learning rate를 감소시키는 방법 (처음에는 크게 학습하다가 점점 작게 학습) <br>\n",
        "각 매개변수 별로 learning rate를 adaptive하게 조절\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>:\n",
        "\n",
        "$h_j = h_j + \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\odot \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $ <br>\n",
        "$\\theta_j = \\theta_j - \\eta \\frac{1}{\\sqrt{h}} \\frac{1}{\\partial \\theta_j}J(\\theta)$ <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RMSProp\n",
        "\n",
        "과거 기울기의 제곱을 더하여 학습을 진행하는 AdaGrad의 단점을 보완한 기법 <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AdaGrad는 학습이 충분히 진행되면 갱신량이 0에 근사하여 파라미터가 갱신되지 않는 일이 발생 <br>\n",
        "먼 과거의 기울기는 조금, 가까운 과거의 기울기는 많이 반영하는 지수이동평균법 사용 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>:\n",
        "\n",
        "$h_{i, j} = \\rho h_{i, j} + (1 - \\rho) \\frac{\\partial}{\\partial \\theta_j}J(\\theta_i) \\odot \\frac{\\partial}{\\partial \\theta_j}J(\\theta_i) $ <br>\n",
        "$\\theta_j = \\theta_j - \\eta \\frac{1}{\\sqrt{h}} \\frac{1}{\\partial \\theta_j}J(\\theta)$ <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Adam\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrWQXG%2Fbtq0zcMR6wE%2FnbuT43rcJL9aR8Ory8xZS1%2Fimg.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "Momentum + RMSProp\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>:\n",
        "\n",
        "1st momentum: $m_j = \\beta_1 m_j + (1 - \\beta_1) \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ <br>\n",
        "2nd momentum: $v_j = \\beta_2 v_j + (1 - \\beta_2) \\left(\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\\right)^2$ <br>\n",
        "Bias correction: $ \\hat{m}_j = \\frac{m_j}{1 - \\beta_1^t} $, $ \\hat{v}_j = \\frac{v_j}{1 - \\beta_2^t} $ <br>\n",
        "Update: $ \\theta_j = \\theta_j - \\eta \\frac{\\hat{m}_j}{\\sqrt{\\hat{v}_j} + \\epsilon}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BackPropagation (역전파)\n",
        "\n",
        "<img src=\"https://media.licdn.com/dms/image/D5612AQGNjUevxbUE_A/article-cover_image-shrink_720_1280/0/1677211887007?e=2147483647&v=beta&t=cwgMLvP3kROyG-XNVzrNO6mBV52uHeNYLKFjPDcuhUI\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "인공 신경망이 순전파 과정을 진행하여 예측값과 실제값의 오차를 계산 <br>\n",
        "이후 경사 하강법을 사용하여 가중치 업데이트를 진행하는데 이 과정을 역전파라고 부름 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "아래 순전파와 역전파의 예에서 사용되는 activation은 sigmoid\n",
        "\n",
        "sigmoid: $f(x)$ = $ {\\displaystyle {\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{1+e^{x}}}}$ \n",
        "\n",
        "$ {\\begin{aligned}{\\frac {\\mathrm {d} }{\\mathrm {d} x}}f(x)&={\\frac {e^{x}\\cdot (1+e^{x})-e^{x}\\cdot e^{x}}{(1+e^{x})^{2}}}\\\\&={\\frac {e^{x}}{(1+e^{x})^{2}}}\\\\&=\\left({\\frac {e^{x}}{1+e^{x}}}\\right)\\left({\\frac {1}{1+e^{x}}}\\right)\\\\&=\\left({\\frac {e^{x}}{1+e^{x}}}\\right)\\left(1-{\\frac {e^{x}}{1+e^{x}}}\\right)\\\\&=f(x)\\left(1-f(x)\\right)\\end{aligned}} $\n",
        "\n",
        "<br>\n",
        "\n",
        "**Forward Propagation (순전파)**\n",
        "\n",
        "![](https://wikidocs.net/images/page/37406/backpropagation_2.PNG)\n",
        "\n",
        "모델 입력부터 output까지의 과정 \n",
        "\n",
        "$z_{1}=w_{1}x_{1} + w_{2}x_{2}=0.3 \\text{×} 0.1 + 0.25 \\text{×} 0.2= 0.08$ <br>\n",
        "$z_{2}=w_{3}x_{1} + w_{4}x_{2}=0.4 \\text{×} 0.1 + 0.35 \\text{×} 0.2= 0.11$\n",
        "\n",
        "$h_{1}=sigmoid(z_{1}) = 0.51998934$ <br>\n",
        "$h_{2}=sigmoid(z_{2}) = 0.52747230$\n",
        "\n",
        "$z_{3}=w_{5}h_{1}+w_{6}h_{2} = 0.45 \\text{×} h_{1} + 0.4 \\text{×} h_{2} = 0.44498412$ <br>\n",
        "$z_{4}=w_{7}h_{1}+w_{8}h_{2} = 0.7 \\text{×} h_{1} + 0.6 \\text{×} h_{2} = 0.68047592$\n",
        "\n",
        "$o_{1}=sigmoid(z_{3})=0.60944600$ <br>\n",
        "$o_{2}=sigmoid(z_{4})=0.66384491$ \n",
        "\n",
        "$E_{o1}=\\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02193381$ <br>\n",
        "$E_{o2}=\\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00203809$\n",
        "\n",
        "$E_{total}=E_{o1}+E_{o2}=0.02397190$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Backward Propagation (역전파)** <br>\n",
        "\n",
        "![](https://wikidocs.net/images/page/37406/backpropagation_3.PNG)\n",
        "\n",
        "loss 계산 이후 parameter가 업데이트되는 과정 <br>\n",
        "$w_5$의 가중치 업데이트를 위해서는 아래의 식 계산 <br>\n",
        "\n",
        "$\\frac{∂E_{total}}{∂w_{5}} = \\frac{∂E_{total}}{∂o_{1}} \\text{×} \\frac{∂o_{1}}{∂z_{3}} \\text{×} \\frac{∂z_{3}}{∂w_{5}}$\n",
        "\n",
        "$\\frac{∂E_{total}}{∂o_{1}}=2 \\text{×} \\frac{1}{2}(target_{o1}-output_{o1})^{2-1} \\text{×} (-1) + 0$\n",
        "\n",
        "$\\frac{∂E_{total}}{∂o_{1}}=-(target_{o1}-output_{o1})=-(0.4-0.60944600)=0.20944600$\n",
        "\n",
        "$\\frac{∂o_{1}}{∂z_{3}}=o_{1}\\text{×}(1-o_{1})=0.60944600(1-0.60944600)=0.23802157$\n",
        "\n",
        "$\\frac{∂z_{3}}{∂w_{5}}=h_{1}=0.51998934$\n",
        "\n",
        "$\\frac{∂E_{total}}{∂w_{5}} = 0.20944600 \\text{×} 0.23802157 \\text{×} 0.51998934 = 0.02592286$\n",
        "\n",
        "$w_{5}^{+}=w_{5}-α\\frac{∂E_{total}}{∂w_{5}}=0.45- 0.5 \\text{×} 0.02592286=0.43703857$\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://wikidocs.net/images/page/37406/backpropagation_4.PNG)\n",
        "\n",
        "이후 위와 동일한 연산으로 첫 번째 $w_1 \\sim w_4$에 대해서 업데이트 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7423, 0.5263],\n",
            "        [0.2437, 0.5846],\n",
            "        [0.0332, 0.1387]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "X = torch.randint(0, 10, (1, 4), dtype=torch.float32)\n",
        "A1 = torch.rand(4, 5)\n",
        "h1 = X@A1 # intercept\n",
        "A2 = torch.rand(5, 3)\n",
        "h2 = h1@A2\n",
        "A3 = torch.rand(3, 2)\n",
        "h3 = h2@A3\n",
        "h3\n",
        "print(A3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7411, 0.5251],\n",
              "        [0.2425, 0.5834],\n",
              "        [0.0320, 0.1375]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "gradient_loss = 1.2\n",
        "A3 -= learning_rate*gradient_loss\n",
        "print(A3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### Activation Function (활성화함수)\n",
        "\n",
        "<font style=\"font-size:20px\"> Affine Transformation</font> <p>\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dn/J4OoB/btrM0mVSwuL/z3QA8bQK5kmBMJr8qZmvxK/img.gif\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "직선과 평형성을 유지하는 변환 <br>\n",
        "Affine 변환의 식은 일반적으로 $Wx + b$이며 이는 선형 변환에 translation(절편) 항이 추가된 변환\n",
        "변환에 의해 모든 점이 이동하게 되며, 이경우 변환의 역할을 공간을 변형시키는 행위라고도 생각할 수있음 <br>\n",
        "    -> 공간의 변형으로 기존 공간에서는 불가능 했던 작업을 가능하게 할 수 있을 것이라는 기대\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FsHi41%2FbtrM4RAYfiK%2FGwq4hqLHs19b7Zocymmp9k%2Fimg.png\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "위의 예에서 original space에서 색을 구분할 수 있는 직선은 존재하지 않음 <br>\n",
        "하지만 공간을 변형하면, 하나의 선분으로 색을 구분할 수 있는 직선을 찾을 수 있음 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> Affine Transformation과 Deep Learning </font> <p>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcvHNEe%2FbtrMWeqUO3A%2FW6zKm5hoi2Vdqq9IhLfpcK%2Fimg.png\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "딥러닝의 각 퍼셉트론(노드)의 가중치는 Wx+b와 나타낼 수 있으며 이를 위의 변환이라고 볼 수 있음 <br>\n",
        "변환 전에 직선으로 구분 불가했던 각 단어의 영역이 변환 후에는 직선으로 쉽게 구분 가능 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJpa9b%2FbtrMUMaOsRf%2Fl4EZRDQUVzLX2SPaALDY50%2Fimg.png\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "하지만 단순 affine transformation 만으로 공간 내의 모든 값을 구분할 수 없음 <br>\n",
        "위의 예를 보면 변환 이후에 색을 구분할 수 있는 직선을 찾을 수 없음 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhJwZr%2FbtrM3aOlIOE%2F155I6mjMdMIVanvN6oduBK%2Fimg.png\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "따라서 위와 같이 변환된 공간에 어떠한 적절한 함수가 있어서 우측 그림과 같이 정리해줄 수 있다면 색을 구분할 수 있는 하나의 직선을 찾을 수 있음 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmqGSf%2FbtrM3at2fBT%2FE5pB1gpZv7UPUsubxFI1qk%2Fimg.png\" width=\"500\" height=\"250\"/>\n",
        "\n",
        "실제로 위의 함수는 $max(0, x)$의 식은 ReLU 함수를 이용하여 얻을 수 있음 <br>\n",
        "**Affine 변환에서 부족한 부분을 활성화 함수를 통하여 표현** <br>\n",
        "이로 더 복잡한 경계면을 표현할 수 있고, 이로 더 복잡한 문제 해결 가능성 제공 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dn/vSQuk/btrMZDctFNy/K9uXwX1zHcYPtxUWcq8Lmk/img.gif\" width=\"300\" height=\"250\"/>\n",
        "\n",
        "실제로 위의 그림은 tanh 함수가 적용된 것으로 공간이 급격히 왜곡되는 부분에서 활성화 함수가 적용된 것 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<font style=\"font-size:20px\"> 노드 수가 공간에 미치는 영향 </font> <p>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fq3R6r%2FbtrMXkLaCUK%2FDtMnLTrJfE8LKwAffriRKK%2Fimg.png\" width=\"500\" height=\"300\"/>\n",
        "\n",
        "layer의 unit(node)수가 증가할수록 더욱 복잡하게 경계면을 분리할 수 있음 <br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "|unit=4|unit=2|\n",
        "|-|-|\n",
        "|<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FINZai%2FbtrMXIrDAW2%2FIzGUZyb3zGKiMGkKZd0KD0%2Fimg.gif\" width=\"500\" height=\"300\"/>|<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzUmiV%2FbtrM0oMYADr%2FR5nxmQWePJh8LIiLFPeepk%2Fimg.gif\" width=\"500\" height=\"300\"/>|\n",
        "\n",
        "\n",
        "좌측과의 경우 공간을 변형시키는 과정이 다름 <br>\n",
        "최종적으로는 좌측은 직선으로 공간 분리에 성공하지만 우측은 실패 <br>\n",
        "적은 노드 수보다 많은 노드 수에서 복잡한 결정 경계를 표현할 수 있음을 보임 <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Activation Functions\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbeswMt%2FbtqYpV2m5DU%2Fvqv8wX4oRhlM99eqhQIRx0%2Fimg.png\" width=\"600\" height=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Sigmoid\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "x가 클수록 1에 가까운 숫자로, 작을수록 0에 가까운 숫자로 변환시키는 활성화 함수 <br>\n",
        "-> 0 ~ 1까지의 비선형 형태로 변경하기 위한 함수 <br>\n",
        "기울기 소실 문제 발생 <br>\n",
        "함수의 중심이 0이 아님 <br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>: $\\frac{1}{1+e^{-x}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tanh\n",
        "\n",
        "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "x가 클수록 1에 가까운 숫자로, 작을수록 -1에 가까운 숫자 출력 <br>\n",
        "-> -1 ~ 1까지의 비선형 형태로 변경하기 위한 함수 <br>\n",
        "기울기 소실 문제 발생 <br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>: $\\frac{e^x-e^{-x}}{e^x+e^{-x}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "##### Rectified Linear Unit (ReLU)\n",
        "\n",
        "<img src=\"https://i.imgur.com/gKA4kA9.jpg\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "입력이 0이상이면 입력을 그대로 출력하고, 0 이하이면 0을 출력 <br>\n",
        "기울기 계산이 간단 <br>\n",
        "0보다 작은 값의 손실 문제 발생 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>: $\\max(0, x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Leaky ReLU\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "입력이 0이상이면 입력을 그대로 출력하고, 0 이하이면 낮은 기울기를 출력 <br>\n",
        "기울기 계산이 간단 <br>\n",
        "0보다 작은 값의 손실 문제 해결 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>: $\\begin{cases} \n",
        "                    \\max(0, x) & \\text{if } x \\gt 0 \\\\\n",
        "                    ax & \\text{if } x \\leq 0 \n",
        "                    \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Exponential Linear Unit (ELU)\n",
        "\n",
        "<img src=\"https://pytorch.org/docs/stable/_images/ELU.png\" width=\"300\" height=\"200\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "입력이 0이상이면 입력을 그대로 출력하고, 0 이하에서도 기울기 출력 <br>\n",
        "기울기 계산이 간단 <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Equation</b>: $\\begin{cases} \n",
        "                    x & \\text{if } x \\gt 0 \\\\\n",
        "                    \\alpha*(\\exp(x)-1) & \\text{if } x \\leq 0 \n",
        "                    \\end{cases}$\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
