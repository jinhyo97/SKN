{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1696d071790>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "reference: [Attention](https://wikidocs.net/22893)\n",
    "\n",
    "paper:\n",
    "- [Encoder-Decoder](https://arxiv.org/pdf/1406.1078)\n",
    "- [Attention (Loung et al.)](https://courses.grainger.illinois.edu/cs546/sp2018/Slides/Mar15_Luong.pdf)\n",
    "- [Attention (Bahdanau et al.)](https://formacion.actuarios.org/wp-content/uploads/2024/05/1409.0473-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.w_a = nn.Linear(self.hidden_dim, 1)\n",
    "        self.w_b = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.w_c = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        if query.dim() == 2:                # batch, dim\n",
    "            query = query[:, np.newaxis, :] # batch, 1, dim\n",
    "\n",
    "        temp = F.tanh(self.w_b(query) + self.w_c(key))      # batch, seq_len, dim\n",
    "        score = self.w_a(temp)                              # batch, seq_len, 1\n",
    "        attention_distribution = F.softmax(score, dim=1)    # batch, seq_len, 1\n",
    "\n",
    "        context_vector = (attention_distribution*value).sum(axis=1) # batch, dim\n",
    "\n",
    "        return context_vector, attention_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2325,  0.3472,  0.1849,  ...,  0.0278,  0.4824,  0.4273],\n",
       "         [ 0.0584,  0.0685,  0.3575,  ..., -0.0513, -0.6138,  0.1850],\n",
       "         [-0.0502, -0.1747, -0.1399,  ..., -0.2725,  0.0506, -0.2972],\n",
       "         ...,\n",
       "         [-0.1104, -0.2626,  0.0837,  ..., -0.4097,  0.0690, -0.1299],\n",
       "         [ 0.0584, -0.0553, -0.3293,  ...,  0.2993,  0.0803, -0.0207],\n",
       "         [-0.2430,  0.1390,  0.0496,  ...,  0.3994, -0.2173, -0.2490]],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([[[0.0437],\n",
       "          [0.0418],\n",
       "          [0.0413],\n",
       "          [0.0529],\n",
       "          [0.0458],\n",
       "          [0.0605],\n",
       "          [0.0435],\n",
       "          [0.0595],\n",
       "          [0.0403],\n",
       "          [0.0638],\n",
       "          [0.0392],\n",
       "          [0.0488],\n",
       "          [0.0623],\n",
       "          [0.0447],\n",
       "          [0.0469],\n",
       "          [0.0784],\n",
       "          [0.0461],\n",
       "          [0.0455],\n",
       "          [0.0496],\n",
       "          [0.0454]],\n",
       " \n",
       "         [[0.0396],\n",
       "          [0.0611],\n",
       "          [0.0482],\n",
       "          [0.0634],\n",
       "          [0.0361],\n",
       "          [0.0441],\n",
       "          [0.0567],\n",
       "          [0.0474],\n",
       "          [0.0587],\n",
       "          [0.0579],\n",
       "          [0.0671],\n",
       "          [0.0464],\n",
       "          [0.0548],\n",
       "          [0.0421],\n",
       "          [0.0271],\n",
       "          [0.0442],\n",
       "          [0.0714],\n",
       "          [0.0361],\n",
       "          [0.0566],\n",
       "          [0.0411]],\n",
       " \n",
       "         [[0.0566],\n",
       "          [0.0499],\n",
       "          [0.0443],\n",
       "          [0.0569],\n",
       "          [0.0539],\n",
       "          [0.0373],\n",
       "          [0.0516],\n",
       "          [0.0380],\n",
       "          [0.0526],\n",
       "          [0.0522],\n",
       "          [0.0555],\n",
       "          [0.0511],\n",
       "          [0.0377],\n",
       "          [0.0386],\n",
       "          [0.0652],\n",
       "          [0.0488],\n",
       "          [0.0748],\n",
       "          [0.0472],\n",
       "          [0.0456],\n",
       "          [0.0424]],\n",
       " \n",
       "         [[0.0538],\n",
       "          [0.0280],\n",
       "          [0.0731],\n",
       "          [0.0414],\n",
       "          [0.0515],\n",
       "          [0.0449],\n",
       "          [0.0363],\n",
       "          [0.0411],\n",
       "          [0.0684],\n",
       "          [0.0395],\n",
       "          [0.0492],\n",
       "          [0.0506],\n",
       "          [0.0388],\n",
       "          [0.0574],\n",
       "          [0.0380],\n",
       "          [0.0544],\n",
       "          [0.0545],\n",
       "          [0.0604],\n",
       "          [0.0425],\n",
       "          [0.0762]],\n",
       " \n",
       "         [[0.0516],\n",
       "          [0.0521],\n",
       "          [0.0521],\n",
       "          [0.0769],\n",
       "          [0.0744],\n",
       "          [0.0399],\n",
       "          [0.0421],\n",
       "          [0.0339],\n",
       "          [0.0476],\n",
       "          [0.0675],\n",
       "          [0.0357],\n",
       "          [0.0438],\n",
       "          [0.0537],\n",
       "          [0.0441],\n",
       "          [0.0401],\n",
       "          [0.0437],\n",
       "          [0.0480],\n",
       "          [0.0602],\n",
       "          [0.0533],\n",
       "          [0.0391]],\n",
       " \n",
       "         [[0.0403],\n",
       "          [0.0537],\n",
       "          [0.0514],\n",
       "          [0.0526],\n",
       "          [0.0442],\n",
       "          [0.0419],\n",
       "          [0.0600],\n",
       "          [0.0446],\n",
       "          [0.0417],\n",
       "          [0.0774],\n",
       "          [0.0485],\n",
       "          [0.0398],\n",
       "          [0.0454],\n",
       "          [0.0600],\n",
       "          [0.0434],\n",
       "          [0.0370],\n",
       "          [0.0617],\n",
       "          [0.0593],\n",
       "          [0.0537],\n",
       "          [0.0435]],\n",
       " \n",
       "         [[0.0535],\n",
       "          [0.0489],\n",
       "          [0.0409],\n",
       "          [0.0527],\n",
       "          [0.0390],\n",
       "          [0.0521],\n",
       "          [0.0593],\n",
       "          [0.0540],\n",
       "          [0.0431],\n",
       "          [0.0614],\n",
       "          [0.0346],\n",
       "          [0.0637],\n",
       "          [0.0566],\n",
       "          [0.0546],\n",
       "          [0.0312],\n",
       "          [0.0691],\n",
       "          [0.0387],\n",
       "          [0.0433],\n",
       "          [0.0552],\n",
       "          [0.0481]],\n",
       " \n",
       "         [[0.0586],\n",
       "          [0.0493],\n",
       "          [0.0591],\n",
       "          [0.0429],\n",
       "          [0.0610],\n",
       "          [0.0407],\n",
       "          [0.0486],\n",
       "          [0.0392],\n",
       "          [0.0650],\n",
       "          [0.0655],\n",
       "          [0.0407],\n",
       "          [0.0462],\n",
       "          [0.0537],\n",
       "          [0.0392],\n",
       "          [0.0486],\n",
       "          [0.0447],\n",
       "          [0.0558],\n",
       "          [0.0379],\n",
       "          [0.0427],\n",
       "          [0.0605]],\n",
       " \n",
       "         [[0.0516],\n",
       "          [0.0569],\n",
       "          [0.0527],\n",
       "          [0.0418],\n",
       "          [0.0426],\n",
       "          [0.0477],\n",
       "          [0.0508],\n",
       "          [0.0441],\n",
       "          [0.0557],\n",
       "          [0.0490],\n",
       "          [0.0614],\n",
       "          [0.0589],\n",
       "          [0.0530],\n",
       "          [0.0484],\n",
       "          [0.0347],\n",
       "          [0.0417],\n",
       "          [0.0589],\n",
       "          [0.0526],\n",
       "          [0.0449],\n",
       "          [0.0528]],\n",
       " \n",
       "         [[0.0602],\n",
       "          [0.0403],\n",
       "          [0.0620],\n",
       "          [0.0572],\n",
       "          [0.0553],\n",
       "          [0.0591],\n",
       "          [0.0479],\n",
       "          [0.0381],\n",
       "          [0.0335],\n",
       "          [0.0541],\n",
       "          [0.0569],\n",
       "          [0.0684],\n",
       "          [0.0317],\n",
       "          [0.0553],\n",
       "          [0.0542],\n",
       "          [0.0518],\n",
       "          [0.0400],\n",
       "          [0.0419],\n",
       "          [0.0394],\n",
       "          [0.0528]],\n",
       " \n",
       "         [[0.0754],\n",
       "          [0.0604],\n",
       "          [0.0475],\n",
       "          [0.0446],\n",
       "          [0.0312],\n",
       "          [0.0331],\n",
       "          [0.0637],\n",
       "          [0.0864],\n",
       "          [0.0287],\n",
       "          [0.0565],\n",
       "          [0.0512],\n",
       "          [0.0465],\n",
       "          [0.0372],\n",
       "          [0.0556],\n",
       "          [0.0392],\n",
       "          [0.0351],\n",
       "          [0.0430],\n",
       "          [0.0530],\n",
       "          [0.0486],\n",
       "          [0.0629]],\n",
       " \n",
       "         [[0.0563],\n",
       "          [0.0643],\n",
       "          [0.0433],\n",
       "          [0.0636],\n",
       "          [0.0512],\n",
       "          [0.0505],\n",
       "          [0.0492],\n",
       "          [0.0430],\n",
       "          [0.0403],\n",
       "          [0.0444],\n",
       "          [0.0358],\n",
       "          [0.0481],\n",
       "          [0.0757],\n",
       "          [0.0437],\n",
       "          [0.0485],\n",
       "          [0.0426],\n",
       "          [0.0451],\n",
       "          [0.0638],\n",
       "          [0.0469],\n",
       "          [0.0439]],\n",
       " \n",
       "         [[0.0340],\n",
       "          [0.0385],\n",
       "          [0.0621],\n",
       "          [0.0471],\n",
       "          [0.0684],\n",
       "          [0.0635],\n",
       "          [0.0428],\n",
       "          [0.0602],\n",
       "          [0.0512],\n",
       "          [0.0452],\n",
       "          [0.0414],\n",
       "          [0.0489],\n",
       "          [0.0371],\n",
       "          [0.0333],\n",
       "          [0.0549],\n",
       "          [0.0608],\n",
       "          [0.0521],\n",
       "          [0.0543],\n",
       "          [0.0468],\n",
       "          [0.0574]],\n",
       " \n",
       "         [[0.0547],\n",
       "          [0.0435],\n",
       "          [0.0390],\n",
       "          [0.0610],\n",
       "          [0.0245],\n",
       "          [0.0509],\n",
       "          [0.0446],\n",
       "          [0.0632],\n",
       "          [0.0460],\n",
       "          [0.0608],\n",
       "          [0.0575],\n",
       "          [0.0571],\n",
       "          [0.0467],\n",
       "          [0.0636],\n",
       "          [0.0605],\n",
       "          [0.0689],\n",
       "          [0.0444],\n",
       "          [0.0409],\n",
       "          [0.0354],\n",
       "          [0.0367]],\n",
       " \n",
       "         [[0.0478],\n",
       "          [0.0595],\n",
       "          [0.0451],\n",
       "          [0.0368],\n",
       "          [0.0491],\n",
       "          [0.0446],\n",
       "          [0.0547],\n",
       "          [0.0487],\n",
       "          [0.0392],\n",
       "          [0.0565],\n",
       "          [0.0437],\n",
       "          [0.0496],\n",
       "          [0.0321],\n",
       "          [0.0401],\n",
       "          [0.0774],\n",
       "          [0.0555],\n",
       "          [0.0507],\n",
       "          [0.0520],\n",
       "          [0.0523],\n",
       "          [0.0646]],\n",
       " \n",
       "         [[0.0478],\n",
       "          [0.0254],\n",
       "          [0.0338],\n",
       "          [0.0595],\n",
       "          [0.0474],\n",
       "          [0.0524],\n",
       "          [0.0387],\n",
       "          [0.0763],\n",
       "          [0.0688],\n",
       "          [0.0564],\n",
       "          [0.0502],\n",
       "          [0.0372],\n",
       "          [0.0436],\n",
       "          [0.0461],\n",
       "          [0.0439],\n",
       "          [0.0504],\n",
       "          [0.0606],\n",
       "          [0.0499],\n",
       "          [0.0556],\n",
       "          [0.0558]],\n",
       " \n",
       "         [[0.0486],\n",
       "          [0.0503],\n",
       "          [0.0561],\n",
       "          [0.0398],\n",
       "          [0.0442],\n",
       "          [0.0460],\n",
       "          [0.0547],\n",
       "          [0.0483],\n",
       "          [0.0403],\n",
       "          [0.0380],\n",
       "          [0.0552],\n",
       "          [0.0568],\n",
       "          [0.0456],\n",
       "          [0.0571],\n",
       "          [0.0615],\n",
       "          [0.0575],\n",
       "          [0.0526],\n",
       "          [0.0422],\n",
       "          [0.0447],\n",
       "          [0.0606]],\n",
       " \n",
       "         [[0.0349],\n",
       "          [0.0565],\n",
       "          [0.0404],\n",
       "          [0.0469],\n",
       "          [0.0682],\n",
       "          [0.0508],\n",
       "          [0.0472],\n",
       "          [0.0587],\n",
       "          [0.0448],\n",
       "          [0.0503],\n",
       "          [0.0596],\n",
       "          [0.0454],\n",
       "          [0.0445],\n",
       "          [0.0550],\n",
       "          [0.0562],\n",
       "          [0.0568],\n",
       "          [0.0462],\n",
       "          [0.0434],\n",
       "          [0.0522],\n",
       "          [0.0419]],\n",
       " \n",
       "         [[0.0548],\n",
       "          [0.0524],\n",
       "          [0.0520],\n",
       "          [0.0434],\n",
       "          [0.0522],\n",
       "          [0.0363],\n",
       "          [0.0374],\n",
       "          [0.0414],\n",
       "          [0.0458],\n",
       "          [0.0360],\n",
       "          [0.0356],\n",
       "          [0.0605],\n",
       "          [0.0658],\n",
       "          [0.0297],\n",
       "          [0.0477],\n",
       "          [0.0685],\n",
       "          [0.0516],\n",
       "          [0.0612],\n",
       "          [0.0475],\n",
       "          [0.0802]],\n",
       " \n",
       "         [[0.0576],\n",
       "          [0.0484],\n",
       "          [0.0792],\n",
       "          [0.0408],\n",
       "          [0.0303],\n",
       "          [0.0303],\n",
       "          [0.0436],\n",
       "          [0.0392],\n",
       "          [0.0471],\n",
       "          [0.0434],\n",
       "          [0.0403],\n",
       "          [0.0577],\n",
       "          [0.0512],\n",
       "          [0.0610],\n",
       "          [0.0476],\n",
       "          [0.0462],\n",
       "          [0.0558],\n",
       "          [0.0622],\n",
       "          [0.0513],\n",
       "          [0.0668]],\n",
       " \n",
       "         [[0.0425],\n",
       "          [0.0450],\n",
       "          [0.0385],\n",
       "          [0.0406],\n",
       "          [0.0553],\n",
       "          [0.0458],\n",
       "          [0.0548],\n",
       "          [0.0399],\n",
       "          [0.0569],\n",
       "          [0.0443],\n",
       "          [0.0626],\n",
       "          [0.0482],\n",
       "          [0.0507],\n",
       "          [0.0702],\n",
       "          [0.0389],\n",
       "          [0.0506],\n",
       "          [0.0515],\n",
       "          [0.0483],\n",
       "          [0.0708],\n",
       "          [0.0446]],\n",
       " \n",
       "         [[0.0447],\n",
       "          [0.0457],\n",
       "          [0.0435],\n",
       "          [0.0631],\n",
       "          [0.0528],\n",
       "          [0.0536],\n",
       "          [0.0419],\n",
       "          [0.0539],\n",
       "          [0.0607],\n",
       "          [0.0560],\n",
       "          [0.0509],\n",
       "          [0.0563],\n",
       "          [0.0377],\n",
       "          [0.0430],\n",
       "          [0.0339],\n",
       "          [0.0504],\n",
       "          [0.0544],\n",
       "          [0.0449],\n",
       "          [0.0497],\n",
       "          [0.0631]],\n",
       " \n",
       "         [[0.0465],\n",
       "          [0.0466],\n",
       "          [0.0354],\n",
       "          [0.0437],\n",
       "          [0.0427],\n",
       "          [0.0649],\n",
       "          [0.0346],\n",
       "          [0.0413],\n",
       "          [0.0582],\n",
       "          [0.0459],\n",
       "          [0.0845],\n",
       "          [0.0507],\n",
       "          [0.0455],\n",
       "          [0.0570],\n",
       "          [0.0567],\n",
       "          [0.0546],\n",
       "          [0.0445],\n",
       "          [0.0453],\n",
       "          [0.0617],\n",
       "          [0.0397]],\n",
       " \n",
       "         [[0.0389],\n",
       "          [0.0453],\n",
       "          [0.0531],\n",
       "          [0.0406],\n",
       "          [0.0476],\n",
       "          [0.0383],\n",
       "          [0.0559],\n",
       "          [0.0586],\n",
       "          [0.0403],\n",
       "          [0.0619],\n",
       "          [0.0514],\n",
       "          [0.0615],\n",
       "          [0.0318],\n",
       "          [0.0779],\n",
       "          [0.0399],\n",
       "          [0.0622],\n",
       "          [0.0460],\n",
       "          [0.0519],\n",
       "          [0.0428],\n",
       "          [0.0542]],\n",
       " \n",
       "         [[0.0319],\n",
       "          [0.0383],\n",
       "          [0.0404],\n",
       "          [0.0520],\n",
       "          [0.0355],\n",
       "          [0.0520],\n",
       "          [0.0418],\n",
       "          [0.0652],\n",
       "          [0.0774],\n",
       "          [0.0525],\n",
       "          [0.0427],\n",
       "          [0.0632],\n",
       "          [0.0533],\n",
       "          [0.0509],\n",
       "          [0.0558],\n",
       "          [0.0590],\n",
       "          [0.0478],\n",
       "          [0.0543],\n",
       "          [0.0347],\n",
       "          [0.0514]],\n",
       " \n",
       "         [[0.0579],\n",
       "          [0.0495],\n",
       "          [0.0368],\n",
       "          [0.0685],\n",
       "          [0.0484],\n",
       "          [0.0414],\n",
       "          [0.0536],\n",
       "          [0.0374],\n",
       "          [0.0661],\n",
       "          [0.0621],\n",
       "          [0.0336],\n",
       "          [0.0363],\n",
       "          [0.0506],\n",
       "          [0.0556],\n",
       "          [0.0601],\n",
       "          [0.0481],\n",
       "          [0.0475],\n",
       "          [0.0465],\n",
       "          [0.0499],\n",
       "          [0.0502]],\n",
       " \n",
       "         [[0.0421],\n",
       "          [0.0434],\n",
       "          [0.0461],\n",
       "          [0.0360],\n",
       "          [0.0590],\n",
       "          [0.0462],\n",
       "          [0.0660],\n",
       "          [0.0543],\n",
       "          [0.0522],\n",
       "          [0.0679],\n",
       "          [0.0419],\n",
       "          [0.0398],\n",
       "          [0.0582],\n",
       "          [0.0457],\n",
       "          [0.0675],\n",
       "          [0.0337],\n",
       "          [0.0702],\n",
       "          [0.0402],\n",
       "          [0.0389],\n",
       "          [0.0506]],\n",
       " \n",
       "         [[0.0498],\n",
       "          [0.0461],\n",
       "          [0.0523],\n",
       "          [0.0514],\n",
       "          [0.0501],\n",
       "          [0.0682],\n",
       "          [0.0485],\n",
       "          [0.0417],\n",
       "          [0.0489],\n",
       "          [0.0519],\n",
       "          [0.0482],\n",
       "          [0.0493],\n",
       "          [0.0499],\n",
       "          [0.0448],\n",
       "          [0.0342],\n",
       "          [0.0637],\n",
       "          [0.0569],\n",
       "          [0.0584],\n",
       "          [0.0433],\n",
       "          [0.0423]],\n",
       " \n",
       "         [[0.0731],\n",
       "          [0.0468],\n",
       "          [0.0374],\n",
       "          [0.0487],\n",
       "          [0.0380],\n",
       "          [0.0342],\n",
       "          [0.0582],\n",
       "          [0.0340],\n",
       "          [0.0396],\n",
       "          [0.0551],\n",
       "          [0.0536],\n",
       "          [0.0623],\n",
       "          [0.0502],\n",
       "          [0.0516],\n",
       "          [0.0447],\n",
       "          [0.0322],\n",
       "          [0.0722],\n",
       "          [0.0517],\n",
       "          [0.0738],\n",
       "          [0.0425]],\n",
       " \n",
       "         [[0.0460],\n",
       "          [0.0588],\n",
       "          [0.0689],\n",
       "          [0.0344],\n",
       "          [0.0500],\n",
       "          [0.0607],\n",
       "          [0.0476],\n",
       "          [0.0346],\n",
       "          [0.0555],\n",
       "          [0.0515],\n",
       "          [0.0412],\n",
       "          [0.0373],\n",
       "          [0.0405],\n",
       "          [0.0605],\n",
       "          [0.0480],\n",
       "          [0.0560],\n",
       "          [0.0314],\n",
       "          [0.0815],\n",
       "          [0.0544],\n",
       "          [0.0408]],\n",
       " \n",
       "         [[0.0404],\n",
       "          [0.0576],\n",
       "          [0.0445],\n",
       "          [0.0364],\n",
       "          [0.0501],\n",
       "          [0.0801],\n",
       "          [0.0387],\n",
       "          [0.0512],\n",
       "          [0.0434],\n",
       "          [0.0638],\n",
       "          [0.0614],\n",
       "          [0.0387],\n",
       "          [0.0599],\n",
       "          [0.0411],\n",
       "          [0.0594],\n",
       "          [0.0630],\n",
       "          [0.0414],\n",
       "          [0.0435],\n",
       "          [0.0369],\n",
       "          [0.0486]],\n",
       " \n",
       "         [[0.0639],\n",
       "          [0.0489],\n",
       "          [0.0481],\n",
       "          [0.0583],\n",
       "          [0.0523],\n",
       "          [0.0431],\n",
       "          [0.0437],\n",
       "          [0.0568],\n",
       "          [0.0482],\n",
       "          [0.0476],\n",
       "          [0.0476],\n",
       "          [0.0535],\n",
       "          [0.0487],\n",
       "          [0.0405],\n",
       "          [0.0455],\n",
       "          [0.0520],\n",
       "          [0.0439],\n",
       "          [0.0443],\n",
       "          [0.0569],\n",
       "          [0.0563]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn((32, 128))\n",
    "key = torch.randn((32, 20, 128))\n",
    "value = torch.randn((32, 20, 128))\n",
    "\n",
    "Attention(128)(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 클래스 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, latent_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 단어 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # LSTM 레이어 정의\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            self.latent_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 입력을 임베딩\n",
    "        x = self.embedding(x)\n",
    "        # LSTM을 통해 hidden state 얻기\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 디코더 클래스 정의\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # 단어 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention = Attention(self.latent_dim)\n",
    "\n",
    "        # 여러 개의 LSTM 레이어 정의\n",
    "        self.lstm1 = nn.LSTM(self.embedding_dim+self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        # 최종 출력 레이어\n",
    "        self.fc_out = nn.Linear(self.latent_dim, self.vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        hidden_state_decoder,\n",
    "        hidden_state_encoder,\n",
    "        hidden_state,\n",
    "        cell_state,\n",
    "        ):\n",
    "        x = x[:, np.newaxis]   # 차원 추가 (배치, 1)\n",
    "        x = self.embedding(x)  # 입력 임베딩\n",
    "\n",
    "        # attention을 통한 context vector\n",
    "        context_vector, _ = self.attention(\n",
    "            hidden_state_decoder,\n",
    "            hidden_state_encoder,\n",
    "            hidden_state_encoder,\n",
    "        )\n",
    "        context_vector = context_vector[:, np.newaxis, :]\n",
    "\n",
    "        # context vector와 x의 embedding 결합\n",
    "        x = torch.concat([x, context_vector], axis=-1)\n",
    "\n",
    "        # 여러 LSTM 레이어를 순차적으로 통과\n",
    "        x, _ = self.lstm1(x, (hidden_state, cell_state))\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, (h_n, c_n) = self.lstm4(x)  # 마지막 LSTM 레이어의 출력과 상태 반환\n",
    "        x = self.fc_out(x)  # 최종 출력 생성 (단어 확률 분포)\n",
    "\n",
    "        return x, (h_n, c_n)  # 출력 및 마지막 hidden state, cell state 반환\n",
    "\n",
    "\n",
    "# Seq2Seq 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio  # teacher forcing 비율 설정\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        batch_size = len(source)  # 배치 크기\n",
    "        target_length = target.shape[1]  # 목표 시퀀스의 길이\n",
    "        target_vocab_size = self.decoder.vocab_size  # 출력 어휘 크기\n",
    "        outputs = torch.zeros(batch_size, target_length, target_vocab_size)  # 출력을 저장할 텐서 초기화\n",
    "\n",
    "        # 인코더를 통해 잠재 벡터 생성\n",
    "        hidden_state_encoder = self.encoder(source)\n",
    "\n",
    "        # <SOS> token의 hidden state 구하기\n",
    "        x = target[:, 0]                # batch\n",
    "        x = x[:, np.newaxis]            # batch, 1\n",
    "        x = self.decoder.embedding(x)   # batch_size, 1, dim\n",
    "        context_vector = torch.zeros_like(x)\n",
    "        x = torch.concat([x, context_vector], axis=-1)\n",
    "        x, _ = self.decoder.lstm1(x)    # batch_size, 1, dim\n",
    "        x, _ = self.decoder.lstm2(x)    # batch_size, 1, dim\n",
    "        x, _ = self.decoder.lstm3(x)    # batch_size, 1, dim\n",
    "        x, (h_n, c_n) = self.decoder.lstm4(x)\n",
    "        hidden_state_decoder = h_n[0]\n",
    "\n",
    "        # 첫 번째 token (sos제외) 입력\n",
    "        x = target[:, 1]\n",
    "\n",
    "        # 목표 시퀀스의 각 타임스텝에 대해 반복\n",
    "        for t in range(1, target_length):\n",
    "            # 디코더를 통해 출력 및 다음 hidden/cell state 얻기\n",
    "            output, (h_n, c_n) = self.decoder(\n",
    "                x,\n",
    "                hidden_state_decoder,\n",
    "                hidden_state_encoder,\n",
    "                h_n,\n",
    "                c_n,\n",
    "            )\n",
    "            hidden_state_decoder = h_n[0]\n",
    "            outputs[:, t - 1, :] = output[:, 0, :]  # 현재 타임스텝의 출력 저장\n",
    "\n",
    "            # teacher forcing 사용 여부 결정\n",
    "            if (np.random.random() < self.teacher_forcing_ratio):\n",
    "                x = output[:, 0, :].argmax(axis=-1)  # 모델의 출력을 다음 입력으로 사용\n",
    "            else:\n",
    "                if t < target_length-2:\n",
    "                    x = target[:, t+1]  # 실제 타겟을 다음 입력으로 사용\n",
    "        \n",
    "        return outputs  # 모든 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9094, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 데이터 생성\n",
    "source = torch.randint(0, 1000, (32, 20))   # 0 ~ 999의 정수 값을 갖는 (batch_size, seq_len) 행렬 생성\n",
    "target = torch.randint(0, 1000, (32, 20))   # 0 ~ 999의 정수 값을 갖는 (batch_size, seq_len) 행렬 생성\n",
    "\n",
    "# 인코더 초기화\n",
    "encoder = Encoder(1000, 256, 256, 4)\n",
    "\n",
    "# 디코더 초기화\n",
    "decoder = Decoder(1000, 256, 256)\n",
    "\n",
    "# seq2seq 초기화\n",
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# cross entropy 사용 방법에 맞게 축 변환\n",
    "pred = seq2seq(source, target).permute(0, 2, 1) # (batch_size, seq_len, dim) -> (batch_size, dim, seq_len)\n",
    "\n",
    "# cross entropy loss 계산\n",
    "F.cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra\n",
       "0  Go.        Va !\n",
       "1  Go.     Marche.\n",
       "2  Go.  En route !\n",
       "3  Go.     Bouge !\n",
       "4  Hi.     Salut !"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 라이브러리를 사용하여 'fra.txt' 파일을 읽어온다.\n",
    "# 파일은 탭으로 구분되어 있으며, 헤더는 없음.\n",
    "data = pd.read_csv('./data/fra.txt', sep='\\t', header=None).iloc[:, :2]\n",
    "\n",
    "# 읽어온 데이터의 열 이름을 'eng'와 'fra'로 설정\n",
    "data.columns = ['eng', 'fra']\n",
    "\n",
    "# 데이터의 처음 5개 행을 출력\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accent(sentence):\n",
    "    '''주어진 문장에서 악센트(발음 기호)를 제거하는 함수'''\n",
    "    \n",
    "    # 'NFD' 형식으로 정규화하여 악센트를 분리\n",
    "    return ''.join(\n",
    "        char\n",
    "        for char in unicodedata.normalize('NFD', sentence)  # 문자를 NFD 형식으로 변환\n",
    "        if unicodedata.category(char) != 'Mn'  # 'Mn' (발음 기호) 범주에 속하지 않는 문자만 선택\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    '''문장을 전처리하는 함수'''\n",
    "    \n",
    "    # 1. 문장을 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 2. 악센트를 제거\n",
    "    sentence = remove_accent(sentence)\n",
    "    \n",
    "    # 3. 문장 부호(!, ?, .) 앞에 공백 추가\n",
    "    sentence = re.sub('([!,?.])', r' \\1', sentence)\n",
    "    \n",
    "    # 4. 여러 개의 공백을 하나의 공백으로 치환\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "\n",
    "    # 전처리된 문장을 반환\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232736/232736 [00:23<00:00, 10055.00it/s]\n",
      "100%|██████████| 232736/232736 [00:26<00:00, 8746.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 영어 문장에 대해 전처리 함수를 적용하고 진행 상황을 표시\n",
    "data.eng = data.eng.progress_apply(lambda x: preprocessing(x))\n",
    "\n",
    "# 프랑스어 문장에 대해 전처리 함수를 적용하고 진행 상황을 표시\n",
    "data.fra = data.fra.progress_apply(lambda x: preprocessing(x))\n",
    "\n",
    "# task: 영어 -> 프랑스어\n",
    "# encoder input: eng\n",
    "# decoder input: fra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장을 공백을 기준으로 분리하여 토큰화한 결과를 새로운 열에 저장\n",
    "data['token_eng'] = data.eng.str.split()\n",
    "\n",
    "# 프랑스어 문장을 공백을 기준으로 분리하여 토큰화한 결과를 새로운 열에 저장\n",
    "data['token_fra'] = data.fra.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 토큰 리스트의 시작에 <SOS> (시작 토큰)과 끝에 <EOS> (종료 토큰)을 추가\n",
    "data.token_eng = data.token_eng.apply(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "\n",
    "# 프랑스어 토큰 리스트의 시작에 <SOS> (시작 토큰)과 끝에 <EOS> (종료 토큰)을 추가\n",
    "data.token_fra = data.token_fra.apply(lambda x: ['<SOS>'] + x + ['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터를 피클 형식으로 저장\n",
    "data.to_pickle('./preprocess_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 토큰 리스트를 source 변수에 저장\n",
    "source = data.token_eng\n",
    "\n",
    "# 프랑스어 토큰 리스트를 target 변수에 저장\n",
    "target = data.token_fra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 단어 목록을 생성하기 위해 모든 토큰을 1차원으로 만들고 중복 제거\n",
    "eng_vocab = list(set(itertools.chain(*source.tolist())))\n",
    "\n",
    "# <PAD> 토큰을 어휘 목록의 첫 번째 항목으로 추가\n",
    "eng_vocab = ['<PAD>'] + eng_vocab\n",
    "\n",
    "# 각 단어에 고유한 인덱스를 부여하여 사전 생성\n",
    "eng_vocab = dict(zip(eng_vocab, range(len(eng_vocab))))\n",
    "\n",
    "# 인덱스를 키로, 단어를 값으로 하는 반전된 사전 생성\n",
    "eng_inverse_vocab = {value: key for key, value in eng_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_idx(tokens: list, vocab: dict):\n",
    "    '''주어진 토큰 리스트를 인덱스 리스트로 변환'''\n",
    "    return [vocab.get(word) for word in tokens]\n",
    "\n",
    "def idx_to_char(tokens: torch.Tensor, inverse_vocab: dict):\n",
    "    '''주어진 인덱스 텐서를 원래의 토큰 리스트로 변환'''\n",
    "    return [inverse_vocab.get(token.item()) for token in tokens]\n",
    "\n",
    "def list_to_tensor(tokens: list):\n",
    "    '''주어진 리스트를 PyTorch 텐서로 변환하고 정수형으로 변환'''\n",
    "    return torch.Tensor(tokens).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 토큰을 인덱스로 변환하여 새로운 열에 저장\n",
    "data['encoded_token_eng'] = data.token_eng.apply(\n",
    "    lambda x: char_to_idx(x, eng_vocab)\n",
    ")\n",
    "\n",
    "# 인덱스 리스트를 PyTorch 텐서로 변환\n",
    "data.encoded_token_eng = data.encoded_token_eng.apply(\n",
    "    lambda x: list_to_tensor(x)\n",
    ")\n",
    "\n",
    "# 변환된 텐서 리스트를 패딩하여 동일한 길이로 맞춤\n",
    "source = pad_sequence(\n",
    "    data.encoded_token_eng.tolist(),\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어 어휘 사전을 초기화하고 <PAD>, <SOS>, <EOS>에 인덱스 부여\n",
    "fra_vocab = ['<PAD>', '<SOS>', '<EOS>']\n",
    "fra_vocab = dict(zip(fra_vocab, range(len(fra_vocab))))\n",
    "\n",
    "# 프랑스어 토큰 목록을 1차원으로 만들어 중복 제거한 후, <SOS>와 <EOS>를 제거\n",
    "_fra_vocab = list(set(itertools.chain(*target.tolist())))\n",
    "_fra_vocab.remove('<SOS>')\n",
    "_fra_vocab.remove('<EOS>')\n",
    "\n",
    "# 나머지 단어들에 인덱스를 부여 (3부터 시작)\n",
    "_fra_vocab = dict(zip(_fra_vocab, range(3, len(_fra_vocab)+3)))\n",
    "\n",
    "# 초기화한 프랑스어 어휘 사전에 나머지 단어들을 업데이트\n",
    "fra_vocab.update(_fra_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어 토큰을 인덱스로 변환하여 새로운 열에 저장\n",
    "data['encoded_token_fra'] = data.token_fra.apply(\n",
    "    lambda x: char_to_idx(x, fra_vocab)\n",
    ")\n",
    "\n",
    "# 인덱스 리스트를 PyTorch 텐서로 변환\n",
    "data.encoded_token_fra = data.encoded_token_fra.apply(\n",
    "    lambda x: list_to_tensor(x)\n",
    ")\n",
    "\n",
    "# 변환된 텐서 리스트를 패딩하여 동일한 길이로 맞춤\n",
    "target = pad_sequence(\n",
    "    data.encoded_token_fra.tolist(),\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 학습용과 임시 데이터로 분할\n",
    "# temp 데이터를 검증용과 테스트용 데이터로 분할\n",
    "train_source, source_temp, train_target, target_temp = train_test_split(\n",
    "    source,\n",
    "    target,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    ")\n",
    "valid_source, test_source, valid_target, test_target = train_test_split(\n",
    "    source_temp,\n",
    "    target_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source, target):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 샘플 수를 반환\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환\n",
    "        source = self.source[idx]\n",
    "        target = self.target[idx]\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            'X': source,\n",
    "            'y': target,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 객체로 변환\n",
    "train_dataset = TranslationDataset(train_source, train_target)\n",
    "valid_dataset = TranslationDataset(valid_source, valid_target)\n",
    "test_dataset = TranslationDataset(test_source, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([ 6316, 11514, 15359,  3189,  6327, 14539,  9984, 15993,  3881,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'y': tensor([    1,  9842,  4419, 27415,  6163, 18521, 15066, 29336,  9990,     2,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0])}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체 확인\n",
    "train_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
