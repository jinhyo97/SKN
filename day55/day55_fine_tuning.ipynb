{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import huggingface_hub\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments,\n",
    ")\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "reference: [transfer learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-15-5971-6_83/MediaObjects/488258_1_En_83_Fig2_HTML.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th colspan=\"3\">Learning Settings</th>\n",
    "    <th>Source and Target Domains</th>\n",
    "    <th>Source and Target Tasks</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"3\">Traditional Machine Learning</th>\n",
    "    <td>the same</td>\n",
    "    <td>the same</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Transfer Learning</td>\n",
    "    <td colspan=\"2\"><i>Inductive Transfer Learning \\ Unsupervised Transfer Learning</i></td>\n",
    "    <td>the same</td>\n",
    "    <td>different but related</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\"><i>Transfer Learning \\ Unsupervised Transfer Learning</i></td>\n",
    "    <td>different but related</td>\n",
    "    <td>different but related</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\"><i>Transductive Transfer Learning</i></td>\n",
    "    <td>different but related</td>\n",
    "    <td>the same</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "| Transfer Learning Settings      | Related Areas                               | Source Domain Labels | Target Domain Labels | Tasks                       |\n",
    "|---------------------------------|---------------------------------------------|----------------------|----------------------|-----------------------------|\n",
    "| Inductive Transfer Learning     | Multi-task Learning                         | Available           | Available           | Regression, Classification  |\n",
    "|             |  Self-taught Learning                                           | Unavailable         | Available           | Regression, Classification  |\n",
    "| Transductive Transfer Learning  | Domain Adaptation, Sample Selection Bias, Co-variate Shift | Available           | Unavailable         | Regression, Classification  |\n",
    "| Unsupervised Transfer Learning  |                                             | Unavailable         | Unavailable         | Clustering, Dimensionality Reduction |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "사전 훈련된 모델을 특정 작업이나 데이터셋에 맞게 조정하는 과정. <br>\n",
    "대량의 데이터로 훈련되어 있는 pre-trained model을 특정 도메인이나 태스크에서 더 나은 성능을 내기 위해 모델의 weight를 조절하는 작업. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 사용할 모델 이름 설정\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "\n",
    "# 지정한 모델 이름으로 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 지정한 모델 이름으로 시퀀스 분류 모델 초기화\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 영화 리뷰 데이터셋을 로드\n",
    "data = load_dataset('imdb')\n",
    "\n",
    "# 학습 데이터셋에서 2000개의 샘플을 무작위로 섞어 선택\n",
    "train = data['train'].shuffle(seed=0).select(range(2000))\n",
    "\n",
    "# 테스트 데이터셋에서 1000개의 샘플을 무작위로 섞어 선택\n",
    "test = data['test'].shuffle(seed=0).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5355b837ba34822a3abc7b30c4345ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbddc2d2e7b4d51bb7a07f67032a252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 입력 예제를 토큰화하는 전처리 함수 정의\n",
    "def preprocessing(example: str):\n",
    "    return tokenizer(\n",
    "        example['text'],          # 입력 텍스트를 가져옴\n",
    "        padding='max_length',     # 최대 길이에 맞춰 패딩 추가\n",
    "        max_length=256,           # 최대 길이를 256으로 설정\n",
    "        truncation=True,          # 최대 길이를 초과할 경우 자르기\n",
    "        return_tensors='pt',     # PyTorch 텐서 형태로 반환\n",
    "    )\n",
    "\n",
    "# 학습 데이터셋을 토큰화하여 새로운 데이터셋 객체 생성\n",
    "tokenized_train_dataset = train.map(preprocessing, batched=True)\n",
    "\n",
    "# 테스트 데이터셋을 토큰화하여 새로운 데이터셋 객체 생성\n",
    "tokenized_test_dataset = test.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    507\n",
       "1    493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이블 비율 확인인\n",
    "pd.Series(np.array([example.get('label') for example in tokenized_test_dataset])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터셋 초기화: data는 토큰화된 데이터셋을 포함\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기(샘플 수)를 반환\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 데이터를 반환\n",
    "        temp = self.data[idx]  # 인덱스에 해당하는 데이터 샘플을 가져옴\n",
    "\n",
    "        return {\n",
    "            'input_ids': temp.get('input_ids'),         # 입력 아이디\n",
    "            'token_type_ids': temp.get('token_type_ids'), # 토큰 타입 아이디\n",
    "            'attention_mask': temp.get('attention_mask'), # 어텐션 마스크\n",
    "            'labels': temp.get('label')                    # 레이블 (감정 분류)\n",
    "        }\n",
    "\n",
    "# IMDBDataset 클래스의 인스턴스를 생성하여 학습 및 테스트 데이터셋을 만듭니다.\n",
    "train_dataset = IMDBDataset(tokenized_train_dataset)  # 토큰화된 학습 데이터셋을 사용하여 train_dataset 생성\n",
    "test_dataset = IMDBDataset(tokenized_test_dataset)    # 토큰화된 테스트 데이터셋을 사용하여 test_dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TrainingArguments 객체 생성: 모델 학습에 필요한 다양한 하이퍼파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert',                 # 모델과 결과를 저장할 디렉토리 경로\n",
    "    evaluation_strategy='epoch',                  # 검증 전략: 에포크마다 검증을 수행\n",
    "    learning_rate=2e-5,                           # 학습률 설정\n",
    "    per_device_train_batch_size=4,                # 학습 시 각 장치에서 사용할 배치 크기\n",
    "    per_device_eval_batch_size=4,                 # 평가 시 각 장치에서 사용할 배치 크기\n",
    "    num_train_epochs=1,                           # 학습할 에포크 수\n",
    "    weight_decay=0.01,                            # 가중치 감쇠(정규화) 값\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성: 모델 학습 및 평가를 위한 트레이너 인스턴스\n",
    "trainer = Trainer(\n",
    "    model=model,                                   # 학습할 모델\n",
    "    args=training_args,                            # 설정된 학습 인자\n",
    "    train_dataset=train_dataset,                   # 학습 데이터셋\n",
    "    eval_dataset=test_dataset,                     # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d322682fde144d3a9f37ebc123823de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ff parameter\n",
    "parameters[-8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ff parameter\n",
    "parameters[-7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layernorm parameter\n",
    "parameters[-6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layernorm parameter\n",
    "parameters[-5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier weight\n",
    "parameters[-4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier bias\n",
    "parameters[-3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0248, -0.0028, -0.0064,  ...,  0.0203, -0.0279,  0.0060],\n",
       "        [-0.0369, -0.0218, -0.0434,  ..., -0.0237,  0.0070, -0.0428]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier weight\n",
    "parameters[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.5986e-05, -3.5986e-05], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier bias\n",
    "parameters[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 모든 파라미터에 대해 학습을 하지 않도록 설정\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False  # 파라미터의 gradient 계산을 비활성화\n",
    "\n",
    "# 모델의 모든 파라미터 중 마지막 파라미터를 리스트로 반환\n",
    "list(model.parameters())[-1]  # gradient 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 모델을 불러옵니다.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 모델의 모든 파라미터에 대해 반복합니다.\n",
    "for i, parameter in enumerate(list(model.parameters())):\n",
    "    # 특정 인덱스(199, 200)의 파라미터만 학습 가능하도록 설정할 수 있는 주석 처리된 코드\n",
    "    # if i in [199, 200]:\n",
    "    #     parameter.requires_grad=True\n",
    "    \n",
    "    # 199 미만의 인덱스를 가진 파라미터의 기울기 계산을 비활성화\n",
    "    if i < 199:\n",
    "        parameter.requires_grad = False  # 해당 파라미터의 gradient 계산을 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 시퀀스 분류 모델을 불러옵니다.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 모델의 모든 파라미터에 대해 이름과 함께 반복합니다.\n",
    "for name, parameter in model.named_parameters():\n",
    "    # 파라미터 이름에 'classifier'가 포함되어 있지 않은 경우\n",
    "    if 'classifier' not in name:\n",
    "        parameter.requires_grad = False  # 해당 파라미터의 gradient 계산을 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 레이어를 제외하고 모든 weight freeze\n",
    "model = resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 모든 파라미터에 대해 이름과 함께 반복합니다.\n",
    "for name, parameter in model.named_parameters():\n",
    "    # 파라미터 이름에 'fc'가 포함되어 있지 않은 경우\n",
    "    if 'fc' not in name:\n",
    "        parameter.requires_grad = False  # 해당 파라미터의 gradient 계산을 비활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Fine-tuning\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "모델의 서로다른 layer는 서로 다른 information을 학습할 것. <br>\n",
    "-> layer마다 서로 다른 정도의 fine tuning이 필요. <br>\n",
    "\n",
    "$ \\eta_{L-1} = \\frac{\\eta_L}{2.6} $\n",
    "\n",
    "<br> \n",
    "\n",
    "<font style=\"font-size:20px\"> 사용 방법 </font>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         {'params': self.model.classifier, 'lr': 0.001},\n",
    ">         {'params': self.model.encoder.layer[-1], 'lr': 0.001/2.6},\n",
    ">         {'params': self.model.encoder.layer[-2], 'lr': 0.001/2.6/2.6},\n",
    ">     )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 3.1002957437181844e-11\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 8.060768933667281e-11\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.095799922753493e-10\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 3\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 5.449079799159082e-10\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 4\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.4167607477813612e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 5\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 3.6835779442315398e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 6\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 9.577302655002003e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 7\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.4900986903005207e-08\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 8\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 6.474256594781355e-08\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 9\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.6833067146431524e-07\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 10\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 4.376597458072196e-07\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 11\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.137915339098771e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 12\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.9585798816568047e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 13\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 7.692307692307692e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 14\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam 옵티마이저를 설정하며, 각 레이어 및 파라미터에 대해 개별 학습률을 설정\n",
    "optimizer = optim.Adam([\n",
    "    # BERT의 임베딩 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.embeddings.parameters(), 'lr': 2e-5/(2.6**14)},\n",
    "    \n",
    "    # BERT의 첫 번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[0].parameters(), 'lr': 2e-5/(2.6**13)},\n",
    "    \n",
    "    # BERT의 11번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-11].parameters(), 'lr': 2e-5/(2.6**12)},\n",
    "    \n",
    "    # BERT의 10번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-10].parameters(), 'lr': 2e-5/(2.6**11)},\n",
    "    \n",
    "    # BERT의 9번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-9].parameters(), 'lr': 2e-5/(2.6**10)},\n",
    "    \n",
    "    # BERT의 8번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-8].parameters(), 'lr': 2e-5/(2.6**9)},\n",
    "    \n",
    "    # BERT의 7번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-7].parameters(), 'lr': 2e-5/(2.6**8)},\n",
    "    \n",
    "    # BERT의 6번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-6].parameters(), 'lr': 2e-5/(2.6**7)},\n",
    "    \n",
    "    # BERT의 5번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-5].parameters(), 'lr': 2e-5/(2.6**6)},\n",
    "    \n",
    "    # BERT의 4번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-4].parameters(), 'lr': 2e-5/(2.6**5)},\n",
    "    \n",
    "    # BERT의 3번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-3].parameters(), 'lr': 2e-5/(2.6**4)},\n",
    "    \n",
    "    # BERT의 2번째 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-2].parameters(), 'lr': 2e-5/(2.6**3)},\n",
    "    \n",
    "    # BERT의 마지막 인코더 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.encoder.layer[-1].parameters(), 'lr': 2e-5/(2.6**2)},\n",
    "    \n",
    "    # BERT의 풀러 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.bert.pooler.parameters(), 'lr': 2e-5/(2.6**1)},\n",
    "    \n",
    "    # 모델의 분류 레이어 파라미터에 대한 학습률\n",
    "    {'params': model.classifier.parameters(), 'lr': 2e-5/(2.6**0)},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert layer의 learning rate를 수작업 없이 refactoring\n",
    "\n",
    "# 파라미터 리스트를 초기화하고 BERT 임베딩 레이어의 파라미터를 추가\n",
    "params = [{'params': model.bert.embeddings.parameters(), 'lr': 2e-5/(2.6**14)}]\n",
    "\n",
    "# BERT 인코더 레이어의 파라미터를 추가 (12층부터 1층까지)\n",
    "params.extend([\n",
    "    {'params': model.bert.encoder.layer[-i].parameters(), 'lr': 2e-5/(2.6**(i+1))}\n",
    "    for i in range(12, 0, -1)\n",
    "])\n",
    "\n",
    "# BERT 풀러 레이어와 분류 레이어의 파라미터를 추가\n",
    "params.extend([\n",
    "    {'params': model.bert.pooler.parameters(), 'lr': 2e-5/(2.6**1)},\n",
    "    {'params': model.classifier.parameters(), 'lr': 2e-5/(2.6**0)},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slanted Triangular Learning Rates\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/new_lr_plot_tNtxBIM.jpg\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "<br>\n",
    "\n",
    "특정 task를 target으로 parameter를 조절하는 경우, 적합한 매개변수 영역으로 빠르게 수렴 후 업데이트하길 원함. <br>\n",
    "이 경우 learning rate를 동일하게 사용하거나 점진적으로 감소하는 것이 바람직하지 않음. <br>\n",
    "-> 학습률을 먼저 선형적으로 증가시킨 후, 선형적으로 감소시킴\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{cut} = \\lfloor T \\cdot \\text{cut frac} \\rfloor\n",
    "\n",
    "\\\\\n",
    "\n",
    "p =\n",
    "\\begin{cases} \n",
    "\\frac{t}{\\text{cut}}, & \\text{if } t < \\text{cut} \\\\\n",
    "1 - \\frac{t - \\text{cut}}{\\text{cut} \\cdot (1/\\text{cut frac} - 1)}, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\\\\ \n",
    "\n",
    "\\eta_t = \\eta_{\\text{max}} \\cdot \\frac{1 + p \\cdot (\\text{ratio} - 1)}{\\text{ratio}}\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\text{where T: num of iterations, cut frac: fraction of iterations}\n",
    "\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         self.model.parameters(),\n",
    ">         lr=self.learning_rate,\n",
    ">     )\n",
    "> \n",
    ">     scheduler = {\n",
    ">         'scheduler': optim.lr_scheduler.OneCycleLR(\n",
    ">             optimizer,\n",
    ">             max_lr=1e-2,\n",
    ">             total_steps=1600,\n",
    ">             pct_start=0.125,\n",
    ">             anneal_strategy='linear'),\n",
    ">         'interval': 'step',\n",
    ">         'frequency': 1,\n",
    ">     }\n",
    "> \n",
    ">     return [optimizer], [scheduler]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "step = 300 # '<len(data)//batch>'\n",
    "epoch = 2 # '<epoch>'\n",
    "\n",
    "# OneCycleLR 스케줄러 설정\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-4,               # 최대 학습률\n",
    "    pct_start=0.15,            # 훈련의 처음 15% 동안 학습률을 증가\n",
    "    total_steps=step * epoch    # 총 훈련 단계 수\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradual unfreezing\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "catastrophic forgetting 위험을 피하기 위해, 모델의 마지막 레이어부터 차례로 unfreeze하는 방법. <br>\n",
    "마지막 layer는 가장 일반적인 지식을 포함하기에 마지막 레이어를 unfreeze 후 한 epoch 동안 fine-tuning. <br>\n",
    "그 다음 layer를 unfreeeze 후 fine-tuning, 이 작업을 모든 layer가 unfreeze될 때까지 반복. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size\"> 사용 방법 </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> def on_train_epoch_start(self):\n",
    ">     self.n_unfreeze_layer += 1\n",
    ">     for param in tuple(self.model.parameters())[-self.n_unfreeze_layer*2:]:\n",
    ">         param.requires_grad = True\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사전 훈련된 BERT 모델로 초기화\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 모든 파라미터의 gradient 업데이트를 비활성화\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# 에폭을 반복하며 특정 파라미터에 대해 gradient 업데이트를 활성화\n",
    "for epoch in range(3):  # 3개의 에폭 동안 훈련\n",
    "    for name, parameter in model.named_parameters():\n",
    "        # 첫 번째 에폭 동안 classifier 계층의 파라미터 업데이트 활성화\n",
    "        if epoch == 0 and 'classifier' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # 두 번째 에폭 동안 pooler 계층의 파라미터 업데이트 활성화\n",
    "        if epoch == 1 and 'pooler' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # 2번째 에폭부터 14번째 에폭 전까지 특정 encoder layer의 파라미터 업데이트 활성화\n",
    "        if 2 <= epoch < 14 and f'bert.encoder.layer.{13-epoch}' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # 14번째 에폭 동안 embeddings 계층의 파라미터 업데이트 활성화\n",
    "        if epoch == 14 and 'embedding' in name:\n",
    "            parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487bc987932c46948e144b795c777724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636bdc2dca46494796657de2eee3310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71254b8ce345a89b72acae4fa3bf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6854f811e4e343a4881f1890fd2ad7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama LlamaDecoderLayer의 learning rate를 0.1부터해서 1/2씩 layer에 적용\n",
    "# ex) LlamaDecoderLayer15: 0.1\n",
    "#     LlamaDecoderLayer14: 0.1 * 1/2\n",
    "#     ...\n",
    "#     LlamaDecoderLayer0 : 0.1 * 1/2**x\n",
    "\n",
    "optim.Adam(\n",
    "    [{'params': model.model.layers[-i].parameters(), 'lr': 0.1 * (1/2**(i-1))} for i in range(1, 17)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a56403b4074e681ecb36004649c5fb19e7bcf1144081029ba5f2cf549331f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
