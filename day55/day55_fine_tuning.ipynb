{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import huggingface_hub\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments,\n",
    ")\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "reference: [transfer learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-981-15-5971-6_83/MediaObjects/488258_1_En_83_Fig2_HTML.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th colspan=\"3\">Learning Settings</th>\n",
    "    <th>Source and Target Domains</th>\n",
    "    <th>Source and Target Tasks</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"3\">Traditional Machine Learning</th>\n",
    "    <td>the same</td>\n",
    "    <td>the same</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Transfer Learning</td>\n",
    "    <td colspan=\"2\"><i>Inductive Transfer Learning \\ Unsupervised Transfer Learning</i></td>\n",
    "    <td>the same</td>\n",
    "    <td>different but related</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\"><i>Transfer Learning \\ Unsupervised Transfer Learning</i></td>\n",
    "    <td>different but related</td>\n",
    "    <td>different but related</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\"><i>Transductive Transfer Learning</i></td>\n",
    "    <td>different but related</td>\n",
    "    <td>the same</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "| Transfer Learning Settings      | Related Areas                               | Source Domain Labels | Target Domain Labels | Tasks                       |\n",
    "|---------------------------------|---------------------------------------------|----------------------|----------------------|-----------------------------|\n",
    "| Inductive Transfer Learning     | Multi-task Learning                         | Available           | Available           | Regression, Classification  |\n",
    "|             |  Self-taught Learning                                           | Unavailable         | Available           | Regression, Classification  |\n",
    "| Transductive Transfer Learning  | Domain Adaptation, Sample Selection Bias, Co-variate Shift | Available           | Unavailable         | Regression, Classification  |\n",
    "| Unsupervised Transfer Learning  |                                             | Unavailable         | Unavailable         | Clustering, Dimensionality Reduction |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì´ë‚˜ ë°ì´í„°ì…‹ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ê³¼ì •. <br>\n",
    "ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ í›ˆë ¨ë˜ì–´ ìˆëŠ” pre-trained modelì„ íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ íƒœìŠ¤í¬ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ëª¨ë¸ì˜ weightë¥¼ ì¡°ì ˆí•˜ëŠ” ì‘ì—…. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "\n",
    "# ì§€ì •í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ì§€ì •í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ì„ ë¡œë“œ\n",
    "data = load_dataset('imdb')\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ 2000ê°œì˜ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ì„ íƒ\n",
    "train = data['train'].shuffle(seed=0).select(range(2000))\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ 1000ê°œì˜ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ì„ íƒ\n",
    "test = data['test'].shuffle(seed=0).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5355b837ba34822a3abc7b30c4345ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbddc2d2e7b4d51bb7a07f67032a252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì…ë ¥ ì˜ˆì œë¥¼ í† í°í™”í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def preprocessing(example: str):\n",
    "    return tokenizer(\n",
    "        example['text'],          # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
    "        padding='max_length',     # ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”© ì¶”ê°€\n",
    "        max_length=256,           # ìµœëŒ€ ê¸¸ì´ë¥¼ 256ìœ¼ë¡œ ì„¤ì •\n",
    "        truncation=True,          # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•  ê²½ìš° ìë¥´ê¸°\n",
    "        return_tensors='pt',     # PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜\n",
    "    )\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
    "tokenized_train_dataset = train.map(preprocessing, batched=True)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
    "tokenized_test_dataset = test.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    507\n",
       "1    493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë ˆì´ë¸” ë¹„ìœ¨ í™•ì¸ì¸\n",
    "pd.Series(np.array([example.get('label') for example in tokenized_test_dataset])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # ë°ì´í„°ì…‹ ì´ˆê¸°í™”: dataëŠ” í† í°í™”ëœ ë°ì´í„°ì…‹ì„ í¬í•¨\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # ë°ì´í„°ì…‹ì˜ í¬ê¸°(ìƒ˜í”Œ ìˆ˜)ë¥¼ ë°˜í™˜\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ì£¼ì–´ì§„ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜\n",
    "        temp = self.data[idx]  # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œì„ ê°€ì ¸ì˜´\n",
    "\n",
    "        return {\n",
    "            'input_ids': temp.get('input_ids'),         # ì…ë ¥ ì•„ì´ë””\n",
    "            'token_type_ids': temp.get('token_type_ids'), # í† í° íƒ€ì… ì•„ì´ë””\n",
    "            'attention_mask': temp.get('attention_mask'), # ì–´í…ì…˜ ë§ˆìŠ¤í¬\n",
    "            'labels': temp.get('label')                    # ë ˆì´ë¸” (ê°ì • ë¶„ë¥˜)\n",
    "        }\n",
    "\n",
    "# IMDBDataset í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "train_dataset = IMDBDataset(tokenized_train_dataset)  # í† í°í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ train_dataset ìƒì„±\n",
    "test_dataset = IMDBDataset(tokenized_test_dataset)    # í† í°í™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ test_dataset ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TrainingArguments ê°ì²´ ìƒì„±: ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert',                 # ëª¨ë¸ê³¼ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "    evaluation_strategy='epoch',                  # ê²€ì¦ ì „ëµ: ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ì„ ìˆ˜í–‰\n",
    "    learning_rate=2e-5,                           # í•™ìŠµë¥  ì„¤ì •\n",
    "    per_device_train_batch_size=4,                # í•™ìŠµ ì‹œ ê° ì¥ì¹˜ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=4,                 # í‰ê°€ ì‹œ ê° ì¥ì¹˜ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°\n",
    "    num_train_epochs=1,                           # í•™ìŠµí•  ì—í¬í¬ ìˆ˜\n",
    "    weight_decay=0.01,                            # ê°€ì¤‘ì¹˜ ê°ì‡ (ì •ê·œí™”) ê°’\n",
    ")\n",
    "\n",
    "# Trainer ê°ì²´ ìƒì„±: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ë¥¼ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤\n",
    "trainer = Trainer(\n",
    "    model=model,                                   # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_args,                            # ì„¤ì •ëœ í•™ìŠµ ì¸ì\n",
    "    train_dataset=train_dataset,                   # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    eval_dataset=test_dataset,                     # í‰ê°€ ë°ì´í„°ì…‹\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d322682fde144d3a9f37ebc123823de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ff parameter\n",
    "parameters[-8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ff parameter\n",
    "parameters[-7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layernorm parameter\n",
    "parameters[-6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layernorm parameter\n",
    "parameters[-5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier weight\n",
    "parameters[-4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier bias\n",
    "parameters[-3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0248, -0.0028, -0.0064,  ...,  0.0203, -0.0279,  0.0060],\n",
       "        [-0.0369, -0.0218, -0.0434,  ..., -0.0237,  0.0070, -0.0428]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier weight\n",
    "parameters[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.5986e-05, -3.5986e-05], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifier bias\n",
    "parameters[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ í•™ìŠµì„ í•˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False  # íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°ì„ ë¹„í™œì„±í™”\n",
    "\n",
    "# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„° ì¤‘ ë§ˆì§€ë§‰ íŒŒë¼ë¯¸í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "list(model.parameters())[-1]  # gradient ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "for i, parameter in enumerate(list(model.parameters())):\n",
    "    # íŠ¹ì • ì¸ë±ìŠ¤(199, 200)ì˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œ\n",
    "    # if i in [199, 200]:\n",
    "    #     parameter.requires_grad=True\n",
    "    \n",
    "    # 199 ë¯¸ë§Œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ íŒŒë¼ë¯¸í„°ì˜ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ë¹„í™œì„±í™”\n",
    "    if i < 199:\n",
    "        parameter.requires_grad = False  # í•´ë‹¹ íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°ì„ ë¹„í™œì„±í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ì´ë¦„ê³¼ í•¨ê»˜ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "for name, parameter in model.named_parameters():\n",
    "    # íŒŒë¼ë¯¸í„° ì´ë¦„ì— 'classifier'ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°\n",
    "    if 'classifier' not in name:\n",
    "        parameter.requires_grad = False  # í•´ë‹¹ íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°ì„ ë¹„í™œì„±í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  weight freeze\n",
    "model = resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ì´ë¦„ê³¼ í•¨ê»˜ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "for name, parameter in model.named_parameters():\n",
    "    # íŒŒë¼ë¯¸í„° ì´ë¦„ì— 'fc'ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°\n",
    "    if 'fc' not in name:\n",
    "        parameter.requires_grad = False  # í•´ë‹¹ íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°ì„ ë¹„í™œì„±í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Fine-tuning\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "ëª¨ë¸ì˜ ì„œë¡œë‹¤ë¥¸ layerëŠ” ì„œë¡œ ë‹¤ë¥¸ informationì„ í•™ìŠµí•  ê²ƒ. <br>\n",
    "-> layerë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ì •ë„ì˜ fine tuningì´ í•„ìš”. <br>\n",
    "\n",
    "$ \\eta_{L-1} = \\frac{\\eta_L}{2.6} $\n",
    "\n",
    "<br> \n",
    "\n",
    "<font style=\"font-size:20px\"> ì‚¬ìš© ë°©ë²• </font>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         {'params': self.model.classifier, 'lr': 0.001},\n",
    ">         {'params': self.model.encoder.layer[-1], 'lr': 0.001/2.6},\n",
    ">         {'params': self.model.encoder.layer[-2], 'lr': 0.001/2.6/2.6},\n",
    ">     )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 3.1002957437181844e-11\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 8.060768933667281e-11\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.095799922753493e-10\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 3\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 5.449079799159082e-10\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 4\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.4167607477813612e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 5\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 3.6835779442315398e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 6\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 9.577302655002003e-09\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 7\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.4900986903005207e-08\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 8\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 6.474256594781355e-08\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 9\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.6833067146431524e-07\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 10\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 4.376597458072196e-07\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 11\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1.137915339098771e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 12\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2.9585798816568047e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 13\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 7.692307692307692e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 14\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•˜ë©°, ê° ë ˆì´ì–´ ë° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ê°œë³„ í•™ìŠµë¥ ì„ ì„¤ì •\n",
    "optimizer = optim.Adam([\n",
    "    # BERTì˜ ì„ë² ë”© ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.embeddings.parameters(), 'lr': 2e-5/(2.6**14)},\n",
    "    \n",
    "    # BERTì˜ ì²« ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[0].parameters(), 'lr': 2e-5/(2.6**13)},\n",
    "    \n",
    "    # BERTì˜ 11ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-11].parameters(), 'lr': 2e-5/(2.6**12)},\n",
    "    \n",
    "    # BERTì˜ 10ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-10].parameters(), 'lr': 2e-5/(2.6**11)},\n",
    "    \n",
    "    # BERTì˜ 9ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-9].parameters(), 'lr': 2e-5/(2.6**10)},\n",
    "    \n",
    "    # BERTì˜ 8ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-8].parameters(), 'lr': 2e-5/(2.6**9)},\n",
    "    \n",
    "    # BERTì˜ 7ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-7].parameters(), 'lr': 2e-5/(2.6**8)},\n",
    "    \n",
    "    # BERTì˜ 6ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-6].parameters(), 'lr': 2e-5/(2.6**7)},\n",
    "    \n",
    "    # BERTì˜ 5ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-5].parameters(), 'lr': 2e-5/(2.6**6)},\n",
    "    \n",
    "    # BERTì˜ 4ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-4].parameters(), 'lr': 2e-5/(2.6**5)},\n",
    "    \n",
    "    # BERTì˜ 3ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-3].parameters(), 'lr': 2e-5/(2.6**4)},\n",
    "    \n",
    "    # BERTì˜ 2ë²ˆì§¸ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-2].parameters(), 'lr': 2e-5/(2.6**3)},\n",
    "    \n",
    "    # BERTì˜ ë§ˆì§€ë§‰ ì¸ì½”ë” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.encoder.layer[-1].parameters(), 'lr': 2e-5/(2.6**2)},\n",
    "    \n",
    "    # BERTì˜ í’€ëŸ¬ ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.bert.pooler.parameters(), 'lr': 2e-5/(2.6**1)},\n",
    "    \n",
    "    # ëª¨ë¸ì˜ ë¶„ë¥˜ ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ í•™ìŠµë¥ \n",
    "    {'params': model.classifier.parameters(), 'lr': 2e-5/(2.6**0)},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert layerì˜ learning rateë¥¼ ìˆ˜ì‘ì—… ì—†ì´ refactoring\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  BERT ì„ë² ë”© ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€\n",
    "params = [{'params': model.bert.embeddings.parameters(), 'lr': 2e-5/(2.6**14)}]\n",
    "\n",
    "# BERT ì¸ì½”ë” ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€ (12ì¸µë¶€í„° 1ì¸µê¹Œì§€)\n",
    "params.extend([\n",
    "    {'params': model.bert.encoder.layer[-i].parameters(), 'lr': 2e-5/(2.6**(i+1))}\n",
    "    for i in range(12, 0, -1)\n",
    "])\n",
    "\n",
    "# BERT í’€ëŸ¬ ë ˆì´ì–´ì™€ ë¶„ë¥˜ ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€\n",
    "params.extend([\n",
    "    {'params': model.bert.pooler.parameters(), 'lr': 2e-5/(2.6**1)},\n",
    "    {'params': model.classifier.parameters(), 'lr': 2e-5/(2.6**0)},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slanted Triangular Learning Rates\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/new_lr_plot_tNtxBIM.jpg\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "<br>\n",
    "\n",
    "íŠ¹ì • taskë¥¼ targetìœ¼ë¡œ parameterë¥¼ ì¡°ì ˆí•˜ëŠ” ê²½ìš°, ì í•©í•œ ë§¤ê°œë³€ìˆ˜ ì˜ì—­ìœ¼ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ë ´ í›„ ì—…ë°ì´íŠ¸í•˜ê¸¸ ì›í•¨. <br>\n",
    "ì´ ê²½ìš° learning rateë¥¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ê±°ë‚˜ ì ì§„ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ì§€ ì•ŠìŒ. <br>\n",
    "-> í•™ìŠµë¥ ì„ ë¨¼ì € ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¨ í›„, ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œì‹œí‚´\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{cut} = \\lfloor T \\cdot \\text{cut frac} \\rfloor\n",
    "\n",
    "\\\\\n",
    "\n",
    "p =\n",
    "\\begin{cases} \n",
    "\\frac{t}{\\text{cut}}, & \\text{if } t < \\text{cut} \\\\\n",
    "1 - \\frac{t - \\text{cut}}{\\text{cut} \\cdot (1/\\text{cut frac} - 1)}, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\\\\ \n",
    "\n",
    "\\eta_t = \\eta_{\\text{max}} \\cdot \\frac{1 + p \\cdot (\\text{ratio} - 1)}{\\text{ratio}}\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\text{where T: num of iterations, cut frac: fraction of iterations}\n",
    "\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         self.model.parameters(),\n",
    ">         lr=self.learning_rate,\n",
    ">     )\n",
    "> \n",
    ">     scheduler = {\n",
    ">         'scheduler': optim.lr_scheduler.OneCycleLR(\n",
    ">             optimizer,\n",
    ">             max_lr=1e-2,\n",
    ">             total_steps=1600,\n",
    ">             pct_start=0.125,\n",
    ">             anneal_strategy='linear'),\n",
    ">         'interval': 'step',\n",
    ">         'frequency': 1,\n",
    ">     }\n",
    "> \n",
    ">     return [optimizer], [scheduler]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "step = 300 # '<len(data)//batch>'\n",
    "epoch = 2 # '<epoch>'\n",
    "\n",
    "# OneCycleLR ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-4,               # ìµœëŒ€ í•™ìŠµë¥ \n",
    "    pct_start=0.15,            # í›ˆë ¨ì˜ ì²˜ìŒ 15% ë™ì•ˆ í•™ìŠµë¥ ì„ ì¦ê°€\n",
    "    total_steps=step * epoch    # ì´ í›ˆë ¨ ë‹¨ê³„ ìˆ˜\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradual unfreezing\n",
    "\n",
    "paper: https://arxiv.org/pdf/1801.06146\n",
    "\n",
    "catastrophic forgetting ìœ„í—˜ì„ í”¼í•˜ê¸° ìœ„í•´, ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë¶€í„° ì°¨ë¡€ë¡œ unfreezeí•˜ëŠ” ë°©ë²•. <br>\n",
    "ë§ˆì§€ë§‰ layerëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í¬í•¨í•˜ê¸°ì— ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ unfreeze í›„ í•œ epoch ë™ì•ˆ fine-tuning. <br>\n",
    "ê·¸ ë‹¤ìŒ layerë¥¼ unfreeeze í›„ fine-tuning, ì´ ì‘ì—…ì„ ëª¨ë“  layerê°€ unfreezeë  ë•Œê¹Œì§€ ë°˜ë³µ. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size\"> ì‚¬ìš© ë°©ë²• </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "> ```python\n",
    "> def on_train_epoch_start(self):\n",
    ">     self.n_unfreeze_layer += 1\n",
    ">     for param in tuple(self.model.parameters())[-self.n_unfreeze_layer*2:]:\n",
    ">         param.requires_grad = True\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ë¡œ ì´ˆê¸°í™”\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradient ì—…ë°ì´íŠ¸ë¥¼ ë¹„í™œì„±í™”\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# ì—í­ì„ ë°˜ë³µí•˜ë©° íŠ¹ì • íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ gradient ì—…ë°ì´íŠ¸ë¥¼ í™œì„±í™”\n",
    "for epoch in range(3):  # 3ê°œì˜ ì—í­ ë™ì•ˆ í›ˆë ¨\n",
    "    for name, parameter in model.named_parameters():\n",
    "        # ì²« ë²ˆì§¸ ì—í­ ë™ì•ˆ classifier ê³„ì¸µì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í™œì„±í™”\n",
    "        if epoch == 0 and 'classifier' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ ì—í­ ë™ì•ˆ pooler ê³„ì¸µì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í™œì„±í™”\n",
    "        if epoch == 1 and 'pooler' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # 2ë²ˆì§¸ ì—í­ë¶€í„° 14ë²ˆì§¸ ì—í­ ì „ê¹Œì§€ íŠ¹ì • encoder layerì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í™œì„±í™”\n",
    "        if 2 <= epoch < 14 and f'bert.encoder.layer.{13-epoch}' in name:\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # 14ë²ˆì§¸ ì—í­ ë™ì•ˆ embeddings ê³„ì¸µì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í™œì„±í™”\n",
    "        if epoch == 14 and 'embedding' in name:\n",
    "            parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487bc987932c46948e144b795c777724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636bdc2dca46494796657de2eee3310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71254b8ce345a89b72acae4fa3bf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6854f811e4e343a4881f1890fd2ad7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama LlamaDecoderLayerì˜ learning rateë¥¼ 0.1ë¶€í„°í•´ì„œ 1/2ì”© layerì— ì ìš©\n",
    "# ex) LlamaDecoderLayer15: 0.1\n",
    "#     LlamaDecoderLayer14: 0.1 * 1/2\n",
    "#     ...\n",
    "#     LlamaDecoderLayer0 : 0.1 * 1/2**x\n",
    "\n",
    "optim.Adam(\n",
    "    [{'params': model.model.layers[-i].parameters(), 'lr': 0.1 * (1/2**(i-1))} for i in range(1, 17)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a56403b4074e681ecb36004649c5fb19e7bcf1144081029ba5f2cf549331f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
