{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers import BertWordPieceTokenizer, Tokenizer\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    BertConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "reference: [BERT](https://wikidocs.net/115055) <br>\n",
    "paper: [BERT](https://arxiv.org/pdf/1810.04805) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    # 쿼리와 키의 내적을 계산합니다.\n",
    "    matmul_qk = query @ key.transpose(-2, -1)\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # 스케일링을 적용합니다.\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    # 마스크가 있을 경우 logits에 마스크를 적용합니다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # 소프트맥스 함수를 적용하여 어텐션 가중치를 계산합니다.\n",
    "    attention_weights = F.softmax(logits, dim=-1)\n",
    "    output = attention_weights @ value  # 어텐션 가중치를 값에 적용합니다.\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # num_heads로 나누어 떨어지지 않으면 오류를 발생시킵니다.\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        # 쿼리, 키, 값을 위한 밀집 레이어를 정의합니다.\n",
    "        self.query_dense = nn.Linear(self.d_model, self.d_model)\n",
    "        self.key_dense = nn.Linear(self.d_model, self.d_model)\n",
    "        self.value_dense = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        # 최종 출력을 위한 밀집 레이어를 정의합니다.\n",
    "        self.dense = nn.Linear(self.d_model, self.d_model)\n",
    "    \n",
    "    def forward(self, inputs: dict):\n",
    "        # 쿼리, 키, 값, 마스크를 입력으로 받습니다.\n",
    "        query, key, value = inputs.get('query'), inputs.get('key'), inputs.get('value')\n",
    "        mask = inputs.get('mask')\n",
    "        batch_size, seq_len = query.shape[:2]\n",
    "\n",
    "        # 쿼리, 키, 값을 밀집 레이어에 통과시킵니다.\n",
    "        query = self.query_dense(query)  # batch_size, seq_len, dim\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 다중 헤드로 나누어 줍니다.\n",
    "        query = query.reshape(batch_size, seq_len, self.num_heads, self.depth)  # batch, seq_len, num_heads, depth\n",
    "        key = key.reshape(batch_size, seq_len, self.num_heads, self.depth)\n",
    "        value = value.reshape(batch_size, seq_len, self.num_heads, self.depth)\n",
    "\n",
    "        # 차원 순서를 변경합니다.\n",
    "        query = query.permute(0, 2, 1, 3)   # batch, num_heads, seq_len, depth\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 스케일드 점곱 어텐션을 계산합니다.\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)  # batch, seq_len, num_heads, depth\n",
    "        concat_attention = scaled_attention.reshape(batch_size, seq_len, self.d_model)  # batch, seq_len, dim\n",
    "\n",
    "        # 최종 출력을 생성합니다.\n",
    "        outputs = self.dense(concat_attention)  # batch, seq_len, dim\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, num_heads: int, dropout_ratio: float):    \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        # 멀티헤드 어텐션 레이어를 정의합니다.\n",
    "        self.multi_head_attention = MultiheadAttention(self.d_model, self.num_heads)\n",
    "        self.dropout1 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        # 피드포워드 네트워크를 정의합니다.\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_ff, self.d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout2 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 멀티헤드 어텐션을 통과시킵니다.\n",
    "        inputs = {'query': x, 'key': x, 'value': x, 'mask': mask}\n",
    "        x_multi_head_output = self.multi_head_attention(inputs)\n",
    "        x_multi_head_output = self.dropout1(x_multi_head_output)\n",
    "        x = self.layer_norm1(x_multi_head_output + x)  # 잔차 연결을 적용합니다.\n",
    "        \n",
    "        # 피드포워드 네트워크를 통과시킵니다.\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        output = self.layer_norm2(x + ffn_output)  # 잔차 연결을 적용합니다.\n",
    "\n",
    "        return output\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_heads: int,\n",
    "        dropout_ratio: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        # 임베딩 레이어를 정의합니다.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.d_model)\n",
    "        self.segment_embedding = nn.Embedding(2, self.d_model)\n",
    "        \n",
    "        # 인코더 레이어를 생성합니다.\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(self.d_model, self.d_ff, self.num_heads, self.dropout_ratio)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 입력 임베딩을 생성합니다.\n",
    "        x = self.embedding(x)\n",
    "        x *= (self.d_model ** 0.5)\n",
    "\n",
    "        # 위치 인코딩을 추가합니다.\n",
    "        positional_encoding = (torch.ones(x.shape[:2]).cumsum(axis=1) - 1).long()\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        x += positional_embedding\n",
    "\n",
    "        # 세그먼트 인코딩을 추가합니다.\n",
    "        segment_encoding = torch.zeros(x.shape[:2]).long()\n",
    "        segment_embedding = self.segment_embedding(segment_encoding)\n",
    "        x += segment_embedding\n",
    "\n",
    "        # 각 인코더 레이어를 통과시킵니다.\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        output = x\n",
    "\n",
    "        return output  # 최종 출력을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2906, -0.6584,  1.9619,  ..., -1.0357, -0.2678,  0.5728],\n",
       "         [-0.0104, -1.9197,  1.0588,  ..., -0.2179, -1.6125, -0.0701],\n",
       "         [ 0.5030,  0.0756,  1.1953,  ...,  0.5741, -1.0067,  1.6260],\n",
       "         ...,\n",
       "         [-0.1088, -0.2704,  0.4653,  ..., -0.2513, -1.8823, -0.3770],\n",
       "         [ 0.6014, -1.2329,  0.6875,  ..., -0.1052, -1.5198,  0.4144],\n",
       "         [ 0.1390, -1.5423,  0.5434,  ..., -0.9341, -0.8175, -0.2473]],\n",
       "\n",
       "        [[ 0.2578,  0.4389, -0.5429,  ..., -1.3440,  0.8259, -0.4212],\n",
       "         [-0.2401, -0.3437, -0.2433,  ..., -1.4168, -0.0399,  0.1204],\n",
       "         [ 1.0967, -0.4443,  0.7128,  ..., -1.0174,  1.0468,  0.8848],\n",
       "         ...,\n",
       "         [-0.5167, -0.3882, -0.8720,  ...,  0.1111,  0.3167, -0.2946],\n",
       "         [ 0.3027, -1.2595,  0.0768,  ..., -0.4451,  0.6046, -0.4611],\n",
       "         [-0.8294,  0.0169,  0.5167,  ..., -0.1644,  0.0142,  1.4447]],\n",
       "\n",
       "        [[ 0.4527, -0.6503,  0.2345,  ..., -0.1982, -1.2915,  0.1563],\n",
       "         [ 0.8184, -0.3630,  0.5080,  ...,  0.0506, -2.1922, -0.6322],\n",
       "         [-0.0122, -1.5273, -0.0720,  ..., -0.7974,  0.8421, -0.1471],\n",
       "         ...,\n",
       "         [ 0.5828, -1.8357,  0.0101,  ..., -0.5347, -1.9473, -0.0345],\n",
       "         [ 0.3667, -1.7582,  0.4395,  ..., -0.5286, -0.4925,  0.4165],\n",
       "         [ 0.4657, -0.2505,  0.0743,  ..., -0.3132, -0.7355,  0.6427]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4044, -0.2696,  0.4547,  ..., -0.1074,  0.9727, -0.3274],\n",
       "         [-0.5282,  0.9650,  0.9927,  ...,  0.6619, -1.2222, -0.7259],\n",
       "         [-0.9619,  0.6906,  0.5773,  ..., -0.2720, -0.9622, -1.6032],\n",
       "         ...,\n",
       "         [-0.5100, -0.3866, -0.0814,  ...,  1.1811, -1.4567,  0.1354],\n",
       "         [-0.3501,  0.7796,  0.9380,  ..., -0.5421, -0.6814, -0.7379],\n",
       "         [-0.0719, -0.4953,  0.5538,  ...,  0.2205,  0.4269, -1.1032]],\n",
       "\n",
       "        [[-0.7419, -0.1704, -0.2386,  ..., -0.3798, -0.1979, -1.2759],\n",
       "         [ 0.6489,  0.7216, -0.3562,  ..., -1.0322, -0.2352, -1.5169],\n",
       "         [ 0.0531, -0.3115, -0.3216,  ..., -0.5997,  0.4267, -0.3240],\n",
       "         ...,\n",
       "         [ 0.1741, -1.1247, -0.7723,  ..., -1.0475,  0.1056,  0.4499],\n",
       "         [ 0.4444,  1.2710, -0.1278,  ..., -1.1978,  0.6895, -0.6125],\n",
       "         [-0.8415, -0.0741, -0.2348,  ...,  0.0536,  0.8047, -1.2049]],\n",
       "\n",
       "        [[-1.6387, -0.5787, -0.1786,  ...,  0.4818, -0.3913, -0.7811],\n",
       "         [-0.3765,  0.3349,  0.6212,  ...,  0.1840, -0.0755, -0.5575],\n",
       "         [ 0.2740,  0.3620, -0.3207,  ..., -0.0469,  0.2823, -1.4063],\n",
       "         ...,\n",
       "         [-0.2132, -0.1030, -0.6525,  ...,  0.1878,  0.0059, -1.2033],\n",
       "         [-0.1420, -0.2182,  0.3035,  ...,  0.1657, -0.3726, -0.8626],\n",
       "         [ 0.0520, -0.0337,  0.1039,  ...,  1.5883, -0.1269, -1.1709]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 입력 데이터 생성: (batch_size=32, seq_len=20) 크기의 텐서를 생성합니다.\n",
    "# 값은 0에서 999 사이의 정수로 설정합니다.\n",
    "x = torch.randint(0, 1000, (32, 20))\n",
    "\n",
    "# BERT 모델 초기화: \n",
    "# seq_len=20, vocab_size=1000, num_layers=12, d_model=768, d_ff=2048, num_heads=12, dropout_ratio=0.1\n",
    "bert = BERT(20, 1000, 12, 768, 2048, 12, 0.1)\n",
    "\n",
    "# BERT 모델에 입력 데이터를 통과시켜 출력을 생성합니다.\n",
    "output = bert(x)\n",
    "\n",
    "# 출력 텐서를 확인합니다.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertWordPieceTokenizer 초기화: 소문자 변환을 활성화합니다.\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "\n",
    "# 훈련 데이터에서 문장 목록을 가져옵니다.\n",
    "sentences = train['sentence1'].tolist()\n",
    "\n",
    "# 문장을 기반으로 WordPiece 토크나이저를 훈련합니다.\n",
    "# vocab_size는 30,000, 최소 빈도수는 3으로 설정합니다.\n",
    "tokenizer.train_from_iterator(sentences, vocab_size=30000, min_frequency=3)\n",
    "\n",
    "# 훈련된 어휘를 JSON 파일로 저장합니다.\n",
    "tokenizer.save('./custom_bert_vocab.json')\n",
    "\n",
    "# 저장된 어휘를 파일에서 불러옵니다.\n",
    "tokenizer = Tokenizer.from_file('./custom_bert_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안', '##녀', '##ᆼ', '##하', '##세', '##요']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주어진 문장을 토크나이즈하여 토큰 목록을 생성합니다.\n",
    "tokens = tokenizer.encode('안녕하세요').tokens\n",
    "\n",
    "# 생성된 토큰 목록을 출력합니다.\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 314271.41 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 400721.51 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 426128.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 처음 2000개의 샘플을 선택하여 train_data로 설정합니다.\n",
    "train_data = dataset['train'].select(range(2000))\n",
    "\n",
    "# 테스트 데이터에서 처음 2000개의 샘플을 선택하여 eval_data로 설정합니다.\n",
    "eval_data = dataset['test'].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 BERT 모델의 이름을 설정합니다.\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 미리 학습된 BERT 토크나이저를 로드합니다.\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocessing(row):\n",
    "    # 각 데이터 행의 'text' 컬럼을 토크나이즈합니다.\n",
    "    # 최대 길이를 256으로 설정하고, 패딩을 적용하며, 필요 시 잘라냅니다.\n",
    "    return tokenizer(row['text'], truncation=True, max_length=256, padding='max_length')\n",
    "\n",
    "# 훈련 데이터에 대해 전처리를 수행하여 토크나이즈된 데이터를 생성합니다.\n",
    "tokenized_train_data = train_data.map(preprocessing, batched=True)\n",
    "\n",
    "# 평가 데이터에 대해 전처리를 수행하여 토크나이즈된 데이터를 생성합니다.\n",
    "tokenized_eval_data = eval_data.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이즈된 훈련 데이터에서 필요한 필드를 추출하여 딕셔너리 형태로 변환합니다.\n",
    "train_data = [\n",
    "    {\n",
    "        'input_ids': row.get('input_ids'),\n",
    "        'token_type_ids': row.get('token_type_ids'),\n",
    "        'attention_mask': row.get('attention_mask'),\n",
    "        'label': row.get('label'),\n",
    "    } for row in tokenized_train_data\n",
    "]\n",
    "\n",
    "# 토크나이즈된 평가 데이터에서 필요한 필드를 추출하여 딕셔너리 형태로 변환합니다.\n",
    "eval_data = [\n",
    "    {\n",
    "        'input_ids': row.get('input_ids'),\n",
    "        'token_type_ids': row.get('token_type_ids'),\n",
    "        'attention_mask': row.get('attention_mask'),\n",
    "        'label': row.get('label'),\n",
    "    } for row in tokenized_eval_data\n",
    "]\n",
    "\n",
    "# PyTorch Dataset을 상속받은 IMDBDataset 클래스 정의\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터 초기화\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터의 길이를 반환합니다.\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 데이터를 반환합니다.\n",
    "        return {\n",
    "            'input_ids': self.data[idx].get('input_ids'),\n",
    "            'attention_mask': self.data[idx].get('attention_mask'),\n",
    "            'labels': self.data[idx].get('label'),\n",
    "        }\n",
    "\n",
    "# IMDBDataset 클래스를 사용하여 훈련 및 평가 데이터셋 생성\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "eval_dataset = IMDBDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 매개변수를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                    # 평가 전략: 에포크마다 평가\n",
    "    learning_rate=2e-5,                       # 학습률\n",
    "    per_device_train_batch_size=16,           # 장치당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=16,            # 장치당 평가 배치 크기\n",
    "    num_train_epochs=2,                       # 훈련할 에포크 수\n",
    "    weight_decay=0.01,                        # 가중치 감쇠\n",
    ")\n",
    "\n",
    "# 미리 학습된 BERT 모델을 불러옵니다 (분류용).\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Trainer 객체를 초기화합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,                              # 사용할 모델\n",
    "    args=training_args,                       # 훈련 매개변수\n",
    "    train_dataset=train_dataset,              # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset,                # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유튜브"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/ratings_test.txt', <http.client.HTTPMessage at 0x25b7b31fa30>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# NSMC 훈련 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    'https://raw.githubusercontent.com/e9t/nsmc/refs/heads/master/ratings_train.txt',\n",
    "    filename='data/ratings_train.txt',  # 저장할 파일 경로 및 이름\n",
    ")\n",
    "\n",
    "# NSMC 테스트 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    'https://raw.githubusercontent.com/e9t/nsmc/refs/heads/master/ratings_test.txt',\n",
    "    filename='data/ratings_test.txt',   # 저장할 파일 경로 및 이름\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋을 로드하고 'document'와 'label' 컬럼만 선택합니다.\n",
    "# 처음 5000개의 샘플을 사용합니다.\n",
    "train = pd.read_table('data/ratings_train.txt', usecols=['document', 'label']).iloc[:5000]\n",
    "\n",
    "# 검증 데이터셋을 로드하고 'document'와 'label' 컬럼만 선택합니다.\n",
    "# 처음 5000개의 샘플을 사용합니다.\n",
    "valid = pd.read_table('data/ratings_test.txt', usecols=['document', 'label']).iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다국어 지원 BERT 모델의 미리 학습된 토크나이저를 로드합니다.\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 데이터의 'document' 컬럼을 토크나이즈하여 'tokenized_comment' 컬럼에 저장합니다.\n",
    "train['tokenized_comment'] = train.document.apply(lambda x: tokenizer(x))\n",
    "\n",
    "# 검증 데이터의 'document' 컬럼을 토크나이즈하여 'tokenized_comment' 컬럼에 저장합니다.\n",
    "valid['tokenized_comment'] = valid.document.apply(lambda x: tokenizer(x))\n",
    "\n",
    "# 훈련 데이터의 'tokenized_comment'에서 'input_ids'의 길이를 계산하여 히스토그램을 그립니다.\n",
    "train.tokenized_comment.apply(lambda x: x.get('input_ids')).apply(len).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 문장을 토크나이즈하여 텐서 형태로 변환합니다.\n",
    "tokenized_train_comment = tokenizer(\n",
    "    train.document.tolist(),               # 훈련 데이터의 문장 목록\n",
    "    return_tensors='pt',                  # 파이토치 텐서로 반환\n",
    "    padding='max_length',                  # 최대 길이에 맞춰 패딩\n",
    "    max_length=100,                        # 최대 길이 100\n",
    "    truncation=True,                       # 초과하는 경우 잘라냄\n",
    ")\n",
    "\n",
    "# 검증 데이터의 문장을 토크나이즈하여 텐서 형태로 변환합니다.\n",
    "tokenized_valid_comment = tokenizer(\n",
    "    valid.document.tolist(),                # 검증 데이터의 문장 목록\n",
    "    return_tensors='pt',                   # 파이토치 텐서로 반환\n",
    "    padding='max_length',                   # 최대 길이에 맞춰 패딩\n",
    "    max_length=100,                         # 최대 길이 100\n",
    "    truncation=True,                        # 초과하는 경우 잘라냄\n",
    ")\n",
    "\n",
    "# 훈련 데이터셋을 구성하는 리스트 생성\n",
    "train_data = [\n",
    "    {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'label': label,\n",
    "    } \n",
    "    for input_ids, token_type_ids, attention_mask, label \n",
    "    in zip(tokenized_train_comment.get('input_ids'),\n",
    "           tokenized_train_comment.get('token_type_ids'),\n",
    "           tokenized_train_comment.get('attention_mask'),\n",
    "           train.label.values)  # 레이블 추가\n",
    "]\n",
    "\n",
    "# 검증 데이터셋을 구성하는 리스트 생성\n",
    "eval_data = [\n",
    "    {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'label': label,\n",
    "    } \n",
    "    for input_ids, token_type_ids, attention_mask, label \n",
    "    in zip(tokenized_valid_comment.get('input_ids'),\n",
    "           tokenized_valid_comment.get('token_type_ids'),\n",
    "           tokenized_valid_comment.get('attention_mask'),\n",
    "           valid.label.values)  # 올바른 레이블 추가\n",
    "]\n",
    "\n",
    "# PyTorch Dataset을 상속받은 CommentDataset 클래스 정의\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 데이터를 반환합니다.\n",
    "        return {\n",
    "            'input_ids': self.data[idx].get('input_ids'),\n",
    "            'attention_mask': self.data[idx].get('attention_mask'),\n",
    "            'labels': self.data[idx].get('label'),\n",
    "        }\n",
    "\n",
    "# 훈련 및 검증 데이터셋 인스턴스 생성\n",
    "train_dataset = CommentDataset(train_data)\n",
    "eval_dataset = CommentDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 매개변수를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                  # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                   # 평가 전략: 에포크마다 평가\n",
    "    learning_rate=2e-5,                      # 학습률\n",
    "    per_device_train_batch_size=16,          # 장치당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=16,           # 장치당 평가 배치 크기\n",
    "    num_train_epochs=2,                      # 훈련할 에포크 수\n",
    "    weight_decay=0.01,                       # 가중치 감쇠\n",
    ")\n",
    "\n",
    "# 미리 학습된 다국어 BERT 모델을 불러옵니다 (분류용).\n",
    "model = BertForSequenceClassification.from_pretrained('google-bert/bert-base-multilingual-uncased')\n",
    "\n",
    "# Trainer 객체를 초기화합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,                             # 사용할 모델\n",
    "    args=training_args,                      # 훈련 매개변수\n",
    "    train_dataset=train_dataset,             # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset,               # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터에서 3번째 문서를 토크나이즈하고, 모델에 입력하여 예측 결과를 얻습니다.\n",
    "# 'return_tensors'를 'pt'로 설정하여 파이토치 텐서로 반환합니다.\n",
    "inputs = tokenizer(valid.document.iloc[3], return_tensors='pt')\n",
    "\n",
    "# 모델에 입력하여 로짓(logits)을 계산하고, 가장 높은 확률을 가진 클래스의 인덱스를 찾습니다.\n",
    "predicted_label = model(**inputs).logits.argmax(axis=-1)\n",
    "\n",
    "# 예측된 레이블을 출력합니다.\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋에서 처음 1000개의 샘플을 선택합니다.\n",
    "train_dataset = dataset['train'].select(range(1000))\n",
    "\n",
    "# 검증 데이터셋에서 처음 1000개의 샘플을 선택합니다.\n",
    "valid_dataset = dataset['validation'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델의 이름을 지정합니다. 여기서는 소문자 처리가 된 BERT 모델을 사용합니다.\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 미리 학습된 BERT 토크나이저를 로드합니다.\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row):\n",
    "    # 주어진 행(row)에서 전제(premise)와 가설(hypothesis)을 토크나이즈합니다.\n",
    "    return tokenizer(\n",
    "        row['premise'],                          # 전제 문장\n",
    "        row['hypothesis'],                       # 가설 문장\n",
    "        truncation=True,                        # 길이가 max_length를 초과하는 경우 잘라냄\n",
    "        padding=True,                           # 최대 길이에 맞춰 패딩 추가\n",
    "        return_tensors='pt',                   # 파이토치 텐서로 반환\n",
    "        max_length=128,                        # 최대 길이 설정\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋에 대해 전처리 함수를 적용하여 토크나이즈된 데이터셋을 생성합니다.\n",
    "tokenized_train_dataset = train_dataset.map(preprocessing, batched=True)\n",
    "\n",
    "# 검증 데이터셋에 대해 전처리 함수를 적용하여 토크나이즈된 데이터셋을 생성합니다.\n",
    "tokenized_valid_dataset = valid_dataset.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이즈된 훈련 데이터셋을 리스트로 변환하며, 레이블이 -1이 아닌 데이터만 포함합니다.\n",
    "tokenized_train_dataset = [\n",
    "    {'input_ids': row.get('input_ids'),                # 입력 ID\n",
    "     'token_type_ids': row.get('token_type_ids'),      # 토큰 타입 ID\n",
    "     'attention_mask': row.get('attention_mask'),       # 어텐션 마스크\n",
    "     'label': row.get('label')}                          # 레이블\n",
    "    for row in tokenized_train_dataset\n",
    "    if row.get('label') != -1                          # 레이블이 -1이 아닌 경우만 포함\n",
    "]\n",
    "\n",
    "# 토크나이즈된 검증 데이터셋을 리스트로 변환하며, 레이블이 -1이 아닌 데이터만 포함합니다.\n",
    "tokenized_valid_dataset = [\n",
    "    {'input_ids': row.get('input_ids'),                # 입력 ID\n",
    "     'token_type_ids': row.get('token_type_ids'),      # 토큰 타입 ID\n",
    "     'attention_mask': row.get('attention_mask'),       # 어텐션 마스크\n",
    "     'label': row.get('label')}                          # 레이블\n",
    "    for row in tokenized_valid_dataset\n",
    "    if row.get('label') != -1                          # 레이블이 -1이 아닌 경우만 포함\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터셋 초기화: 주어진 데이터를 저장합니다.\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환합니다.\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터 항목을 반환합니다.\n",
    "        return {\n",
    "            'input_ids': self.data[idx].get('input_ids'),          # 입력 ID\n",
    "            'token_type_ids': self.data[idx].get('token_type_ids'),# 토큰 타입 ID\n",
    "            'attention_mask': self.data[idx].get('attention_mask'), # 어텐션 마스크\n",
    "            'labels': self.data[idx].get('label'),                  # 레이블\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이즈된 훈련 데이터셋을 NLIDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = NLIDataset(tokenized_train_dataset)\n",
    "\n",
    "# 토크나이즈된 검증 데이터셋을 NLIDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "valid_dataset = NLIDataset(tokenized_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 20/63 [07:00<15:04, 21.04s/it]\n",
      "                                               \n",
      "100%|██████████| 63/63 [03:29<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0193928480148315, 'eval_runtime': 47.6332, 'eval_samples_per_second': 20.574, 'eval_steps_per_second': 1.302, 'epoch': 1.0}\n",
      "{'train_runtime': 209.456, 'train_samples_per_second': 4.765, 'train_steps_per_second': 0.301, 'train_loss': 1.1285164242699033, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 학습을 위한 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/NLI',                   # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                         # 평가 전략: 매 에폭마다 평가\n",
    "    learning_rate=2e-5,                           # 학습률\n",
    "    warmup_steps=50,                              # 웜업 단계 수\n",
    "    per_device_train_batch_size=16,               # 디바이스당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=16,                # 디바이스당 평가 배치 크기\n",
    "    num_train_epochs=1,                           # 훈련 에폭 수\n",
    "    weight_decay=0.01,                            # 가중치 감소\n",
    "    logging_dir='./logs',                         # 로그 저장 디렉토리\n",
    ")\n",
    "\n",
    "# 미리 학습된 BERT 모델을 로드하고, 레이블 수를 설정합니다. 여기서는 3개의 레이블이 있습니다.\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Trainer 객체를 생성하여 모델, 학습 인자, 훈련 데이터셋, 평가 데이터셋을 지정합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,                  # 훈련 데이터셋\n",
    "    eval_dataset=valid_dataset,                   # 검증 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 훈련된 모델을 저장합니다.\n",
    "model.save_pretrained('./bert_nli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KoNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNLI 데이터셋을 TSV 형식으로 읽어옵니다. \n",
    "# 데이터는 탭('\\t')으로 구분되어 있습니다.\n",
    "data = pd.read_csv('./data/snli_1.0_train.ko.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_ids(label: str):\n",
    "    # 레이블을 정수 ID로 변환하는 딕셔너리 정의\n",
    "    ids = {\n",
    "        'entailment': 0,     # 포함 관계\n",
    "        'neutral': 1,        # 중립\n",
    "        'contradiction': 2,  # 모순\n",
    "    }\n",
    "    # 주어진 레이블에 해당하는 ID를 반환, 없으면 None 반환\n",
    "    return ids.get(label)\n",
    "\n",
    "# 데이터프레임의 'gold_label' 컬럼에 대해 convert_label_to_ids 함수를 적용하여 정수 ID로 변환\n",
    "data.gold_label = data.gold_label.apply(lambda x: convert_label_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 랜덤 시드를 설정하여 결과의 재현성을 보장합니다.\n",
    "seed = 0\n",
    "\n",
    "# 데이터의 처음 5000개 샘플만 선택합니다.\n",
    "data = data.iloc[:5000]\n",
    "\n",
    "# 전체 데이터를 80% 훈련 데이터와 20% 테스트/검증 데이터로 분할합니다.\n",
    "train, temp = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "# 나머지 20%의 데이터(temp)를 다시 50%씩 나누어 검증(valid)과 테스트(test) 데이터로 분할합니다.\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "# 다국어 BERT 모델의 토크나이저를 로드합니다.\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-multilingual-uncased')\n",
    "\n",
    "# 훈련 데이터의 'sentence1'과 'sentence2'를 토큰화하여 'tokenized' 컬럼에 저장합니다.\n",
    "train['tokenized'] = train.apply(lambda x: tokenizer(\n",
    "    x['sentence1'],        # 첫 번째 문장\n",
    "    x['sentence2'],        # 두 번째 문장\n",
    "    truncation=True,       # 문장이 max_length를 초과할 경우 잘라냄\n",
    "    padding='max_length',  # max_length에 맞춰 패딩 추가\n",
    "    max_length=128,       # 최대 길이 설정\n",
    "    return_tensors='pt'   # PyTorch 텐서로 반환\n",
    "), axis=1)\n",
    "\n",
    "# 검증 데이터에 대해서도 동일한 방식으로 토큰화합니다.\n",
    "valid['tokenized'] = valid.apply(lambda x: tokenizer(\n",
    "    x['sentence1'],\n",
    "    x['sentence2'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    "), axis=1)\n",
    "\n",
    "# 테스트 데이터에 대해서도 동일한 방식으로 토큰화합니다.\n",
    "test['tokenized'] = test.apply(lambda x: tokenizer(\n",
    "    x['sentence1'],\n",
    "    x['sentence2'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorNLIDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터프레임을 클래스 속성으로 저장합니다.\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환합니다.\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 데이터를 가져옵니다.\n",
    "        temp = self.data.iloc[idx].tokenized\n",
    "        \n",
    "        # 토큰화된 데이터를 딕셔너리 형식으로 변환합니다.\n",
    "        temp = {\n",
    "            'input_ids': temp.get('input_ids')[0],           # 입력 ID\n",
    "            'token_type_ids': temp.get('token_type_ids')[0], # 토큰 유형 ID\n",
    "            'attention_mask': temp.get('attention_mask')[0], # 어텐션 마스크\n",
    "            'labels': self.data.iloc[idx].gold_label          # 레이블\n",
    "        }\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KorNLIDataset 클래스를 사용하여 훈련, 검증, 테스트 데이터셋을 초기화합니다.\n",
    "train_dataset = KorNLIDataset(train)  # 훈련 데이터셋\n",
    "valid_dataset = KorNLIDataset(valid)  # 검증 데이터셋\n",
    "test_dataset = KorNLIDataset(test)    # 테스트 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련을 위한 하이퍼파라미터를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./KoNLI/results/',            # 결과 저장 디렉토리\n",
    "    eval_strategy='epoch',                     # 평가 전략: 매 에포크마다 평가\n",
    "    learning_rate=2e-5,                       # 학습률\n",
    "    warmup_steps=50,                          # 웜업 스텝 수\n",
    "    per_device_train_batch_size=16,           # 장치당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=16,            # 장치당 평가 배치 크기\n",
    "    num_train_epochs=1,                        # 훈련 에포크 수\n",
    "    weight_decay=0.01,                        # 가중치 감소\n",
    "    logging_dir='./KoNLI/logs',               # 로그 저장 디렉토리\n",
    ")\n",
    "\n",
    "# BERT 모델을 사전 훈련된 가중치로 초기화합니다.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'google-bert/bert-base-multilingual-uncased', num_labels=3  # 클래스 수: 3 (entailment, neutral, contradiction)\n",
    ")\n",
    "\n",
    "# Trainer를 초기화하여 모델 훈련을 준비합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "    eval_dataset=valid_dataset,    # 검증 데이터셋\n",
    ")\n",
    "\n",
    "# 모델을 훈련합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 훈련된 모델을 지정한 경로에 저장합니다.\n",
    "model.save_pretrained('./bert_nli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a56403b4074e681ecb36004649c5fb19e7bcf1144081029ba5f2cf549331f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
