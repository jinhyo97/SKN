{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TFGPT2Model,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2\n",
    "\n",
    "reference: [GPT2](https://wikidocs.net/184363) <br>\n",
    "paper: [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'skt/kogpt2-base-v2' 모델을 불러와서 모델 객체를 생성합니다.\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# 'skt/kogpt2-base-v2'에 맞는 토크나이저를 불러와서 토크나이저 객체를 생성합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[27752,  7918,  8137,  9443, 12384,  9154, 11357]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 문장을 토크나이저를 사용하여 인코딩합니다.\n",
    "# 'return_tensors' 매개변수를 'pt'로 설정하여 PyTorch 텐서 형식으로 반환합니다.\n",
    "encoded = tokenizer('파이썬을 잘 학습하기 위해서는', return_tensors='pt')\n",
    "\n",
    "# 인코딩된 텐서를 출력합니다.\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬을 잘 학습하기 위해서는 먼저 자신의 생각을 논리적으로 표현하는 능력을 길러야 한다.\n",
      "논리적인 사고는 논리적 사고를 통해 문제를 해결하는 능력이다.\n",
      "이러한 능력은 문제 해결에 필요한 핵심 역량인 것이다.\n",
      "따라서 논술은 단순히 암기하는 것이 아니라 다양한 경험을 바탕으로 한 종합적인 사고와 함께 해야 하는 중요한 과정이다.\n",
      "논술에서 가장 중요하게 다루는 것은 바로 ‘논리적’이다.\n",
      "즉, 논술에서는 주어진 문제에 대한 정확한 이해와 더불어 문제의 핵심을 파악하는 데 초점을 맞추어야 한다는 말이다.\n",
      "또한, 이러한 과정을 거쳐야만 비로소 자신이 생각하는 바를 정확하게 이해할 수 있다.\n",
      "그렇다면 어떻게 하면 효과적인 대안이 될까?\n",
      "우선 어떤 방법으로 해결할 것인가?\n",
      "먼저 제시된 문제와 관련된 구체적인 사례를 중심으로 살펴보자.\n",
      "첫째, 제시문을 읽고 그 내용을 요약해 보자.\n",
      "둘째로, 글의 전체적인 흐름을 파악하자.\n",
      "셋째로 글을 읽는 사람의 입장에서 생각해보자.</d> 지난달 30일 오후 서울 강남구 삼성동 코엑스 컨벤션홀의 대형 스크린에는 '2018 평창 동계올림픽' 홍보 포스터가 내걸렸다.\n",
      "평창동계 올림픽은 오는 2022년 2월 9일부터 3월 8일까지 열흘간 열린다.\n",
      "홍보 포스터를 본 누리꾼들은 \"아직까지도 많은 분들이 관심을 갖고 계신 것 같다\"며 \"이번에도 좋은 결과가 있을 것으로 기대된다\"고 입을 모았다.\n",
      "이어 그는 이번 주말께 공식 홈페이\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용하여 텍스트를 생성합니다.\n",
    "output = model.generate(\n",
    "    # 인코딩된 입력 ID를 모델에 전달합니다.\n",
    "    encoded.get('input_ids'),\n",
    "    # 생성할 최대 길이를 설정합니다.\n",
    "    max_length=256,\n",
    "    # 반복되는 단어를 억제하기 위한 패널티 값을 설정합니다.\n",
    "    repetition_penalty=2.0,\n",
    "    # 캐시 사용을 설정하여 이전의 결과를 활용합니다.\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# 생성된 텍스트를 디코딩하여 사람이 읽을 수 있는 형식으로 변환합니다.\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 입력 문장\n",
    "inputs = '파이썬을 잘 학습하기 위해서는'\n",
    "\n",
    "# 입력 문장을 토크나이저를 사용하여 인코딩합니다.\n",
    "encoded = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "# 입력 문장의 길이가 64 이하인 동안 반복합니다.\n",
    "while len(encoded.get('input_ids')[0]) < 64:\n",
    "    # 모델을 사용하여 다음 토큰에 대한 출력을 생성합니다.\n",
    "    output = model(**encoded)\n",
    "    \n",
    "    # 마지막 위치의 로짓에서 상위 k개의 인덱스를 추출합니다.\n",
    "    indices = torch.topk(output.logits[0, -1], k=4).indices\n",
    "    \n",
    "    # 랜덤하게 선택된 인덱스를 기반으로 다음 토큰을 결정합니다.\n",
    "    top1 = np.random.choice(indices)\n",
    "    \n",
    "    # 선택된 토큰의 ID를 디코딩하여 문자로 변환합니다.\n",
    "    next_token = tokenizer.convert_ids_to_tokens([top1])[0].replace('▁', ' ')\n",
    "    \n",
    "    # 새로운 토큰을 기존 입력에 추가합니다.\n",
    "    inputs = ''.join([inputs, next_token])\n",
    "    \n",
    "    # 업데이트된 입력을 다시 인코딩합니다.\n",
    "    encoded = tokenizer(inputs, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬을 잘 학습하기 위해서는 ‘학습법’이 중요하다.\n",
      "이 책은 독해력과 문제 해결력, 문제 해결력, 사고력과 창의력을 키우는 학습법을 알려준다.\n",
      "독서는 자기 주도학습이 아니라 학습자의 자기 주도적인 학습을 돕는 학습법이다.\n",
      "이 책들은 독서를 하면서 자신의 생각을 정리하고 스스로 문제를 해결해 나가는 과정을 담았다는 점이 눈길을\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    'bitext/Bitext-customer-support-llm-chatbot-training-dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face의 Transformers 라이브러리에서 OpenAI의 GPT-2 토크나이저를 불러옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'openai-community/gpt2',\n",
    ")\n",
    "\n",
    "# 시작 토큰(BOS, Beginning of Sequence)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.bos_token='</s>'\n",
    "\n",
    "# 종료 토큰(EOS, End of Sequence)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.eos_token='</s>'\n",
    "\n",
    "# 패딩 토큰(PAD, Padding)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.pad_token='<pad>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋에서 훈련 데이터의 일부를 선택합니다.\n",
    "# 여기서는 첫 2000개의 샘플을 선택하여 훈련 데이터셋을 만듭니다.\n",
    "train_dataset = dataset['train'].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row):\n",
    "    sentence = ''.join([\n",
    "        '<usr> ',\n",
    "        row['instruction'],\n",
    "        ' <sys> ',\n",
    "        row['response'],\n",
    "    ])\n",
    "    sentence = ''.join(['</s>', sentence, '</s>'])\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        sentence,\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## list로 데이터가 들어가서 동작 안 됨\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3438\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3434\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3435\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3436\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3438\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3304\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing\u001b[39m(row):\n\u001b[1;32m----> 2\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<usr> \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstruction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m <sys> \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     11\u001b[0m         sentence,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# padding='max_length',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 1: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "## list로 데이터가 들어가서 동작 안 됨\n",
    "train_dataset = train_dataset.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋의 각 샘플에 대해 전처리 함수를 적용하여 토크나이즈된 데이터셋을 생성합니다.\n",
    "train_dataset = [\n",
    "    preprocessing(train_dataset[i])  # 각 샘플에 대해 preprocessing 함수를 호출합니다.\n",
    "    for i in range(len(train_dataset))  # 데이터셋의 모든 인덱스에 대해 반복합니다.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터셋 초기화: 입력 데이터를 저장합니다.\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이(샘플 수)를 반환합니다.\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 샘플을 가져옵니다.\n",
    "        temp = self.data[idx]\n",
    "        \n",
    "        # 결과 딕셔너리를 생성합니다.\n",
    "        result = {\n",
    "            'input_ids': temp.get('input_ids')[0],        # 입력 ID\n",
    "            'attention_mask': temp.get('attention_mask')[0],  # 주의 마스크\n",
    "            'labels': temp.get('input_ids')[0]             # 학습에 사용할 라벨 (입력 ID와 동일)\n",
    "        }\n",
    "\n",
    "        return result  # 결과 딕셔너리를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 훈련 데이터셋을 ChatbotDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = ChatbotDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 위한 설정을 정의합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./GPT2/chatbot/results/',  # 결과를 저장할 디렉토리\n",
    "    eval_strategy='epoch',                   # 평가 전략: 매 에포크마다 평가\n",
    "    learning_rate=2e-5,                     # 학습률\n",
    "    warmup_steps=50,                        # 워밍업 단계 수\n",
    "    per_device_train_batch_size=16,        # 훈련 시 배치 크기\n",
    "    per_device_eval_batch_size=16,         # 평가 시 배치 크기\n",
    "    num_train_epochs=1,                     # 총 훈련 에포크 수\n",
    "    weight_decay=0.01,                      # 가중치 감쇠\n",
    "    logging_dir='./GPT2/chatbot/logs',     # 로그를 저장할 디렉토리\n",
    ")\n",
    "\n",
    "# 미리 학습된 GPT-2 모델을 불러옵니다.\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "# Trainer 객체를 생성하여 모델 훈련을 준비합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,               # 훈련할 모델\n",
    "    args=training_args,       # 훈련 설정\n",
    "    train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "    # eval_dataset=valid_dataset,   # 검증 데이터셋 (주석 처리됨)\n",
    ")\n",
    "\n",
    "# 모델 훈련을 시작합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 훈련이 완료된 모델을 지정한 디렉토리에 저장합니다.\n",
    "model.save_pretrained('./GPT2/chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/ChatBotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임의 각 행에 대해 Q(질문)와 A(답변)를 결합하여 새로운 열 'joined_data'를 생성합니다.\n",
    "data['joined_data'] = data.apply(\n",
    "    lambda x: ''.join(['</s><usr>', x['Q'], ' <sys>', x['A'], '</s>']),  # 각 질문과 답변을 포맷합니다.\n",
    "    axis=1  # 행 단위로 적용합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face의 Transformers 라이브러리에서 SKT의 KoGPT-2 토크나이저를 불러옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# 종료 토큰(EOS, End of Sequence)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.eos_token = '</s>'\n",
    "\n",
    "# 시작 토큰(BOS, Beginning of Sequence)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.bos_token = '</s>'\n",
    "\n",
    "# 패딩 토큰(PAD, Padding)으로 사용할 토큰을 설정합니다.\n",
    "tokenizer.pad_token = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'joined_data' 열의 각 문장을 토크나이즈하여 'tokenized_data' 열에 저장합니다.\n",
    "data['tokenized_data'] = data.joined_data.apply(\n",
    "    lambda x: tokenizer(\n",
    "        x,                          # 토크나이즈할 입력 문장\n",
    "        padding='max_length',      # 최대 길이까지 패딩합니다.\n",
    "        max_length=256,            # 최대 길이를 256으로 설정합니다.\n",
    "        truncation=True,           # 길이를 초과할 경우 잘라냅니다.\n",
    "        return_tensors='pt',      # PyTorch 텐서 형식으로 반환합니다.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class KorChatbotDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 데이터셋 초기화: 입력 데이터를 저장합니다.\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이(샘플 수)를 반환합니다.\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 토크나이즈된 데이터를 가져옵니다.\n",
    "        temp = self.data.iloc[idx].tokenized_data\n",
    "        \n",
    "        # 결과 딕셔너리를 생성합니다.\n",
    "        result = {\n",
    "            'input_ids': temp.get('input_ids')[0],        # 입력 ID\n",
    "            'attention_mask': temp.get('attention_mask')[0],  # 주의 마스크\n",
    "            'labels': temp.get('input_ids')[0],             # 학습에 사용할 라벨 (입력 ID와 동일)\n",
    "        }\n",
    "\n",
    "        return result  # 결과 딕셔너리를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 70%의 훈련 데이터와 30%의 검증 데이터로 분할합니다.\n",
    "train, valid = train_test_split(data, test_size=0.3, random_state=0)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터를 각각 KorChatbotDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = KorChatbotDataset(train)\n",
    "valid_dataset = KorChatbotDataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 위한 설정을 정의합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./GPT2/kor_chatbot/results/',  # 결과를 저장할 디렉토리\n",
    "    eval_strategy='epoch',                       # 평가 전략: 매 에포크마다 평가\n",
    "    learning_rate=2e-5,                         # 학습률\n",
    "    warmup_steps=50,                            # 워밍업 단계 수\n",
    "    per_device_train_batch_size=32,            # 훈련 시 배치 크기\n",
    "    per_device_eval_batch_size=32,             # 평가 시 배치 크기\n",
    "    num_train_epochs=1,                         # 총 훈련 에포크 수\n",
    "    weight_decay=0.01,                          # 가중치 감쇠\n",
    "    logging_dir='./GPT2/kor_chatbot/logs',     # 로그를 저장할 디렉토리\n",
    ")\n",
    "\n",
    "# 미리 학습된 KoGPT-2 모델을 불러옵니다.\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Trainer 객체를 생성하여 모델 훈련을 준비합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,                # 훈련할 모델\n",
    "    args=training_args,        # 훈련 설정\n",
    "    train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "    eval_dataset=valid_dataset,   # 검증 데이터셋\n",
    ")\n",
    "\n",
    "# 모델 훈련을 시작합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 훈련이 완료된 모델을 지정한 디렉토리에 저장합니다.\n",
    "model.save_pretrained('./GPT2/kor_chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [1:26:17<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 미리 학습된 KoGPT-2 모델을 지정한 경로에서 불러옵니다.\n",
    "model = GPT2LMHeadModel.from_pretrained('./model/kor_chatbot')\n",
    "\n",
    "def generate_chat(user_input: str, topk: int = 5, max_length: int = 128):\n",
    "    # 사용자 입력을 포맷하여 모델에 전달할 입력 문자열을 생성합니다.\n",
    "    inputs = f'</s><usr>{user_input} <sys>'\n",
    "    \n",
    "    # 입력 문자열을 토크나이즈하여 텐서 형식으로 변환합니다.\n",
    "    encoded = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "    # 최대 길이에 도달할 때까지 반복하여 다음 단어를 생성합니다.\n",
    "    while len(encoded.get('input_ids')[0]) <= max_length:\n",
    "        # 모델을 사용하여 다음 토큰에 대한 출력을 생성합니다.\n",
    "        output = model(**encoded)\n",
    "        \n",
    "        # 마지막 위치의 로짓에서 상위 k개의 인덱스를 추출합니다.\n",
    "        indices = torch.topk(output.logits[0, -1], k=topk).indices\n",
    "        \n",
    "        # 랜덤하게 선택된 인덱스를 기반으로 다음 토큰을 결정합니다.\n",
    "        top1 = np.random.choice(indices)\n",
    "        \n",
    "        # 선택된 토큰의 ID를 디코딩하여 문자로 변환합니다.\n",
    "        next_token = tokenizer.convert_ids_to_tokens([top1])[0].replace('▁', ' ')\n",
    "        \n",
    "        # 새로운 토큰을 기존 입력에 추가합니다.\n",
    "        inputs = ''.join([inputs, next_token])\n",
    "        \n",
    "        # 업데이트된 입력을 다시 인코딩합니다.\n",
    "        encoded = tokenizer(inputs, return_tensors='pt')\n",
    "    \n",
    "    return inputs  # 최종 생성된 문자열을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'잘될 것 같아서 좋죠.” '"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용자 입력 '힘들어서 결혼할까봐'를 기반으로 챗봇의 응답을 생성합니다.\n",
    "# 최대 길이는 32로 설정합니다.\n",
    "result = generate_chat('힘들어서 결혼할까봐', max_length=32)\n",
    "result.split('<sys> ')[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a56403b4074e681ecb36004649c5fb19e7bcf1144081029ba5f2cf549331f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
