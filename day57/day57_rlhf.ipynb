{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    create_reference_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Human Feedback\n",
    "\n",
    "paper: https://arxiv.org/pdf/2203.02155\n",
    "\n",
    "![](https://huyenchip.com/assets/pics/rlhf/6-sft-rlhf.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 기존 모델의 한계 </font>\n",
    "\n",
    "**신뢰할 수 없는 답변이나, 유해하거나, 유저에게 도움이 안 되는 결과를 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RLHF\n",
    "\n",
    "사람의 선호를 reward로 하여 모델을 fine-tuning\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 학습 방법 </font>\n",
    "\n",
    "1. 레이블러는 입력 프롬프트에서 원하는 동작의 데모 제공. <br>\n",
    "지도 학습을 사용하여 이 데이터에서 사전 학습된 GPT-3 모델 fine-tuning.\n",
    "\n",
    "2. 주어진 입력에서 레이블러가 선호하는 output 사이의 비교 데이터 수집. <br>\n",
    "reward model을 학습시켜 선호하는 출력 예측\n",
    "\n",
    "3. PPO를 사용하여 reward model에 대한 policy 최적화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "1. Supervised Fine-Tuning (SFT): <br>\n",
    "레이블된 데이터로 GPT-3 모델을 미세 조정\n",
    "\n",
    "2. Reward Modeling (RM): <br>\n",
    "SFT 모델에서 prompt와 response를 기반으로 reward를 출력하도록 모델 학습\n",
    "방법:\n",
    "\n",
    "3. Reinforcement Learning (RL): <br>\n",
    "PPO를 사통해 SFT 모델을 미세 조정하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 긍정 답변 유도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 모델용 사전 학습된 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
    "\n",
    "# 패딩 토큰을 설정합니다. 여기서는 EOS(End Of Sentence) 토큰을 패딩 토큰으로 지정합니다.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# IMDb 데이터셋의 'train' split을 로드합니다.\n",
    "data = load_dataset('imdb', split='train')\n",
    "\n",
    "# 데이터셋의 'text' 열 이름을 'review'로 변경하여 일관성을 유지합니다.\n",
    "data = data.rename_columns({'text': 'review'})\n",
    "\n",
    "# 각 리뷰 텍스트 길이가 200자 이상인 샘플만 필터링합니다.\n",
    "data = data.filter(lambda x: len(x['review']) > 200)\n",
    "\n",
    "# 데이터셋의 텐서 형식을 'pt' (PyTorch 텐서)로 설정하여 모델에 바로 사용할 수 있도록 합니다.\n",
    "data.set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텍스트의 최소 및 최대 길이를 정의합니다.\n",
    "input_min_text_length = 2    # 최소 텍스트 길이\n",
    "input_max_text_length = 8    # 최대 텍스트 길이\n",
    "\n",
    "# LengthSampler 객체를 사용하여, 입력 텍스트 길이를 최소와 최대 길이 범위 내에서 무작위로 샘플링합니다.\n",
    "input_size = LengthSampler(input_min_text_length, input_max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb 데이터셋의 각 샘플에 대해 토큰화 및 변환을 수행하는 함수입니다.\n",
    "def tokenize(sample):\n",
    "    # 'review' 텍스트를 토크나이즈하고, 지정된 입력 길이(input_size)로 자릅니다.\n",
    "    sample['input_ids'] = tokenizer.encode(sample['review'])[: input_size()]\n",
    "    \n",
    "    # 토큰화된 'input_ids'를 다시 텍스트로 디코딩하여 'query' 키에 저장합니다.\n",
    "    sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "    \n",
    "    # 토큰화가 완료된 샘플을 반환합니다.\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b91ca93004ce294b51dc1878f0525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터셋의 각 샘플에 대해 'tokenize' 함수를 적용하여 텍스트를 토큰화하고 필요한 필드를 추가합니다.\n",
    "data = data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더의 배치 단위로 데이터를 구성하는 collator 함수입니다.\n",
    "def collator(data):\n",
    "    # 데이터 리스트(data)에서 동일한 키의 값을 모아 배치를 구성하는 딕셔너리를 반환합니다.\n",
    "    # 각 키에 대해 해당 키를 가진 모든 샘플의 값을 리스트로 묶습니다.\n",
    "    return dict((key, [datum[key] for datum in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c380d8a37a6468e82358c3ca6c52df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'lvwerra/gpt2-imdb'로 사전 학습된 Causal Language Model(자연어 생성 모델)을 로드합니다.\n",
    "# 강화 학습(RL)을 위해 value head를 추가하여 감정이나 보상 등 특정 값 예측이 가능하도록 합니다.\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('lvwerra/gpt2-imdb')\n",
    "\n",
    "# 학습 전 원래 사전 학습된 모델을 참조 모델로 로드합니다.\n",
    "# 이는 학습 후 모델이 원래 모델과 얼마나 다른지 비교하거나 평가할 때 사용됩니다.\n",
    "model_reference = AutoModelForCausalLMWithValueHead.from_pretrained('lvwerra/gpt2-imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\accelerator.py:443: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    }
   ],
   "source": [
    "# Proximal Policy Optimization(PPO) 구성 설정을 정의합니다.\n",
    "config = PPOConfig(\n",
    "    model_name='lvwerra/gpt2-imdb',  # 사용할 모델 이름\n",
    "    learning_rate=2e-5,              # 학습률\n",
    "    log_with='wandb',                # 실험 추적 도구로 Weights and Biases를 사용하도록 지정\n",
    ")\n",
    "\n",
    "# PPO 알고리즘을 사용하여 모델을 학습시키기 위한 트레이너(PPOTrainer)를 초기화합니다.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,               # PPO 구성 설정\n",
    "    model,                # 학습할 모델\n",
    "    model_reference,      # 참조 모델 (기존 사전 학습된 모델)\n",
    "    tokenizer,            # 토크나이저\n",
    "    dataset=data,         # 데이터셋 (IMDb 데이터셋을 전처리한 것)\n",
    "    data_collator=collator,  # 배치 구성을 위한 collator 함수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face의 파이프라인을 사용하여 감정 분석 모델을 초기화합니다.\n",
    "model_reward = pipeline(\n",
    "    'sentiment-analysis',  # 감정 분석 태스크 지정\n",
    "    model='lvwerra/distilbert-imdb',  # 사용할 사전 학습된 DistilBERT 모델 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 'bert-base-uncased'로 사전 학습된 BERT 모델을 로드하여 텍스트 분류 작업을 위한 모델을 초기화합니다.\n",
    "bert = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.994786262512207},\n",
       " {'label': 'POSITIVE', 'score': 0.0052136643789708614}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'this move is horrible'라는 텍스트에 대한 감정 분석을 수행합니다.\n",
    "# top_k 파라미터를 None으로 설정하여 모델이 모든 감정 클래스의 점수를 반환하도록 합니다.\n",
    "result = model_reward('this move is horrible', top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 텍스트의 최소 및 최대 길이를 정의합니다.\n",
    "output_min_length = 4    # 최소 출력 길이\n",
    "output_max_length = 20   # 최대 출력 길이\n",
    "\n",
    "# LengthSampler 객체를 사용하여, 출력 텍스트 길이를 최소와 최대 길이 범위 내에서 무작위로 샘플링합니다.\n",
    "output_length_sampler = LengthSampler(\n",
    "    output_min_length,     # 출력 길이의 최소값\n",
    "    output_max_length      # 출력 길이의 최대값\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 트레이너의 데이터 로더를 통해 각 에포크와 배치를 반복합니다.\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    # 현재 배치에서 입력 쿼리의 ID를 가져옵니다.\n",
    "    queries = batch.get('input_ids')\n",
    "\n",
    "    response_tensors = []  # 생성된 응답을 저장할 리스트\n",
    "\n",
    "    # 각 쿼리에 대해 응답을 생성합니다.\n",
    "    for query in queries:\n",
    "        # 출력 길이를 샘플링합니다.\n",
    "        generation_length = output_length_sampler()\n",
    "        \n",
    "        # 모델을 사용하여 응답을 생성합니다.\n",
    "        response = ppo_trainer.generate(\n",
    "            query,\n",
    "            min_length=-1,           # 생성할 최소 길이\n",
    "            top_k=0,                 # Top-k 샘플링 비활성화\n",
    "            top_p=1,                 # Top-p 샘플링을 사용하여 무작위 생성\n",
    "            do_sample=True,          # 샘플링 모드 활성화\n",
    "            pad_token_id=tokenizer.eos_token_id,  # 패딩 토큰 ID 설정\n",
    "            max_new_tokens=generation_length  # 생성할 최대 새로운 토큰 수\n",
    "        )\n",
    "        \n",
    "        # 응답의 길이를 계산하여 응답 텐서를 저장합니다.\n",
    "        response_length = len(response) - len(query)\n",
    "        response_tensors.append(response[-response_length:])\n",
    "    \n",
    "    # 생성된 응답을 디코딩하여 배치에 추가합니다.\n",
    "    batch['response'] = [tokenizer.decode(response.squeeze())\n",
    "                         for response in response_tensors]\n",
    "    \n",
    "    # 쿼리와 응답을 합쳐서 텍스트 리스트를 생성합니다.\n",
    "    texts = [''.join([query, response])\n",
    "             for query, response\n",
    "             in zip(batch.get('query'), batch.get('response'))]\n",
    "    \n",
    "    # 합쳐진 텍스트에 대한 감정 점수를 계산합니다.\n",
    "    sentiment_scores = model_reward(\n",
    "        texts,\n",
    "        top_k=None,              # 모든 감정 클래스의 점수를 반환\n",
    "        batch_size=32,          # 배치 크기 설정\n",
    "    )\n",
    "    \n",
    "    # 긍정 점수를 추출하여 보상으로 사용합니다.\n",
    "    positive_scores = [\n",
    "        item.get('score')\n",
    "        for sentiment_score in sentiment_scores\n",
    "        for item in sentiment_score\n",
    "        if item.get('label') == 'POSITIVE'\n",
    "    ]\n",
    "    \n",
    "    # 보상을 텐서로 변환합니다.\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    # PPO 트레이너의 한 스텝을 진행하고 통계를 기록합니다.\n",
    "    stats = ppo_trainer.step(queries, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, response_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_form(text, model, tokenizer):\n",
    "    # 입력 텍스트를 토크나이저를 사용하여 텐서 형태로 변환합니다.\n",
    "    tokenized = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # 모델의 그래디언트 계산을 비활성화하여 메모리 사용을 최적화합니다.\n",
    "    with torch.no_grad():\n",
    "        # 토큰화된 입력을 모델에 전달하여 예측 결과를 얻습니다.\n",
    "        outputs = model(**tokenized)\n",
    "    \n",
    "    # 모델의 로짓에 소프트맥스 함수를 적용하여 확률 분포를 계산합니다.\n",
    "    probs = F.softmax(outputs.logits)[0]\n",
    "    \n",
    "    # 긍정 및 부정의 확률 점수를 기반으로 결과를 구성합니다.\n",
    "    positive = {'label': 'POSITIVE', 'score': probs[1].item()}  # 긍정 점수\n",
    "    negative = {'label': 'NEGATIVE', 'score': probs[0].item()}  # 부정 점수\n",
    "    \n",
    "    # 긍정 및 부정 결과를 리스트 형태로 반환합니다.\n",
    "    return [positive, negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_26376\\1922592531.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(outputs.logits)[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.40136656165122986},\n",
       " {'label': 'NEGATVE', 'score': 0.5986334085464478}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용할 BERT 모델과 토크나이저의 이름을 정의합니다.\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# 사전 학습된 BERT 모델을 로드합니다. 이 모델은 텍스트 분류를 위한 것입니다.\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 해당 모델에 맞는 토크나이저를 로드합니다.\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 감정 분석을 수행할 입력 텍스트를 정의합니다.\n",
    "text = 'how\\'s your days going on?'\n",
    "\n",
    "# 정의한 텍스트에 대해 감정 분석을 수행하고 결과를 반환합니다.\n",
    "sentiment_result = make_sentiment_form(text, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위의 rlhf의 코드에서 model_reward를 직접 학습한 bert model로 변경\n",
    "## data: imdb\n",
    "## model: AutoModelForSequenceClassification\n",
    "\n",
    "## 단계\n",
    "## - 1: imdb data로 bert 모델 학습\n",
    "## BERT 모델과 토크나이저를 초기화합니다.\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "## IMDb 데이터셋의 각 샘플을 전처리하는 함수입니다.\n",
    "def preprocessing(sample):\n",
    "    return tokenizer(\n",
    "        sample['text'],\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "## 데이터셋에 전처리 함수를 적용하고 훈련 및 검증 데이터셋을 나눕니다.\n",
    "data = load_dataset('imdb')\n",
    "train = data.map(preprocessing, batched=True)\n",
    "eval = train.select(range(20000, 25000)) # 평가 데이터셋: 20000~25000번째 샘플\n",
    "train = train.select(range(20000))       # 훈련 데이터셋: 처음 20000개의 샘플\n",
    "\n",
    "## 모델 훈련을 위한 하이퍼파라미터를 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./imdb',            # 모델과 로그 저장 경로\n",
    "    eval_strategy='epoch',          # 매 에포크마다 평가를 수행\n",
    "    learning_rate=2e-5,             # 학습률\n",
    "    warmup_steps=50,                # 웜업 스텝 설정\n",
    "    per_device_train_batch_size=16, # 훈련 배치 크기\n",
    "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
    "    num_train_epochs=2,             # 에포크 수\n",
    "    weight_decay=0.01,              # 가중치 감소율\n",
    "    logging_dir='./imdb',           # 로그 디렉토리\n",
    ")\n",
    "\n",
    "## Trainer를 초기화하여 BERT 모델을 IMDb 데이터셋으로 학습할 준비를 합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,  # 훈련 데이터셋 설정\n",
    "    eval_dataset=eval,    # 평가 데이터셋 설정\n",
    ")\n",
    "\n",
    "## 모델을 훈련합니다.\n",
    "trainer.train()\n",
    "\n",
    "## 훈련이 완료된 모델을 저장합니다.\n",
    "model.save_pretrained('./imdb')\n",
    "\n",
    "## - 2: 학습된 모델을 model_reward로 로드\n",
    "##      - 학습된 모델을 'model_reward'로 로드하여, 이후 RLHF에서 보상 모델로 사용합니다.\n",
    "model_reward = AutoModelForSequenceClassification.from_pretrained('./imdb')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "## - 3: model의 결괏값을 아래의 형태로 변경\n",
    "##      [{'label': label, 'score': score}, {'label': label, 'score': score}]\n",
    "##      - positive: 긍정 예측 점수, negative: 부정 예측 점수\n",
    "def make_sentiment_form(text, model, tokenizer):\n",
    "    tokenized = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "    probs = F.softmax(outputs.logits)[0]\n",
    "    \n",
    "    positive = {'label': 'POSITIVE', 'score': probs[1].item()}\n",
    "    negative = {'label': 'NEGATVE', 'score': probs[0].item()}\n",
    "    \n",
    "    return [positive, negative]\n",
    "\n",
    "## 4: RLHF 단계 - 학습을 위한 데이터 토크나이즈\n",
    "input_min_text_length = 2\n",
    "input_max_text_length = 8\n",
    "input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "## 샘플을 토크나이즈하고 쿼리 텍스트를 저장합니다.\n",
    "def tokenize(sample):\n",
    "    sample['input_ids'] = tokenizer.encode(sample['review'])[: input_size()]\n",
    "    sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "\n",
    "    return sample\n",
    "\n",
    "# 데이터셋에 tokenize 함수를 적용하여 샘플을 토크나이즈합니다.\n",
    "data = data.map(tokenize)\n",
    "\n",
    "# 데이터 배치를 위한 collator를 설정합니다. 각 키에 대해 데이터에서 해당 키의 값을 리스트로 수집합니다.\n",
    "def collator(data):\n",
    "    return dict((key, [datum[key] for datum in data]) for key in data[0])\n",
    "\n",
    "# PPO 학습을 위해 모델과 참조 모델을 초기화합니다.\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('lvwerra/gpt2-imdb')\n",
    "model_reference = AutoModelForCausalLMWithValueHead.from_pretrained('lvwerra/gpt2-imdb')\n",
    "\n",
    "# PPO 학습 구성 설정\n",
    "config = PPOConfig(\n",
    "    model_name='lvwerra/gpt2-imdb',  # 모델 이름\n",
    "    learning_rate=2e-5,               # 학습률 설정\n",
    "    log_with='wandb',                 # 로그 수집 방식\n",
    ")\n",
    "\n",
    "# PPOTrainer 초기화 - PPO 알고리즘을 사용하여 모델을 학습할 준비를 합니다.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    model_reference,\n",
    "    tokenizer,\n",
    "    dataset=data,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# 생성 응답의 길이를 설정하는 샘플러를 초기화합니다.\n",
    "output_min_length = 4\n",
    "output_max_length = 20\n",
    "output_length_sampler = LengthSampler(\n",
    "    output_min_length,\n",
    "    output_max_length,\n",
    ")\n",
    "\n",
    "# RLHF 학습 루프 - 각 에포크마다 보상을 계산하고 모델을 업데이트합니다.\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    queries = batch.get('input_ids')  # 현재 배치의 쿼리 입력을 가져옵니다.\n",
    "\n",
    "    # 응답 생성 및 응답 텐서를 수집합니다.\n",
    "    response_tensors = []\n",
    "    for query in queries:\n",
    "        generation_length = output_length_sampler()  # 생성할 응답의 길이를 샘플링합니다.\n",
    "        response = ppo_trainer.generate(\n",
    "            query,\n",
    "            min_length=-1,\n",
    "            top_k=0,\n",
    "            top_p=1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,  # 패딩 토큰 설정\n",
    "            max_new_tokens=generation_length  # 최대 생성 토큰 수 설정\n",
    "        )\n",
    "        response = response[0]  # 첫 번째 응답을 선택합니다.\n",
    "        response_length = len(response) - len(query)  # 응답의 길이를 계산합니다.\n",
    "        response_tensors.append(response[-response_length:])  # 응답을 수집합니다.\n",
    "\n",
    "    # 응답을 디코딩하여 배치에 추가합니다.\n",
    "    batch['response'] = [tokenizer.decode(response.squeeze())\n",
    "                         for response in response_tensors]\n",
    "\n",
    "    # 쿼리와 응답 텍스트를 합쳐 감성 분석을 수행하고 긍정 점수를 추출합니다.\n",
    "    texts = [''.join([query, response])\n",
    "             for query, response\n",
    "             in zip(batch.get('query'), batch.get('response'))]\n",
    "    sentiment_scores = [\n",
    "      make_sentiment_form(text, model_reward, bert_tokenizer)\n",
    "      for text in texts\n",
    "    ]\n",
    "    \n",
    "    # 긍정 감정 점수를 추출합니다.\n",
    "    positive_scores = [\n",
    "        item.get('score')\n",
    "        for sentiment_score in sentiment_scores\n",
    "        for item in sentiment_score\n",
    "        if item.get('label') == 'POSITIVE'\n",
    "    ]\n",
    "\n",
    "    # 보상을 생성하여 PPO 트레이너의 학습 단계로 전달합니다.\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    # 학습 통계를 기록하고 PPO 학습 단계를 실행합니다.\n",
    "    stats = ppo_trainer.step(queries, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)  # 학습 통계와 보상을 로그합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 비교\n",
    "\n",
    "num_samples = 16  # 샘플링할 데이터 개수를 16개로 설정\n",
    "game_data = {}  # 결과 데이터를 저장할 딕셔너리 생성\n",
    "data.set_format('pandas')  # 데이터셋 형식을 pandas DataFrame으로 설정\n",
    "df_samples = data[:].sample(num_samples)  # 데이터셋에서 16개의 샘플을 랜덤으로 추출하여 DataFrame으로 저장\n",
    "game_data['query'] = df_samples['query'].tolist()  # 샘플의 'query' 열을 리스트로 변환하여 game_data에 저장\n",
    "queries = df_samples['input_ids'].tolist()  # 샘플의 'input_ids' 열을 리스트로 변환하여 queries에 저장\n",
    "device = 'cuda'  # 모델을 'cuda' 장치에서 실행하도록 설정\n",
    "\n",
    "responses_reference, responses = [], []  # 참고 응답과 실제 응답을 저장할 리스트 초기화\n",
    "\n",
    "for i in range(num_samples):  # 샘플 개수만큼 반복\n",
    "    query = torch.tensor([queries[i]]).to(device)  # 현재 쿼리를 텐서로 변환하여 GPU 장치로 이동\n",
    "\n",
    "    generation_length = output_length_sampler()  # 출력할 응답 길이를 샘플링\n",
    "\n",
    "    # 참고 모델로 응답 생성\n",
    "    response_reference = model_reference.generate(\n",
    "        query,\n",
    "        min_length=-1,  # 최소 생성 길이를 -1로 설정\n",
    "        top_k=0,  # top-k 샘플링 비활성화\n",
    "        top_p=1,  # top-p 샘플링 비활성화\n",
    "        do_sample=True,  # 샘플링 방식으로 응답 생성\n",
    "        pad_token_id=tokenizer.eos_token_id,  # padding에 사용할 토큰 설정\n",
    "        max_new_tokens=generation_length,  # 생성할 최대 토큰 길이 설정\n",
    "    )[0]\n",
    "    response_length = len(response_reference) - len(query)  # 생성된 응답의 실제 길이 계산\n",
    "    responses_reference.append(response_reference[-response_length:])  # 참고 응답 리스트에 추가\n",
    "\n",
    "    # 모델로 실제 응답 생성\n",
    "    response = model.generate(\n",
    "        query,\n",
    "        min_length=-1,  # 최소 생성 길이를 -1로 설정\n",
    "        top_k=0,  # top-k 샘플링 비활성화\n",
    "        top_p=1,  # top-p 샘플링 비활성화\n",
    "        do_sample=True,  # 샘플링 방식으로 응답 생성\n",
    "        pad_token_id=tokenizer.eos_token_id,  # padding에 사용할 토큰 설정\n",
    "        max_new_tokens=generation_length,  # 생성할 최대 토큰 길이 설정\n",
    "    )[0]\n",
    "    response_length = len(response) - len(query)  # 생성된 응답의 실제 길이 계산\n",
    "    responses.append(response[-response_length:])  # 실제 응답 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `responses_reference`의 각 응답을 디코딩하여 수정 전 원래 응답을 얻음.\n",
    "game_data['response (before)'] = [\n",
    "    tokenizer.decode(response_reference) for response_reference in responses_reference\n",
    "]\n",
    "\n",
    "# `responses`의 각 응답을 디코딩하여 수정 후 응답을 얻음.\n",
    "game_data['response (after)'] = [\n",
    "    tokenizer.decode(response) for response in responses\n",
    "]\n",
    "\n",
    "# 각 쿼리와 해당하는 수정 전 응답을 연결하여 텍스트 샘플 리스트를 만듦.\n",
    "texts = [\n",
    "    ''.join([query, response])\n",
    "    for query, response\n",
    "    in zip(game_data.get('query'), game_data.get('response (before)'))\n",
    "]\n",
    "\n",
    "# 수정 전 쿼리-응답 텍스트 샘플에 대해 감정 점수를 계산함.\n",
    "# `top_k=None` 옵션은 모든 감정 레이블의 점수를 반환함.\n",
    "sentiment_scores = model_reward(texts, top_k=None)\n",
    "\n",
    "# 'POSITIVE' 레이블로 지정된 감정 점수를 필터링하여 추출함.\n",
    "positive_scores = [\n",
    "    item.get('score')\n",
    "    for sentiment_score in sentiment_scores\n",
    "    for item in sentiment_score\n",
    "    if item.get('label') == 'POSITIVE'\n",
    "]\n",
    "\n",
    "# 수정 전 응답에 대한 긍정 감정 점수를 game_data 딕셔너리에 저장함.\n",
    "game_data['rewards (before)'] = positive_scores\n",
    "\n",
    "# 수정 후 응답에 대해서도 동일한 과정을 반복함.\n",
    "# 각 쿼리와 해당하는 수정 후 응답을 연결하여 텍스트 샘플 리스트를 만듦.\n",
    "texts = [\n",
    "    ''.join([query, response])\n",
    "    for query, response\n",
    "    in zip(game_data.get('query'), game_data.get('response (after)'))\n",
    "]\n",
    "\n",
    "# 수정 후 쿼리-응답 텍스트 샘플에 대해 감정 점수를 계산함.\n",
    "sentiment_scores = model_reward(texts, top_k=None)\n",
    "\n",
    "# 'POSITIVE' 레이블로 지정된 감정 점수를 필터링하여 추출함.\n",
    "positive_scores = [\n",
    "    item.get('score')\n",
    "    for sentiment_score in sentiment_scores\n",
    "    for item in sentiment_score\n",
    "    if item.get('label') == 'POSITIVE'\n",
    "]\n",
    "\n",
    "# 수정 후 응답에 대한 긍정 감정 점수를 game_data 딕셔너리에 저장함.\n",
    "game_data['rewards (after)'] = positive_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_data 딕셔너리를 데이터프레임으로 변환하여 결과를 표 형태로 생성함\n",
    "results = pd.DataFrame(game_data)\n",
    "\n",
    "# 데이터프레임 출력\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
