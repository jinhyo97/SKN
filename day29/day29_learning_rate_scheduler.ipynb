{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler\n",
    "\n",
    "딥러닝 모델을 훈련 시, learning rate를 조정하는 기법. <br>\n",
    "learning rate는 네트워크의 가중치를 얼마나 크게 업데이트할지를 결정하는 하이퍼파라미터로, <br>\n",
    "적절한 학습률을 설정하는 것은 모델이 빠르고 안정적으로 수렴하는 데 매우 중요. <br>\n",
    "\n",
    "learning rate scheduler는 훈련 과정에서 학습률을 동적으로 조정하여 모델 성능을 개선하는 데 기여 <br>\n",
    "- Step Decay: 학습률을 일정한 에포크마다 고정된 비율로 감소\n",
    "- Exponential Decay: 학습률을 지수 함수에 따라 점진적으로 감소\n",
    "- Cyclic Learning Rate: 학습률을 주기적으로 증가와 감소를 반복\n",
    "- Plateau: 검증 손실이 개선되지 않는 경우 학습률을 감소\n",
    "- Combination: 다양한 스케줄러를 조합\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 20pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "configure_optimizers에 scheduler를 추가. <br>\n",
    "return에 scheduler 객체 return 추가. <br>\n",
    "\n",
    "> ```python\n",
    "> def configure_optimizers(self):\n",
    ">     optimizer = optim.Adam(\n",
    ">         self.model.parameter(),\n",
    ">         lr=self.learning_rate,\n",
    ">     )\n",
    ">     scheduler = lr_scheduler.OneCycleLR(\n",
    ">         optimizer,\n",
    ">         max_lr=1e-3,\n",
    ">         total_steps=self.trainer.estimated_stepping_batches,\n",
    ">     )\n",
    ">     return optimizer, scheduler\n",
    "> ````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2BFko%2FbtqOUlVgZU3%2FOA2OWctZ3wF2Eakhpaf1Y1%2Fimg.png)\n",
    "\n",
    "learning rate를 step_size(epoch)마다 감마로 감쇠. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- step_size (int) – 어느 주기로 learning rate를 감소시킬 건지 설정\n",
    "- gamma (float) – 감소될 학습률 (default: 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExponentialLR\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7kmnL%2FbtqOMp5Xu77%2FE8Jk5ydKXYsw71nZcSN831%2Fimg.png)\n",
    "\n",
    "매 epoch마다 이전 learning rate에 gamma만큼 곱하여 감쇄\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = ExponentialLR(optimizer, gamma=0.1)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- gamma (float) – 감소될 학습률 (default: 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CyclicLR\n",
    "\n",
    "|triangular|triangular2|exp_range|\n",
    "|----------|-----------|---------|\n",
    "|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FVwbCj%2FbtqON8iY9Fp%2FVNODkZFUIbtG3I0wEkEiK1%2Fimg.png)|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FRb1s0%2FbtqO0glZvNG%2FwHB48CykF7jt4zkzk7ROSk%2Fimg.png)|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbtIC6z%2FbtqON9vvgpt%2FL5qQdBWHFlzg6GU9Bh2vk1%2Fimg.png)|\n",
    "\n",
    "일정 주기로 설정한 최솟값과 최댓값 사이의 learning rate를 갖도록 함 <br>\n",
    "learning rate를 매 배치 후 변경 <br>\n",
    "- triangular: 선형적으로 증가하고 감소\n",
    "- triangular2: 학습률이 선형적으로 증가하고 감소하지만, 사이클이 진행됨에 따라 최댓값이 줄어듦\n",
    "- exp_range: 지수적으로 증가하고 감소\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- base_lr: 사이클의 최소 learning rate\n",
    "- max_lr: 사이클의 최대 learning rate\n",
    "- step_size_up: learning rate가 base_lr에서 max_lr로 증가하는 데 필요한 단계 수\n",
    "- step_size_down: learning rate가 max_lr에서 다시 base_lr로 감소하는 데 필요한 단계 수.\n",
    "- mode: 사이클의 모양을 지정하는 매개변수\n",
    "    - triangular, triangular2, exp_range 중 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb25iNZ%2FbtqOKXvn072%2FawCat07o8yb9MHLRHeFUek%2Fimg.png)\n",
    "\n",
    "모델의 성능이 개선되지 않을 때 학습률을 자동으로 감소시켜 학습을 안정화하고 성능을 향상시키기 위한 방법\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 16pt;\"> 파이썬 코드 </span>\n",
    "\n",
    "> ```python\n",
    "> scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=0)\n",
    "> ````\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size: 14pt;\"> Paramter </span>\n",
    "\n",
    "- mode: 모니터링할 metric의 방향을 설정\n",
    "    - min은 메트릭이 최소일 때, max는 메트릭이 최대일 때 학습률 감소\n",
    "- factor: 학습률 감소 비율로 factor가 0.1이라면 이전 learning rate에 0.1을 곱함\n",
    "- patience: 메트릭이 개선되지 않을 때까지 기다리는 에포크 수\n",
    "- min_lr: 최소 learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_category_into_integer(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    주어진 DataFrame의 특정 열들을 범주형에서 정수형으로 변환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): 변환할 데이터프레임\n",
    "    - columns (list): 범주형에서 정수형으로 변환할 열 이름의 리스트\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: 변환된 데이터프레임\n",
    "    - dict: 각 열에 대해 적합한 LabelEncoder 객체를 포함하는 딕셔너리\n",
    "    \"\"\"\n",
    "    label_encoders = {}  # 각 열의 LabelEncoder 객체를 저장할 딕셔너리입니다.\n",
    "    \n",
    "    for column in columns:\n",
    "        # 각 열에 대해 LabelEncoder 객체를 생성합니다.\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # LabelEncoder를 사용하여 해당 열의 범주형 데이터를 정수형으로 변환합니다.\n",
    "        df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
    "        \n",
    "        # 변환된 LabelEncoder 객체를 딕셔너리에 저장합니다.\n",
    "        label_encoders.update({column: label_encoder})\n",
    "    \n",
    "    # 변환된 데이터프레임과 LabelEncoder 객체를 포함하는 딕셔너리를 반환합니다.\n",
    "    return df, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_22328\\3012579943.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 2 0 2 0 2 2 1 2 0 2 2 2 1 2 2 1 1 2 0 2 2 0 0 1 0 0 2 2 2 2 1 1 2 2 2\n",
      " 2 0 1 0 1 2 1 2 2 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 2 2 0\n",
      " 0 1 1 2 0 2 2 2 2 2 0 2 2 2 2 2 2 1 0 2 1 1 1 0 2 2 2 2 2 2 1 1 1 0 0 2 0\n",
      " 2 2 2 1 1 2 2 1 1 1 0 2 2 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 0 2 0 1 2 2 1 2 0\n",
      " 2 2 1 1 2 1 0 0 2 1 2 2 2 2 2 2 2 2 0 2 1 2 1 0 2 1 0 1 2 1 2 0 2 1 2 1 0\n",
      " 2 1 2 1 1 1 1 1 1 2 2 0 2 1 0 1 2 0 2 2 2 0 0 1 2 0 0 1 2 2 0 0 2 1 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 0 0 1 2 2 2 0 0 2 0 0 1 0 0 0 1 2 1 2 1 1 0 0 2 2 1\n",
      " 1 0 2 1 2 0 0 0 2 0 0 2 0 1 0 1 1 1 1 1 2 2 2 2 2 2 0 1 2 1 2 2 2 0 0 0 2\n",
      " 2 0 2 2 0 2 2 0 2 2 0 1 2 1 1 0 2 2 0 2 2 2 1 1 1 2 2 2 2 2 1 2 1 2 0 2 1\n",
      " 1 1 2 2 2 2 2 1 1 2 0 1 2 0 0 2 1 0 1 1 2 2 1 0 1 0 2 0 1 0 0 2 0 1 0 2 0\n",
      " 1 2 0 2 2 1 1 2 1 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 0 2 2 2 2 0 0 1 2 2 2 0\n",
      " 0 2 0 1 1 2 0 2 0 2 1 2 1 1 2 2 1 0 0 0 0 2 2 1 0 0 1 2 1 0 1 2 2 0 0 0 2\n",
      " 2 1 2 2 2 2 1 0 0 2 2 1 0 2 1 0 1 0 0 1 0 2 2 0 2 1 2 2 0 1 2 0 2 2 0 1 0\n",
      " 2 2 1 2 2 1 1 2 0 2 2 2 0 1 0 2 0 2 0 2 1 2 1 2 2 0 2 2 0 2 0 2 1 2 2 1 2\n",
      " 1 0 0 2 0 2 2 1 1 2 1 0 1 1 2 2 2 2 0 0 2 2 1 1 2 2 2 0 0 2 2 0 1 2 0 2 0\n",
      " 0 2 2 2 1 1 0 0 0 0 2 1 2 0 1 2 1 2 1 1 0 2 1 1 2 0 2 1 1 2 2 0 0 0 2 2 0\n",
      " 2 1 0 2 1 2 2 2 1 1 2 1 2 0 2 2 0 2 0 2 2 2 2 1 1 2 2 0 2 0 0 2 2 2 2 2 0\n",
      " 1 2 1 0 2 2 2 1 1 0 2 2 2 0 2 1 0 2 2 1 2 2 2 1 2 2 0 2 0 2 2 1 2 1 2 2 0\n",
      " 2 2 2 1 0 2 2 2 2 1 2 2 2 0 1 2 0 0 2 2 1 0 1 1 1 0 2 2 0 0 2 1 2 2 2 0 1\n",
      " 2 2 1 2 2 1 0 0 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 1.1 K  | train\n",
      "----------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:00<00:00, 41.67it/s, v_num=0, acc/val_acc=0.500, loss/val_loss=0.714, acc/train_acc=0.744, loss/train_loss=0.559]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:128\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mTitanicDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurvived\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 259\u001b[0m\n\u001b[0;32m    245\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    246\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,  \u001b[38;5;66;03m# 학습할 최대 에포크 수\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    254\u001b[0m )\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# PyTorch Lightning Trainer를 사용하여 모델 학습을 시작\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# 'titanic_module'은 학습할 모델과 학습 및 검증 루프를 정의한 LightningModule 인스턴스\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# 'titanic_data_module'은 데이터셋을 로드하고 데이터로더를 제공하는 LightningDataModule 인스턴스\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitanic_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 학습할 모델 인스턴스\u001b[39;49;00m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitanic_data_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 데이터셋과 데이터로더를 제공하는 데이터 모듈 인스턴스\u001b[39;49;00m\n\u001b[0;32m    262\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# 사용 가능한 GPU가 있는 경우 'cuda'를, 그렇지 않으면 'cpu'를 사용하도록 장치를 설정합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 64\n",
    "\n",
    "# 에포크 수 (Epochs): 전체 데이터셋을 몇 번 반복하여 학습할 것인지 지정\n",
    "epochs = 1000\n",
    "\n",
    "# 학습률 (Learning Rate): 모델 파라미터를 업데이트할 때 사용되는 학습률\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 은닉층의 뉴런 수 (Hidden Dimension): 은닉층의 차원 또는 뉴런 수\n",
    "hidden_dim = 64\n",
    "\n",
    "\n",
    "# 'titanic' 데이터셋을 로드\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# 데이터프레임의 각 열에서 결측값의 수를 계산하여 출력\n",
    "titanic.isna().sum()\n",
    "\n",
    "# 'deck' 열을 문자열형으로 변환\n",
    "titanic.deck = titanic.deck.astype(str)\n",
    "\n",
    "# 데이터프레임에서 결측값이 있는 모든 행을 제거\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "# 데이터프레임의 범주형 열을 정수형으로 변환\n",
    "# 'sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive' 열을 정수형으로 변환\n",
    "titanic, _ = convert_category_into_integer(titanic, ('sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'))\n",
    "\n",
    "# 데이터프레임의 모든 열을 float32 데이터 타입으로 변환\n",
    "titanic = titanic.astype(np.float32)\n",
    "\n",
    "# 원본 데이터셋을 학습용 데이터와 임시 데이터로 분할\n",
    "# 전체 데이터의 40%를 임시 데이터로, 60%를 학습용 데이터로 사용\n",
    "train, temp = train_test_split(titanic, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'survived' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.drop('survived', axis=1).iloc[idx].values).float()\n",
    "        \n",
    "        # 'survived' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.tensor(self.data.iloc[idx].survived).long()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "\n",
    "# 학습 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "train_dataset = TitanicDataset(train)\n",
    "\n",
    "# 검증 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "valid_dataset = TitanicDataset(valid)\n",
    "\n",
    "# 테스트 데이터를 TitanicDataset 클래스를 사용하여 데이터셋 객체로 변환합니다.\n",
    "test_dataset = TitanicDataset(test)\n",
    "\n",
    "\n",
    "class TitanicDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare(self, train_dataset, valid_dataset, test_dataset):     \n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":      \n",
    "            self.train_data = self.train_dataset\n",
    "            self.valid_data = self.valid_dataset\n",
    "\n",
    "        if stage == \"test\":     \n",
    "            self.test_data = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.valid_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "\n",
    "titanic_data_module = TitanicDataModule(batch_size=batch_size)\n",
    "titanic_data_module.prepare(train_dataset, valid_dataset, test_dataset)\n",
    "\n",
    "\n",
    "class TitanicModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,    # 구축한 모델\n",
    "        learning_rate: float,      # 학습률\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model         # 모델 초기화\n",
    "        self.learning_rate = learning_rate  # 학습률 초기화\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.loss = F.cross_entropy(logit, y)  # 손실 계산\n",
    "\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        self.acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "        return self.loss  # 손실 반환\n",
    "    \n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        # 학습 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict(\n",
    "            {'acc/train_acc': self.acc, 'loss/train_loss': self.loss},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        self.val_loss = F.cross_entropy(logit, y)  # 검증 손실 계산\n",
    "\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "        self.val_acc = (predicted_label == y).float().mean()  # 정확도 계산\n",
    "\n",
    "        return self.val_loss  # 검증 손실 반환\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # 검증 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict(\n",
    "            {'acc/val_acc': self.val_acc, 'loss/val_loss': self.val_loss},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 테스트 단계에서 호출되는 메서드\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원을 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        logit = F.softmax(output, dim=-1)  # 소프트맥스 활성화 함수 적용\n",
    "        predicted_label = logit.argmax(dim=-1)  # 예측된 레이블 계산\n",
    "\n",
    "        return predicted_label  # 예측된 레이블 반환\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 옵티마이저를 설정하는 메서드\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),  # 모델 파라미터를 옵티마이저에 전달\n",
    "            lr=self.learning_rate,    # 학습률 설정\n",
    "        )\n",
    "\n",
    "        return {'optimizer': optimizer}  # 옵티마이저 반환\n",
    "\n",
    "class Model(nn.Module):  # nn.Module을 상속받아 새로운 모델 클래스를 정의합니다.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.input_dim = input_dim  # 입력 차원 크기를 저장합니다.\n",
    "        self.hidden_dim = hidden_dim  # 숨겨진 층의 차원 크기를 저장합니다.\n",
    "        self.output_dim = output_dim  # 출력 차원 크기를 저장합니다.\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)  # 입력 차원에서 숨겨진 차원으로의 선형 변환을 정의합니다.\n",
    "        self.relu = nn.ReLU()  # ReLU 활성화 함수를 정의합니다.\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)  # 숨겨진 차원에서 출력 차원으로의 선형 변환을 정의합니다.\n",
    "    \n",
    "    def forward(self, x):  # 순전파 메서드\n",
    "        x = self.linear(x)  # 입력 데이터에 대해 선형 변환을 적용합니다.\n",
    "        x = self.relu(x)  # ReLU 활성화 함수를 적용하여 비선형성을 추가합니다.\n",
    "        x = self.output(x)  # 두 번째 선형 변환을 적용하여 최종 출력을 계산합니다.\n",
    "\n",
    "        return x  # 최종 출력을 반환합니다.\n",
    "\n",
    "# len(titanic.columns) - 1: 입력 특성의 수 (타이타닉 데이터셋에서 'survived' 열을 제외한 나머지 열)\n",
    "# hidden_dim: 은닉층의 뉴런 수 (변수로 설정된 값)\n",
    "# 2: 출력 클래스의 수 (이진 분류 문제이므로 두 개 클래스)\n",
    "model = Model(len(titanic.columns) - 1, hidden_dim, 2)\n",
    "\n",
    "# TitanicModule 클래스의 인스턴스를 생성합니다.\n",
    "titanic_module = TitanicModule(\n",
    "    model=model,              # 학습할 모델 인스턴스 (예: Model 객체)\n",
    "    learning_rate=learning_rate  # 학습률 (예: 1e-3)\n",
    ")\n",
    "\n",
    "# Trainer 인스턴스 생성\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,  # 학습할 최대 에포크 수\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss/val_loss', mode='min', patience=3)\n",
    "    ],\n",
    "    logger=TensorBoardLogger(\n",
    "        'tensorboard',\n",
    "        f'titanic/seed={seed},batch_size={batch_size},learning_rate={learning_rate},hidden_dim={hidden_dim}',\n",
    "    )\n",
    ")\n",
    "\n",
    "# PyTorch Lightning Trainer를 사용하여 모델 학습을 시작\n",
    "# 'titanic_module'은 학습할 모델과 학습 및 검증 루프를 정의한 LightningModule 인스턴스\n",
    "# 'titanic_data_module'은 데이터셋을 로드하고 데이터로더를 제공하는 LightningDataModule 인스턴스\n",
    "trainer.fit(\n",
    "    model=titanic_module,       # 학습할 모델 인스턴스\n",
    "    datamodule=titanic_data_module,  # 데이터셋과 데이터로더를 제공하는 데이터 모듈 인스턴스\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Diamond price 예측 코드를 pytorch lightning으로 변환 (logger, earlystop with patience=5)\n",
    "# 2. learning rate=0.1, 0.04, 0.008에 따른 실험 결과 비교\n",
    "\n",
    "# 사용 가능한 GPU가 있는 경우 'cuda'를, 그렇지 않으면 'cpu'를 사용하도록 장치를 설정합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # NumPy의 시드를 설정합니다.\n",
    "torch.manual_seed(seed)  # PyTorch의 시드를 설정합니다.\n",
    "\n",
    "# 장치가 CUDA(GPU)일 경우, CUDA의 시드를 설정합니다.\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)  # 현재 장치의 CUDA 시드를 설정합니다.\n",
    "    torch.cuda.manual_seed_all(seed)  # 모든 CUDA 장치의 시드를 설정합니다.\n",
    "    torch.backends.cudnn.deterministic = False  # Deterministic 연산을 비활성화합니다. 비활성화하면 더 빠른 연산이 가능할 수 있습니다.\n",
    "    torch.backends.cudnn.benchmark = True  # 벤치마크를 활성화하여 최적의 성능을 위해 CUDA 커널을 자동으로 튜닝합니다.\n",
    "\n",
    "\n",
    "# 배치 크기 (Batch Size): 한 번의 업데이트에서 사용할 데이터 샘플 수\n",
    "batch_size = 1024\n",
    "\n",
    "# 에포크 수 (Epochs): 전체 데이터셋을 몇 번 반복하여 학습할 것인지 지정\n",
    "epochs = 1000\n",
    "\n",
    "# 학습률 (Learning Rate): 모델 파라미터를 업데이트할 때 사용되는 학습률\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 은닉층의 뉴런 수 (Hidden Dimension): 은닉층의 차원 또는 뉴런 수\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_22328\\3012579943.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 3 1 ... 4 3 2]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_22328\\3012579943.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 1 1 ... 0 4 0]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_22328\\3012579943.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[3 2 4 ... 2 3 3]' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, column] = label_encoder.fit_transform(df[column])\n"
     ]
    }
   ],
   "source": [
    "# 'diamonds' 데이터셋을 로드\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "# 데이터프레임에서 결측값이 있는 모든 행을 제거\n",
    "diamonds = diamonds.dropna()\n",
    "\n",
    "# 데이터프레임의 범주형 열을 정수형으로 변환\n",
    "# 'cut', 'color', 'clarity'\n",
    "diamonds, _ = convert_category_into_integer(diamonds, ('cut', 'color', 'clarity'))\n",
    "\n",
    "# 데이터프레임의 모든 열을 float32 데이터 타입으로 변환\n",
    "diamonds = diamonds.astype(np.float32)\n",
    "\n",
    "# 원본 데이터셋을 학습용 데이터와 임시 데이터로 분할\n",
    "# 전체 데이터의 40%를 임시 데이터로, 60%를 학습용 데이터로 사용\n",
    "train, temp = train_test_split(diamonds, test_size=0.4, random_state=seed)\n",
    "\n",
    "# 임시 데이터를 검증용 데이터와 테스트용 데이터로 분할\n",
    "# 임시 데이터의 절반을 검증용 데이터로, 나머지 절반을 테스트용 데이터로 사용\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# 훈련 세트의 'carat', 'depth', 'table', 'price', 'x', 'y', 'z' 열을 표준화합니다.\n",
    "# 표준화는 훈련 데이터의 평균과 표준편차를 사용하여 수행됩니다.\n",
    "train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.fit_transform(train.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 검증 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(valid.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )\n",
    "\n",
    "# 테스트 세트의 동일한 열을 훈련 세트에서 계산된 평균과 표준편차를 사용하여 표준화합니다.\n",
    "test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] = \\\n",
    "    standard_scaler.transform(test.loc[:, ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, data):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출하여 초기화합니다.\n",
    "        self.data = data  # 데이터프레임을 저장합니다.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 전체 샘플 수를 반환합니다.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 `idx`에 해당하는 데이터 샘플을 반환합니다.\n",
    "        \n",
    "        # 데이터프레임에서 'price' 열을 제외한 특성값을 가져와서 NumPy 배열로 변환한 뒤, PyTorch 텐서로 변환합니다.\n",
    "        X = torch.from_numpy(self.data.iloc[idx].drop('price').values).float()\n",
    "        \n",
    "        # 'price' 열의 값을 텐서로 변환하여 레이블을 생성합니다.\n",
    "        y = torch.Tensor([self.data.iloc[idx].price]).float()\n",
    "        \n",
    "        # 입력 데이터와 레이블을 딕셔너리 형태로 반환합니다.\n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 DiamondsDataset 클래스를 사용하여 데이터셋 객체로 변환\n",
    "train_dataset = DiamondsDataset(train)\n",
    "\n",
    "# 검증 데이터를 DiamondsDataset 클래스를 사용하여 데이터셋 객체로 변환\n",
    "valid_dataset = DiamondsDataset(valid)\n",
    "\n",
    "# 테스트 데이터를 DiamondsDataset 클래스를 사용하여 데이터셋 객체로 변환\n",
    "test_dataset = DiamondsDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DiamondsDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size  # 배치 크기 저장\n",
    "\n",
    "    def prepare(self, train_dataset, valid_dataset, test_dataset):\n",
    "        # 데이터셋을 저장하는 메서드\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # 데이터셋을 로드하여 데이터를 설정하는 메서드\n",
    "        if stage == \"fit\":\n",
    "            # 학습과 검증 단계에 사용할 데이터셋 설정\n",
    "            self.train_data = self.train_dataset\n",
    "            self.valid_data = self.valid_dataset\n",
    "\n",
    "        if stage == \"test\":\n",
    "            # 테스트 단계에 사용할 데이터셋 설정\n",
    "            self.test_data = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # 학습 데이터 로더 반환\n",
    "        return DataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size,  # 배치 크기 설정\n",
    "            shuffle=True,  # 데이터셋을 섞어서 로드\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # 검증 데이터 로더 반환\n",
    "        return DataLoader(\n",
    "            dataset=self.valid_data,\n",
    "            batch_size=self.batch_size,  # 배치 크기 설정\n",
    "            shuffle=False,  # 데이터셋을 섞지 않음\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # 테스트 데이터 로더 반환\n",
    "        return DataLoader(\n",
    "            dataset=self.test_data,\n",
    "            batch_size=self.batch_size,  # 배치 크기 설정\n",
    "            shuffle=False,  # 데이터셋을 섞지 않음\n",
    "        )\n",
    "\n",
    "\n",
    "# DiamondsDataModule 인스턴스 생성 및 데이터 준비\n",
    "diamonds_data_module = DiamondsDataModule(batch_size=batch_size)\n",
    "diamonds_data_module.prepare(train_dataset, valid_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 4.9 K  | train\n",
      "----------------------------------------\n",
      "4.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 K     Total params\n",
      "0.019     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s, v_num=4, loss/val_loss=0.0292, learning_rate=0.001, loss/train_loss=0.0164]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "class Model(nn.Module):  # nn.Module을 상속받아 새로운 모델 클래스를 정의\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):  # 생성자 메서드\n",
    "        super().__init__()  # 부모 클래스의 생성자 호출\n",
    "        self.input_dim = input_dim  # 입력 차원 크기 저장\n",
    "        self.hidden_dim = hidden_dim  # 숨겨진 층의 차원 크기 저장\n",
    "        self.output_dim = output_dim  # 출력 차원 크기 저장\n",
    "\n",
    "        # 입력 차원에서 숨겨진 차원으로의 선형 변환\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()  # ReLU 활성화 함수\n",
    "        # 숨겨진 차원에서 또 다른 숨겨진 차원으로의 선형 변환\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()  # ReLU 활성화 함수\n",
    "        # 숨겨진 차원에서 출력 차원으로의 선형 변환\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):  # 순전파 메서드\n",
    "        x = self.linear1(x)  # 입력 데이터에 대해 선형 변환 적용\n",
    "        x = self.relu1(x)  # ReLU 활성화 함수 적용\n",
    "        x = self.linear2(x)  # 또 다른 선형 변환 적용\n",
    "        x = self.relu2(x)  # ReLU 활성화 함수 적용\n",
    "        x = self.output(x)  # 최종 선형 변환 적용\n",
    "\n",
    "        return x  # 최종 출력 반환\n",
    "\n",
    "# Model 인스턴스 생성\n",
    "# len(diamonds.columns) - 1: 입력 특성의 수\n",
    "# hidden_dim: 은닉층의 뉴런 수\n",
    "# 1: 출력 차원 (회귀 문제의 경우)\n",
    "model = Model(len(diamonds.columns)-1, hidden_dim, 1)\n",
    "\n",
    "\n",
    "class DiamondsModule(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float):  # 초기화 메서드\n",
    "        super().__init__()\n",
    "        self.model = model  # 모델 초기화\n",
    "        self.learning_rate = learning_rate  # 학습률 초기화\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        self.loss = F.mse_loss(output, y)  # 손실 계산\n",
    "\n",
    "        return self.loss  # 손실 반환\n",
    "    \n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        # 학습 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict(\n",
    "            {'loss/train_loss': self.loss},  # 학습 손실 로깅\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch.get('X')  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y')  # 레이블 데이터를 장치로 이동\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "        self.val_loss = F.mse_loss(output, y)  # 검증 손실 계산\n",
    "\n",
    "        return self.val_loss  # 검증 손실 반환\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # 검증 에포크가 끝날 때 호출되는 메서드\n",
    "        self.log_dict(\n",
    "            {'loss/val_loss': self.val_loss,  # 검증 손실 로깅\n",
    "             'learning_rate': self.learning_rate},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 테스트 단계에서 호출되는 메서드\n",
    "        X = batch.get('X').to(device)  # 입력 데이터를 장치로 이동\n",
    "        y = batch.get('y').to(device)  # 레이블 데이터를 장치로 이동\n",
    "        y = y.squeeze()  # 레이블의 차원 축소\n",
    "\n",
    "        output = self.model(X)  # 모델의 예측값 계산\n",
    "\n",
    "        return output  # 예측된 레이블 반환\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 옵티마이저와 스케줄러 설정\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),  # 모델 파라미터를 옵티마이저에 전달\n",
    "            lr=self.learning_rate,    # 학습률 설정\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,  # 옵티마이저 반환\n",
    "            'scheduler': scheduler,  # 학습률 스케줄러 반환\n",
    "        }  \n",
    "\n",
    "# DiamondsModule 인스턴스 생성\n",
    "diamonds_module = DiamondsModule(\n",
    "    model=model,  # 학습할 모델 인스턴스\n",
    "    learning_rate=learning_rate  # 학습률\n",
    ")\n",
    "\n",
    "# Trainer 인스턴스 생성 및 설정\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,  # 학습할 최대 에포크 수\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss/val_loss', mode='min', patience=20)  # 조기 종료 콜백 설정\n",
    "    ],\n",
    "    logger=TensorBoardLogger(\n",
    "        'tensorboard',\n",
    "        f'diamonds/seed={seed},batch_size={batch_size},learning_rate={learning_rate},hidden_dim={hidden_dim}',  # 로그 디렉토리 설정\n",
    "    )\n",
    ")\n",
    "\n",
    "# 모델 학습 시작\n",
    "trainer.fit(\n",
    "    model=diamonds_module,  # 학습할 모델\n",
    "    datamodule=diamonds_data_module,  # 데이터 모듈\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
