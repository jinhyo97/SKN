{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth \"xformers==0.0.28.post2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import (\n",
    "    get_chat_template,\n",
    "    train_on_responses_only,\n",
    "    standardize_sharegpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth\n",
    "\n",
    "models: https://huggingface.co/unsloth\n",
    "\n",
    "LLM(대규모 언어 모델)의 훈련을 최적화하기 위한 도구. <br>\n",
    "훈련 속도를 높이고 메모리 사용량을 줄이며 정확성을 유지. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 특징 </font>\n",
    "\n",
    "1. 기존에 비해 LLM 훈련 속도를 약 30배 향상시킬 수 있음 <br>\n",
    "ex) Alpaca 모델의 훈련 시간이 85시간이 걸렸는데 비해, Unsloth의 경우 3시간으로 단축.\n",
    "2. 메모리 사용량 감소 <br>\n",
    "최적화된 메모리 관리 기술을 사용하여 훈련 중에 메모리 양을 줄일 수 있음 <br>\n",
    "-> 배치 사이즈를 키울 수 있어 더 빠른 훈련 모델을 훈련할 수 있게 함\n",
    "3. 정확성 유지\n",
    "Unsloth는 훈련 속도를 높이면서도 모델의 정확도 유지 <br>\n",
    "4. 다양한 하드웨어 지원 <br>\n",
    "NVIDIA, Intel 및 AMD GPU를 지원하여 다양한 하드웨어 환경에서 사용 가능\n",
    "5. LoRA 지원 <br>\n",
    "Unsloth를 사용하면 LoRA를 훨씬 쉽고 빠르게 이용 가능\n",
    "\n",
    "<br>\n",
    "\n",
    "<font style=\"font-size:20px\"> 적용 기술 </font>\n",
    "\n",
    "1. 커널 최적화 기술 <br>\n",
    "Unsloth는 Triton과 같은 커널 최적화 기술을 사용하여 모델의 핵심 연산 최적화. <br>\n",
    "\\- Triton 커널은 PyTorch 모델의 연산을 GPU에서 더 효율적으로 실현할 수 있도록 최적화된 커널로, 일반적인 PyTorch 연산보다 훨씬 빠른 속도 제공. <br>\n",
    "2. 메모리 최적화 <br>\n",
    "Unsloth는 메모리 사용을 최적화하여 GPU의 메모리 사용량을 줄임 <br>\n",
    "-> 처리할 수 있는 데이터 양을 늘려 더 많은 연산을 동시에 수행할 수 있게 함\n",
    "3. 경사하강법 최적화 <br>\n",
    "Unsloth는 경사하강법 최적화 과정을 최적화하여 모델의 학습 속도를 높임 <br>\n",
    "-> 모델의 가중치를 더 빠르게 업데이트하여 학습 속도를 높이는 데 도움이 됨\n",
    "4. 병렬화 및 배치 처리 <br>\n",
    "Unsloth는 병렬화 기술과 배치 처리를 통해 모델의 학습을 더 효율적으로 수행 <br>\n",
    "-> 더 큰 사이즈의 데이터를 처리할 수 있게 하고 배치 학습 효율을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 방법\n",
    "\n",
    "> ```python\n",
    "> # model & tokenizer load\n",
    "> model_name = 'unsloth/Llama-3.2-3B-Instruct'\n",
    "> model, tokenizer = FastLanguageModel.from_pretrained(\n",
    ">     model_name = model_name,\n",
    ">     load_in_4bit=True,\n",
    ">     dtype=None, # auto detection\n",
    ">     max_seq_length=2048,    # 아무 값이나 선택해도 무방\n",
    "> )\n",
    "> \n",
    "> model = FastLanguageModel.get_peft_model(\n",
    ">     model,\n",
    ">     r=8, # Suggested 8, 16, 32, 64, 128\n",
    ">     target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    ">                     'gate_proj', 'up_proj', 'down_proj',],\n",
    ">     lora_alpha=16,\n",
    ">     lora_dropout = 0,\n",
    ">     bias=\"none\",\n",
    ">     use_gradient_checkpointing=\"unsloth\",\n",
    ">     random_state=0,\n",
    "> )\n",
    "> \n",
    "> # load chat template tokenizer\n",
    "> tokenizer = get_chat_template(\n",
    ">     tokenizer,\n",
    ">     chat_template='llama-3.1',\n",
    "> )\n",
    "> \n",
    "> # preprocessing function\n",
    "> def formatting_prompts_func(examples):\n",
    ">     conversations = examples['conversations']\n",
    ">     texts = [\n",
    ">         tokenizer.apply_chat_template(\n",
    ">             conversation,\n",
    ">             tokenize=False,\n",
    ">             add_generation_prompt=False)\n",
    ">         for conversation\n",
    ">         in conversations\n",
    ">     ]\n",
    "> \n",
    ">     return {\n",
    ">         'text' : texts,\n",
    ">     }\n",
    "> \n",
    "> # data load and preprocessing\n",
    "> dataset = load_dataset(<dataset>)\n",
    "> dataset = standardize_sharegpt(dataset)\n",
    "> dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "> \n",
    "> # training_args\n",
    "> training_args = TrainingArguments(\n",
    ">         per_device_train_batch_size=2,\n",
    ">         gradient_accumulation_steps=4,\n",
    ">         warmup_steps=5,\n",
    ">         # num_train_epochs=1,\n",
    ">         max_steps=60,\n",
    ">         learning_rate=2e-4,\n",
    ">         fp16=not is_bfloat16_supported(),   # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
    ">         bf16=is_bfloat16_supported(),       # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
    ">         logging_steps=1,\n",
    ">         optim='adamw_8bit',\n",
    ">         weight_decay=0.01,\n",
    ">         lr_scheduler_type='linear',         # 스케줄러 유형\n",
    ">         seed=0,\n",
    ">         output_dir=<dir>,\n",
    ">         report_to=\"WandB\",\n",
    "> )\n",
    "> \n",
    "> # trainer\n",
    "> trainer = SFTTrainer(\n",
    ">     model=model,\n",
    ">     tokenizer=tokenizer,\n",
    ">     train_dataset=dataset,\n",
    ">     dataset_text_field='text',\n",
    ">     data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    ">     dataset_num_proc=16,\n",
    ">     packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
    ">     args=training_args\n",
    "> )\n",
    "> \n",
    "> # trainer setting\n",
    "> trainer = train_on_responses_only(\n",
    ">     trainer,\n",
    ">     instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    ">     response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "> )\n",
    "> \n",
    "> # train\n",
    "> trainer_stats = trainer.train()\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델 이름 설정\n",
    "model_name = 'unsloth/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# FastLanguageModel을 통해 모델과 토크나이저를 사전 학습된 모델에서 불러옴\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,          # 불러올 모델 이름\n",
    "    load_in_4bit=True,              # 4비트 양자화를 사용하여 메모리 효율성을 높임\n",
    "    dtype=None,                     # 데이터 타입을 지정하지 않아 기본 타입을 사용\n",
    "    max_seq_length=2048,            # 최대 시퀀스 길이를 2048로 설정\n",
    ")\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) 기반의 미세 조정 기법을 적용하여 모델 업데이트\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,                            # LoRA의 랭크(r) 값 설정\n",
    "    target_modules=[                # LoRA를 적용할 모델의 특정 모듈을 지정\n",
    "        'q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "        'gate_proj', 'up_proj', 'down_proj',\n",
    "    ],\n",
    "    lora_alpha=16,                  # LoRA의 알파(alpha) 값 설정\n",
    "    lora_dropout=0,                 # 드롭아웃 비율을 0으로 설정하여 드롭아웃 비활성화\n",
    "    bias='none',                    # 바이어스 추가 설정\n",
    "    use_gradient_checkpointing='unsloth', # 메모리 최적화를 위해 gradient checkpointing 사용\n",
    "    random_state=0,                 # 시드 값을 설정해 재현성 확보\n",
    ")\n",
    "\n",
    "# 토크나이저에 대해 사용자 정의 채팅 템플릿을 적용하여 대화 형식을 맞춤화\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template='llama-3.1',      # 채팅 템플릿으로 'llama-3.1' 형식 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 각 샘플을 전처리하는 함수 정의\n",
    "def preprocessing(sample):\n",
    "    # 샘플 내의 대화(conversations) 필드를 가져옴\n",
    "    conversations = sample.get('conversations')\n",
    "    \n",
    "    # 각 conversation을 템플릿에 맞춰 텍스트 형식으로 변환\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation,           # 대화 내용\n",
    "            tokenize=False,         # 토큰화 생략 (토큰화는 나중에 진행)\n",
    "            add_generation_prompt=False # 생성 프롬프트 추가 안 함\n",
    "        ) for conversation in conversations\n",
    "    ]\n",
    "\n",
    "    # 변환된 텍스트 리스트를 딕셔너리 형태로 반환\n",
    "    return {'text': texts}\n",
    "\n",
    "# FineTome-100k 데이터셋 로드, 학습(train) 데이터셋 분할 사용\n",
    "dataset = load_dataset('mlabonne/FineTome-100k', split='train')\n",
    "\n",
    "# 대화 형식을 표준화 (ShareGPT와 유사한 형식으로 변환)\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "# 데이터셋을 맵핑하여 각 샘플에 대해 전처리 수행\n",
    "dataset = dataset.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments 클래스를 사용해 학습에 필요한 다양한 하이퍼파라미터를 설정\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,          # 각 디바이스(GPU)에서의 배치 사이즈를 2로 설정\n",
    "    gradient_accumulation_steps=4,          # 그라디언트 누적 횟수를 4로 설정하여 더 큰 배치 효과 달성\n",
    "    warmup_steps=10,                        # 학습 초기에 워밍업 단계로 사용할 스텝 수\n",
    "    max_steps=100,                          # 전체 학습 스텝 수 제한\n",
    "    learning_rate=2e-4,                     # 학습률을 0.0002로 설정\n",
    "    fp16=not is_bfloat16_supported(),       # FP16(half precision) 사용 여부 설정; bfloat16을 지원하지 않을 때 사용\n",
    "    bf16=is_bfloat16_supported(),           # BF16(bfloat16)을 지원할 경우 사용 여부 설정\n",
    "    logging_steps=10,                       # 학습 로그 기록 간격을 10 스텝마다로 설정\n",
    "    seed=0,                                 # 시드 값 설정으로 재현성 확보\n",
    "    output_dir='./llama3.2_w_unsloth',      # 학습 결과와 체크포인트를 저장할 디렉터리\n",
    "    report_to='wandb',                      # 로그 및 메트릭을 `wandb`에 보고\n",
    ")\n",
    "\n",
    "# SFTTrainer 인스턴스를 생성하여 모델 학습 준비\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                            # 학습할 모델\n",
    "    tokenizer=tokenizer,                    # 모델의 토크나이저\n",
    "    train_dataset=dataset,                  # 학습에 사용할 데이터셋\n",
    "    dataset_text_field='text',              # 데이터셋의 텍스트 필드명\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer), # 데이터 수집 및 전처리 설정\n",
    "    dataset_num_proc=16,                    # 데이터셋 처리에 사용할 프로세스 수\n",
    "    packing=False,                          # 패킹 설정 (여기서는 패킹 사용 안 함)\n",
    "    args=training_args,                     # 학습에 사용할 TrainingArguments 설정\n",
    ")\n",
    "\n",
    "# 학습 데이터를 사용자 질문과 응답 데이터만으로 제한하는 함수 호출\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part='<|start_header_id|>user<|end_header_id|>\\n\\n',   # 사용자 입력의 시작 및 종료 부분\n",
    "    response_part='<|start_header_id|>assistant<|end_header_id|>\\n\\n', # 어시스턴트 응답의 시작 및 종료 부분\n",
    ")\n",
    "\n",
    "# 모델 학습 시작\n",
    "results = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
