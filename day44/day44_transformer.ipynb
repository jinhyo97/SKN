{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "reference: [Transformer](https://wikidocs.net/31379) <br>\n",
    "paper: [Transformer](https://arxiv.org/pdf/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = self.positional_encoding(self.seq_len, self.d_model)\n",
    "    \n",
    "    def get_angles(self, seq_len, i, d_model):\n",
    "        angles = 1 / (10000**(2*(i//2)/d_model))\n",
    "\n",
    "        return seq_len * angles\n",
    "    \n",
    "    def positional_encoding(self, seq_len, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            seq_len=torch.arange(seq_len, dtype=torch.float32).unsqueeze(1),\n",
    "            i=torch.arange(d_model, dtype=torch.float32).unsqueeze(0),\n",
    "            d_model=d_model,\n",
    "        )\n",
    "\n",
    "        sines = torch.sin(angle_rads[:, ::2])\n",
    "        cosines = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        embedding = torch.zeros(angle_rads.shape)\n",
    "        embedding[:, ::2] = sines\n",
    "        embedding[:, 1::2] = cosines\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: batch, seq_len, dim\n",
    "        # embedding: seq_len, dim\n",
    "        return x + self.embedding[:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4819e+00,  2.9181e+00, -4.2600e-01,  ...,  5.4064e-01,\n",
       "          -1.0419e+00, -1.3650e+00],\n",
       "         [ 1.5785e+00,  8.8602e-01, -5.9404e-01,  ...,  1.7294e+00,\n",
       "           1.7922e+00,  1.8669e+00],\n",
       "         [ 1.1808e+00, -8.4693e-01,  1.1339e+00,  ...,  1.2836e+00,\n",
       "          -1.3964e+00,  9.3487e-01],\n",
       "         ...,\n",
       "         [ 8.5370e-02,  1.0630e+00, -1.6198e-01,  ...,  2.3108e+00,\n",
       "           1.0211e+00,  2.5693e+00],\n",
       "         [ 6.9371e-01,  1.6353e+00,  4.7266e-01,  ...,  2.5780e+00,\n",
       "           1.5265e+00, -3.4820e-01],\n",
       "         [ 9.0398e-01,  2.8847e-01, -9.7043e-01,  ...,  1.3065e+00,\n",
       "           2.4085e-01,  7.7492e-01]],\n",
       "\n",
       "        [[-9.2326e-01,  1.4857e+00, -2.5263e+00,  ...,  8.4135e-01,\n",
       "          -2.2634e-02,  1.6392e+00],\n",
       "         [ 1.1847e+00, -4.9983e-01,  2.1163e+00,  ..., -6.9308e-01,\n",
       "          -2.7996e-01,  8.5579e-01],\n",
       "         [-3.5988e-01, -1.3606e-01,  8.6927e-01,  ...,  2.3313e-01,\n",
       "           1.0369e+00, -7.9081e-02],\n",
       "         ...,\n",
       "         [-2.5520e+00, -4.7474e-01, -1.0319e-01,  ...,  1.7157e+00,\n",
       "           5.5338e-01,  1.0630e+00],\n",
       "         [-6.9341e-01, -1.7638e-01, -1.2799e+00,  ...,  3.0777e+00,\n",
       "          -8.4748e-01,  2.0537e+00],\n",
       "         [-3.1400e-01,  4.4129e-01, -3.9120e-01,  ...,  6.2612e-01,\n",
       "          -1.9344e-01,  1.5507e+00]],\n",
       "\n",
       "        [[-5.7880e-02,  2.3050e-01, -2.2067e-02,  ...,  1.0078e+00,\n",
       "           5.7986e-01,  1.0788e+00],\n",
       "         [ 4.1221e-01,  2.1162e+00,  1.0924e+00,  ...,  1.9972e+00,\n",
       "          -2.4367e-01,  2.8289e-01],\n",
       "         [ 1.0234e+00, -1.0288e+00,  2.2763e+00,  ...,  6.6146e-01,\n",
       "           2.8624e-02,  1.6401e+00],\n",
       "         ...,\n",
       "         [-1.4033e+00, -1.2588e+00,  6.9288e-01,  ...,  1.3343e+00,\n",
       "           5.6640e-01,  2.6745e+00],\n",
       "         [ 9.3004e-01,  1.8689e+00,  2.5934e-01,  ...,  7.6781e-01,\n",
       "           5.5578e-01, -3.7659e-01],\n",
       "         [ 1.5148e+00, -6.6811e-01, -1.3895e-01,  ...,  2.7772e+00,\n",
       "           1.5029e+00,  2.0893e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.8378e+00,  9.5987e-01, -5.9941e-01,  ...,  4.8454e-01,\n",
       "          -4.9431e-01,  1.1131e+00],\n",
       "         [-1.9672e-01,  4.8127e-01,  1.3801e+00,  ...,  1.9347e+00,\n",
       "           1.9327e+00,  2.3514e+00],\n",
       "         [ 1.4796e-01, -2.2269e-01, -5.5842e-01,  ...,  1.1108e+00,\n",
       "           1.3064e+00,  2.0371e+00],\n",
       "         ...,\n",
       "         [-1.0654e+00, -2.0400e+00, -1.8340e+00,  ...,  1.6567e+00,\n",
       "           2.1464e-03,  3.1316e+00],\n",
       "         [ 5.2702e-01,  9.4579e-01, -7.8247e-01,  ...,  1.3095e+00,\n",
       "          -1.3717e+00,  2.8210e+00],\n",
       "         [ 3.6680e-01,  1.1778e+00,  3.2584e-02,  ...,  1.8268e+00,\n",
       "          -1.2025e+00,  1.1665e+00]],\n",
       "\n",
       "        [[-1.0641e+00,  1.1674e+00, -7.2554e-02,  ...,  2.4441e-02,\n",
       "          -5.9000e-01,  1.1038e+00],\n",
       "         [ 3.5433e-01,  8.7182e-01,  1.4737e+00,  ...,  7.4763e-01,\n",
       "          -1.5996e-01,  1.5668e+00],\n",
       "         [ 2.0028e+00, -1.4453e+00,  4.0012e-01,  ...,  4.3697e-01,\n",
       "           2.7916e-01,  9.7356e-01],\n",
       "         ...,\n",
       "         [-1.9234e+00, -1.4635e-01,  1.9903e-01,  ...,  1.2872e+00,\n",
       "           3.5790e-01,  6.9013e-01],\n",
       "         [ 5.4389e-02,  6.7729e-01,  1.0002e+00,  ...,  2.1432e+00,\n",
       "           9.1633e-01,  2.3434e+00],\n",
       "         [ 1.2762e+00,  8.0125e-01,  3.7970e-01,  ...,  1.1355e+00,\n",
       "           3.5450e-01,  2.1476e+00]],\n",
       "\n",
       "        [[ 1.2020e+00,  2.6062e+00,  7.6958e-01,  ...,  9.0767e-01,\n",
       "           8.6273e-01,  2.3171e+00],\n",
       "         [ 5.9289e-01,  1.2259e+00,  2.9228e-01,  ...,  3.5943e-01,\n",
       "          -8.0149e-03, -4.6392e-01],\n",
       "         [ 1.0985e+00, -1.8459e+00,  9.9706e-01,  ...,  1.9778e+00,\n",
       "          -4.7135e-01,  2.4769e+00],\n",
       "         ...,\n",
       "         [-2.6869e+00, -9.2997e-01, -1.7064e+00,  ..., -4.5010e-01,\n",
       "           4.1264e-01,  8.9276e-01],\n",
       "         [ 1.2913e+00, -2.6162e-01, -1.4714e+00,  ...,  5.6243e-01,\n",
       "          -1.0007e+00,  3.0193e+00],\n",
       "         [ 6.4825e-03,  1.0757e+00,  3.8524e-01,  ..., -3.1271e-01,\n",
       "          -7.3975e-01,  8.5471e-01]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((32, 20, 512))\n",
    "positional_encodng = PositionalEncoding(20, 512)\n",
    "positional_encodng(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    matmul_qk = query @ key.transpose(-2, -1)\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    if mask is not None:\n",
    "        logits += (mask * 1e-9)\n",
    "\n",
    "    attention_weights = F.softmax(logits, dim=1)\n",
    "    output = attention_weights @ value\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.9979, -0.1066, -0.4298,  ...,  0.7281,  1.2474, -0.1675],\n",
       "          [ 0.7960,  0.2741,  0.5972,  ...,  1.4414, -1.4739,  0.8064],\n",
       "          [ 0.6012, -0.6820, -0.5358,  ..., -0.0535,  2.1505,  1.3132],\n",
       "          ...,\n",
       "          [-0.1742,  0.0524,  0.7115,  ..., -0.5074, -0.6272, -1.1105],\n",
       "          [-0.1750, -0.8036, -0.9733,  ...,  0.0575,  1.4971, -1.5334],\n",
       "          [ 1.3329,  1.6873,  1.3500,  ..., -0.2212,  1.1462, -0.9416]],\n",
       " \n",
       "         [[ 0.8099,  1.6177, -0.3885,  ..., -1.9721, -1.6653, -0.9206],\n",
       "          [ 0.9991, -2.8307, -1.2456,  ...,  1.4407,  1.0783,  1.5209],\n",
       "          [ 2.3108, -1.5950,  0.8184,  ..., -0.4678, -0.5187, -0.7061],\n",
       "          ...,\n",
       "          [-1.1083,  0.8813,  2.1368,  ..., -0.8860,  2.0806,  0.4568],\n",
       "          [-0.2779,  1.9584,  0.3380,  ..., -0.2399,  1.5800, -0.8274],\n",
       "          [-1.1893, -1.6869,  1.2096,  ..., -0.3806,  2.1417,  0.8775]],\n",
       " \n",
       "         [[ 0.8766,  0.7907, -0.9868,  ...,  0.3646, -1.8486, -0.0483],\n",
       "          [-0.7748,  0.7050, -0.0394,  ...,  0.0383, -1.2587, -0.6358],\n",
       "          [-0.4933,  0.3209, -0.3740,  ..., -0.5820, -1.0814,  0.9774],\n",
       "          ...,\n",
       "          [-0.7635, -0.7118,  0.6018,  ...,  0.3177,  0.8670,  1.9579],\n",
       "          [-1.0535, -1.2810, -0.7473,  ..., -1.0834, -1.2478,  1.2759],\n",
       "          [-0.0941, -0.4458, -1.8961,  ..., -1.2870, -1.3181, -0.0626]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.6569,  1.6803, -1.0857,  ..., -0.5325, -0.8705,  1.4296],\n",
       "          [ 0.0905, -2.1504,  1.0002,  ..., -1.6823,  0.5616, -0.2373],\n",
       "          [-0.0766, -1.0932, -1.1704,  ..., -0.6950,  0.2811,  0.7946],\n",
       "          ...,\n",
       "          [ 0.3624, -1.6697,  0.5328,  ...,  0.7999,  1.0547,  0.7213],\n",
       "          [ 0.6073, -0.2983,  0.2936,  ...,  0.0386,  1.3432,  1.1037],\n",
       "          [ 1.4418,  0.7391, -0.9424,  ..., -0.0765,  0.8926, -0.5082]],\n",
       " \n",
       "         [[ 0.0864,  0.6160, -0.4215,  ...,  0.3176, -0.4808,  0.3301],\n",
       "          [-2.2525, -0.9052, -1.4225,  ...,  1.5686,  0.7071, -0.0108],\n",
       "          [-0.9598,  0.3450,  0.2425,  ...,  0.5978, -0.0486,  0.3843],\n",
       "          ...,\n",
       "          [-1.5129, -0.4398, -0.3583,  ...,  0.1431,  1.3430, -0.6077],\n",
       "          [ 1.0503,  0.4324, -0.2300,  ..., -0.2117,  0.4804,  2.3195],\n",
       "          [-0.4154,  1.2665,  0.2098,  ..., -0.0550, -2.0445, -2.0017]],\n",
       " \n",
       "         [[-0.7998, -0.6808,  1.3480,  ...,  1.1793, -0.2612,  1.3474],\n",
       "          [ 0.7815,  0.4958,  1.1419,  ...,  0.3411, -2.0715, -1.0470],\n",
       "          [ 2.2975, -0.0324, -1.4275,  ..., -0.6741, -0.0191, -0.1645],\n",
       "          ...,\n",
       "          [-1.4165, -0.9761, -1.3822,  ...,  0.3393,  0.5301, -0.1020],\n",
       "          [-0.6694,  1.5762, -1.2322,  ...,  2.2083,  0.0420,  1.3347],\n",
       "          [-0.2859, -0.1777, -0.9720,  ..., -0.0301,  0.1680,  1.2832]]]),\n",
       " tensor([[[1.0000e+00, 5.3240e-10, 7.6073e-11,  ..., 1.6776e-10,\n",
       "           3.0284e-12, 1.7338e-09],\n",
       "          [4.1291e-09, 1.0000e+00, 5.7784e-12,  ..., 5.0835e-10,\n",
       "           2.7944e-11, 7.5001e-10],\n",
       "          [1.5321e-09, 1.5006e-11, 1.0000e+00,  ..., 3.5063e-10,\n",
       "           3.6698e-12, 9.1115e-10],\n",
       "          ...,\n",
       "          [5.8512e-10, 2.2862e-10, 6.0723e-11,  ..., 1.0000e+00,\n",
       "           8.9032e-13, 4.4148e-10],\n",
       "          [2.9128e-10, 3.4656e-10, 1.7526e-11,  ..., 2.4552e-11,\n",
       "           1.0000e+00, 1.1928e-09],\n",
       "          [2.9439e-09, 1.6420e-10, 7.6817e-11,  ..., 2.1492e-10,\n",
       "           2.1057e-11, 1.0000e+00]],\n",
       " \n",
       "         [[1.0000e+00, 5.1757e-11, 1.3975e-10,  ..., 1.3814e-10,\n",
       "           1.0547e-10, 4.9183e-12],\n",
       "          [1.2153e-09, 1.0000e+00, 2.5402e-10,  ..., 8.8672e-11,\n",
       "           7.9245e-11, 8.0189e-12],\n",
       "          [1.6408e-10, 1.2701e-11, 1.0000e+00,  ..., 1.0437e-10,\n",
       "           5.9331e-11, 1.6484e-11],\n",
       "          ...,\n",
       "          [6.7048e-10, 1.8328e-11, 4.3143e-10,  ..., 1.0000e+00,\n",
       "           7.8884e-10, 5.8856e-12],\n",
       "          [6.3564e-10, 2.0339e-11, 3.0455e-10,  ..., 9.7953e-10,\n",
       "           1.0000e+00, 9.6033e-12],\n",
       "          [1.2956e-09, 8.9955e-11, 3.6981e-09,  ..., 3.1942e-10,\n",
       "           4.1973e-10, 1.0000e+00]],\n",
       " \n",
       "         [[1.0000e+00, 8.7354e-10, 3.6336e-10,  ..., 5.8893e-11,\n",
       "           1.4697e-11, 4.0437e-10],\n",
       "          [3.4293e-09, 1.0000e+00, 6.8882e-11,  ..., 1.6977e-10,\n",
       "           1.1777e-11, 6.0420e-11],\n",
       "          [2.6322e-09, 1.2710e-10, 1.0000e+00,  ..., 3.1869e-11,\n",
       "           4.3478e-12, 9.0723e-11],\n",
       "          ...,\n",
       "          [3.5536e-10, 2.6095e-10, 2.6546e-11,  ..., 1.0000e+00,\n",
       "           1.4018e-11, 3.7359e-11],\n",
       "          [3.5346e-10, 7.2146e-11, 1.4434e-11,  ..., 5.5872e-11,\n",
       "           1.0000e+00, 4.8408e-10],\n",
       "          [1.5531e-09, 5.9111e-11, 4.8102e-11,  ..., 2.3780e-11,\n",
       "           7.7309e-11, 1.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.0000e+00, 9.3848e-11, 1.0271e-09,  ..., 1.0194e-10,\n",
       "           3.3092e-11, 2.0651e-11],\n",
       "          [4.0948e-11, 1.0000e+00, 2.7574e-09,  ..., 8.8711e-11,\n",
       "           1.4822e-10, 3.8501e-10],\n",
       "          [7.5234e-11, 4.6292e-10, 1.0000e+00,  ..., 5.1220e-11,\n",
       "           1.2644e-10, 1.2110e-10],\n",
       "          ...,\n",
       "          [8.7426e-11, 1.7437e-10, 5.9969e-10,  ..., 1.0000e+00,\n",
       "           7.9775e-11, 8.3081e-11],\n",
       "          [5.0774e-11, 5.2120e-10, 2.6483e-09,  ..., 1.4272e-10,\n",
       "           1.0000e+00, 2.9358e-11],\n",
       "          [2.0524e-11, 8.7698e-10, 1.6431e-09,  ..., 9.6278e-11,\n",
       "           1.9017e-11, 1.0000e+00]],\n",
       " \n",
       "         [[1.0000e+00, 1.2550e-09, 1.1810e-10,  ..., 2.9347e-10,\n",
       "           1.7326e-10, 2.7017e-10],\n",
       "          [2.0953e-08, 1.0000e+00, 6.4522e-11,  ..., 2.4255e-09,\n",
       "           4.2091e-10, 2.6499e-10],\n",
       "          [5.5556e-09, 1.8180e-10, 1.0000e+00,  ..., 8.9762e-10,\n",
       "           2.7304e-10, 4.8560e-11],\n",
       "          ...,\n",
       "          [1.6885e-09, 8.3587e-10, 1.0979e-10,  ..., 1.0000e+00,\n",
       "           1.9625e-10, 7.7276e-11],\n",
       "          [4.5442e-09, 6.6120e-10, 1.5223e-10,  ..., 8.9459e-10,\n",
       "           1.0000e+00, 1.9403e-10],\n",
       "          [6.3693e-09, 3.7419e-10, 2.4336e-11,  ..., 3.1663e-10,\n",
       "           1.7441e-10, 1.0000e+00]],\n",
       " \n",
       "         [[1.0000e+00, 9.9883e-10, 1.8988e-10,  ..., 1.7679e-09,\n",
       "           1.6029e-10, 1.0126e-10],\n",
       "          [1.1529e-10, 1.0000e+00, 5.6086e-10,  ..., 3.2064e-09,\n",
       "           1.7442e-10, 2.0899e-10],\n",
       "          [3.7268e-10, 9.5365e-09, 1.0000e+00,  ..., 1.0525e-09,\n",
       "           4.5114e-10, 1.2052e-10],\n",
       "          ...,\n",
       "          [3.4186e-10, 5.3714e-09, 1.0370e-10,  ..., 1.0000e+00,\n",
       "           2.0534e-10, 1.0610e-10],\n",
       "          [4.4560e-10, 4.2005e-09, 6.3898e-10,  ..., 2.9519e-09,\n",
       "           1.0000e+00, 1.2128e-10],\n",
       "          [4.1172e-10, 7.3614e-09, 2.4965e-10,  ..., 2.2307e-09,\n",
       "           1.7738e-10, 1.0000e+00]]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn((32, 20, 512))\n",
    "scaled_dot_product_attention(query, query, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert self.d_model%self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = nn.Linear(self.d_model, self.d_model)\n",
    "        self.key_dense = nn.Linear(self.d_model, self.d_model)\n",
    "        self.value_dense = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.dense = nn.Linear(self.d_model, self.d_model)\n",
    "    \n",
    "    def forward(self, inputs: dict):\n",
    "        query, key, value = inputs.get('query'), inputs.get('key'), inputs.get('value')\n",
    "        mask = inputs.get('mask')\n",
    "        batch_size, seq_len = query.shape[:2]\n",
    "\n",
    "        query = self.query_dense(query) # batch_size, seq_len, dim\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = query.reshape(batch_size, seq_len, self.num_heads, self.depth)  # batch, seq_len, num_heads, depth\n",
    "        key = key.reshape(batch_size, seq_len, self.num_heads, self.depth)\n",
    "        value = value.reshape(batch_size, seq_len, self.num_heads, self.depth)\n",
    "\n",
    "        query = query.permute(0, 2, 1, 3)   # batch, num_heads, seq_len, depth\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)   # batch, num_heads, seq_len, depth\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)                 # batch, seq_len, num_heads, depth\n",
    "        concat_attention = scaled_attention.reshape(batch_size, seq_len, self.d_model)  # batch, seq_len, dim\n",
    "\n",
    "        outputs = self.dense(concat_attention)  # batch, seq_len, dim\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, num_heads: int, dropout_ratio: float):    \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.multi_head_attention = MultiheadAttention(self.d_model, self.num_heads)\n",
    "        self.dropout1 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_ff, self.d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout2 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        inputs = {'query': x, 'key': x, 'value': x, 'mask': mask}\n",
    "        x_multi_head_output = self.multi_head_attention(inputs)\n",
    "        x_multi_head_output = self.dropout1(x_multi_head_output)\n",
    "        x = self.layer_norm1(x_multi_head_output + x)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        output = self.layer_norm2(x + ffn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9848,  0.6702,  1.3187,  ..., -1.8875,  0.7444, -0.6271],\n",
       "         [-0.2133, -0.5521, -0.4357,  ...,  1.3083, -0.9965,  0.5198],\n",
       "         [ 0.9016,  0.3068, -0.1425,  ..., -0.2294,  0.3499,  0.8375],\n",
       "         ...,\n",
       "         [ 1.6197,  0.8537,  0.2259,  ...,  0.1337, -0.7509,  0.5322],\n",
       "         [ 1.5149,  1.3291,  1.7559,  ...,  1.4643, -1.3589,  2.1033],\n",
       "         [-0.7850, -0.8179, -0.9058,  ...,  0.9911,  0.9380,  1.4185]],\n",
       "\n",
       "        [[-1.8846, -0.8105,  1.5370,  ...,  0.6021, -0.8217, -2.6228],\n",
       "         [-0.9016,  0.0991,  0.5283,  ..., -0.1668, -1.8993, -0.6681],\n",
       "         [-1.6052, -0.0443,  0.9234,  ...,  1.5764, -1.0744,  1.1181],\n",
       "         ...,\n",
       "         [-0.5053,  0.0063, -2.5317,  ..., -0.6285, -2.0232, -1.3003],\n",
       "         [-0.4995, -1.2293, -0.7102,  ...,  0.9446,  0.7619,  0.2118],\n",
       "         [-1.8678,  0.9061, -0.1156,  ..., -0.2425, -1.2721, -0.6108]],\n",
       "\n",
       "        [[-1.7102,  0.6463, -2.1659,  ...,  0.0577, -0.5304,  0.7939],\n",
       "         [ 0.6090,  0.9775,  0.4625,  ..., -0.0581, -0.7148, -0.5538],\n",
       "         [-0.0509,  1.8914,  1.7055,  ..., -0.6910, -1.3455, -0.0598],\n",
       "         ...,\n",
       "         [-0.6611,  0.3214,  0.5990,  ...,  0.2656, -0.2871,  0.9087],\n",
       "         [-1.3295, -0.0674,  0.3251,  ..., -1.4897, -1.3426, -0.4196],\n",
       "         [-0.7096, -0.1402, -0.0335,  ..., -0.7530, -0.5085, -0.9859]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0058,  0.3058, -0.3915,  ...,  0.9727, -0.2954,  0.7557],\n",
       "         [ 0.2129,  1.7485, -0.4753,  ..., -1.2293, -0.7110,  1.1712],\n",
       "         [-1.1266, -0.3136,  0.0476,  ...,  0.0978, -0.0227, -0.2100],\n",
       "         ...,\n",
       "         [-0.7100, -0.2740,  2.1676,  ..., -0.9134, -0.1783, -0.3901],\n",
       "         [ 0.4713,  1.4054, -0.4010,  ..., -1.6306, -0.1755,  1.2239],\n",
       "         [ 1.6732,  0.4676,  0.9748,  ...,  0.8153,  0.1481, -0.4917]],\n",
       "\n",
       "        [[ 0.9479,  0.1369, -1.0959,  ..., -0.3715,  0.8246, -0.4960],\n",
       "         [ 1.2801, -0.7442,  0.2450,  ..., -0.7649,  0.1529,  0.6120],\n",
       "         [ 0.8004, -1.9754, -1.8373,  ..., -2.6655, -0.4615, -0.9779],\n",
       "         ...,\n",
       "         [ 1.0157,  1.0717, -0.4278,  ..., -0.3632,  0.6171,  1.2593],\n",
       "         [-0.4498,  0.7585, -1.0095,  ..., -1.0960,  1.3383,  0.6219],\n",
       "         [ 1.2399, -0.1365, -1.7152,  ...,  0.4301,  0.8347,  2.3011]],\n",
       "\n",
       "        [[ 0.3533, -0.5065,  1.1751,  ...,  1.7766,  0.5613,  0.1705],\n",
       "         [ 0.0590, -0.5238, -0.4018,  ..., -0.5702, -0.0448,  1.5557],\n",
       "         [ 0.5836,  1.5325,  1.4125,  ..., -0.7688,  2.0056, -1.0054],\n",
       "         ...,\n",
       "         [-0.4297,  0.5556, -1.3929,  ...,  0.6479, -1.4832, -0.0331],\n",
       "         [-0.3044,  0.2610,  0.8149,  ..., -0.7881,  0.3970, -0.6374],\n",
       "         [-0.5805, -1.3502,  0.2598,  ..., -1.2426,  1.9832, -1.1392]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 20, 512)\n",
    "\n",
    "encoder_layer = EncoderLayer(512, 2048, 8, 0.1)\n",
    "encoder_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_heads: int,\n",
    "        dropout_ratio: float,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(self.seq_len, self.d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(self.d_model, self.d_ff, self.num_heads, self.dropout_ratio)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x *= (self.d_model ** 0.5)\n",
    "        x += self.positional_encoding(x)\n",
    "\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        output = x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8475,  0.0231, -0.0385,  ..., -0.3078, -0.0699, -0.7248],\n",
       "         [ 1.9553, -0.5205, -0.0569,  ..., -0.3857, -0.1446, -1.7681],\n",
       "         [ 0.5508,  0.8090, -0.1973,  ...,  0.0668, -0.9554, -1.3088],\n",
       "         ...,\n",
       "         [ 1.3308, -1.1035, -0.9705,  ...,  0.1107, -0.8299, -2.2690],\n",
       "         [ 1.6041,  0.3398,  0.5741,  ...,  0.4815,  0.1334, -2.1437],\n",
       "         [-0.7404,  0.8648,  0.5233,  ...,  0.1159, -0.4453, -1.6110]],\n",
       "\n",
       "        [[ 0.3412, -1.1536,  0.4852,  ..., -0.0238, -1.6976, -1.2767],\n",
       "         [ 1.4794, -0.4740,  1.0270,  ..., -1.1288, -1.7871,  0.1446],\n",
       "         [ 0.1760,  0.5634,  1.4709,  ..., -0.7118, -1.2055,  0.0260],\n",
       "         ...,\n",
       "         [ 0.0421, -0.2462,  0.5400,  ..., -0.6138, -0.7388,  0.9914],\n",
       "         [-0.6090,  0.6252, -0.3613,  ..., -0.8281, -1.5281,  0.1960],\n",
       "         [-0.7196, -0.4042,  1.1014,  ..., -0.4013, -1.5679, -0.3405]],\n",
       "\n",
       "        [[ 0.5150, -0.8768, -0.0494,  ...,  0.0285, -0.1379, -1.7948],\n",
       "         [ 0.3135, -0.1646,  0.2634,  ..., -0.7625, -0.6218, -2.1581],\n",
       "         [ 2.4456, -0.0157,  0.0486,  ..., -0.8581, -0.8691, -1.8759],\n",
       "         ...,\n",
       "         [ 1.6167,  0.3147,  0.5729,  ...,  0.6068, -1.4931, -2.2378],\n",
       "         [ 1.5492, -0.2345,  1.1016,  ...,  0.6955, -0.6230, -1.7795],\n",
       "         [ 0.9719, -0.0778, -0.3247,  ..., -0.6433, -0.7870, -1.8660]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.6337,  1.9877,  1.0563,  ..., -0.6736, -0.1473,  0.4536],\n",
       "         [ 1.2382,  1.8789,  0.6845,  ..., -0.8411, -0.4594, -0.8491],\n",
       "         [ 0.0472,  0.2638,  0.9569,  ...,  0.0077, -0.4289, -1.0151],\n",
       "         ...,\n",
       "         [ 1.3027, -0.0771,  2.1791,  ..., -1.0419, -1.2206, -0.3054],\n",
       "         [ 0.9826,  1.1683,  0.9609,  ..., -0.5233, -0.1518, -0.6337],\n",
       "         [ 0.3081,  0.1732,  1.3231,  ..., -0.7843, -1.2095, -0.3804]],\n",
       "\n",
       "        [[ 1.0151,  1.0238, -0.1777,  ..., -1.4867, -1.4365,  1.4941],\n",
       "         [ 1.3641,  0.2195, -0.0402,  ..., -0.3658, -1.9084,  0.9786],\n",
       "         [ 1.5630, -0.3771, -0.6452,  ..., -0.5268, -0.2530,  0.6659],\n",
       "         ...,\n",
       "         [ 2.3092,  1.5213, -0.6815,  ..., -1.8523, -0.2852,  1.8405],\n",
       "         [-0.6273, -0.3585, -0.9668,  ..., -0.3737,  0.7725,  0.8464],\n",
       "         [ 1.2548,  0.7046, -0.8281,  ..., -1.7432, -1.1223, -0.0117]],\n",
       "\n",
       "        [[ 1.6214, -0.1029, -0.0091,  ..., -1.4763, -1.2981, -2.0211],\n",
       "         [-0.0614,  0.5983, -0.3974,  ..., -0.6596, -0.5131,  0.8388],\n",
       "         [ 0.8093, -0.2574, -0.3177,  ..., -0.2138, -0.4050, -1.7055],\n",
       "         ...,\n",
       "         [ 0.0631, -0.3741, -0.3465,  ..., -1.5407,  0.9325,  0.5096],\n",
       "         [ 1.4023,  0.4957,  0.3889,  ..., -1.3097,  0.5829, -1.0210],\n",
       "         [ 0.7304, -0.8869, -0.8741,  ..., -0.4347,  0.0084, -1.0899]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 1000, (32, 20))\n",
    "\n",
    "encoder = Encoder(20, 1000, 6, 512, 2048, 8, 0.1)\n",
    "encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.multi_head_attention1 = MultiheadAttention(self.d_model, self.num_heads)\n",
    "        self.dropout1 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.multi_head_attention2 = MultiheadAttention(self.d_model, self.num_heads)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_ff, self.d_model),\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(self.dropout_ratio)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out, mask=None):\n",
    "        inputs = {'query': x, 'key': x, 'value': x, 'mask': mask}\n",
    "        x_multi_head_output = self.multi_head_attention1(inputs)\n",
    "        x_multi_head_output = self.dropout1(x_multi_head_output)\n",
    "        x = self.layer_norm1(x + x_multi_head_output)\n",
    "\n",
    "        inputs = {'query': x, 'key': x, 'value': x, 'mask': mask}\n",
    "        x_multi_head_output = self.multi_head_attention2(inputs)\n",
    "        x_multi_head_output = self.dropout2(x_multi_head_output)\n",
    "        x = self.layer_norm2(x + x_multi_head_output)\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        output = self.layer_norm3(x + ffn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6483, -0.6845, -0.4586,  ...,  0.0303, -0.1651,  1.7737],\n",
       "         [ 0.7005, -0.3682, -0.5978,  ..., -0.6435, -0.8065,  0.1445],\n",
       "         [-0.2216,  0.1596,  2.0457,  ..., -0.1270,  1.4141,  0.0807],\n",
       "         ...,\n",
       "         [-0.2865,  1.1048, -0.3875,  ..., -0.5319, -0.7416,  0.5476],\n",
       "         [ 1.0356,  0.6441,  0.9505,  ...,  0.9825,  0.9149, -0.3902],\n",
       "         [-1.2154, -0.7707,  0.4412,  ...,  0.2220,  1.2852, -2.4506]],\n",
       "\n",
       "        [[ 0.5901,  0.6001, -0.5165,  ...,  0.2963,  0.2034,  1.0321],\n",
       "         [ 0.8078,  0.1995, -0.3824,  ...,  0.9018, -0.7067,  1.0001],\n",
       "         [ 0.2083, -2.0880,  0.2439,  ...,  1.8097,  0.4219,  0.5539],\n",
       "         ...,\n",
       "         [-1.3043,  0.3543, -0.9915,  ...,  0.5974,  1.4838,  1.6512],\n",
       "         [-0.5786, -0.1079, -1.2314,  ..., -0.3962, -0.2895,  0.5198],\n",
       "         [ 1.3889, -0.6902, -0.6552,  ...,  0.6637, -0.3890,  0.2125]],\n",
       "\n",
       "        [[-0.1468,  0.2401,  0.8810,  ...,  1.5274,  1.8057, -1.3392],\n",
       "         [-0.7990,  1.9811,  0.5768,  ...,  0.2154, -1.2315, -0.7945],\n",
       "         [-0.7394, -0.1346,  1.1182,  ...,  1.0145, -1.6313,  0.2253],\n",
       "         ...,\n",
       "         [ 0.8421,  0.6082,  1.1485,  ...,  0.9734, -1.6726, -0.1928],\n",
       "         [ 0.0395, -0.6967,  1.9275,  ..., -0.5119, -0.5374,  0.0042],\n",
       "         [-0.1348,  0.4082,  1.2223,  ...,  0.6988, -0.3029, -1.7928]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7126,  0.8236,  1.1997,  ..., -1.1874,  0.2892, -2.4398],\n",
       "         [ 0.2297,  0.2635,  1.4924,  ..., -0.6938, -1.4139, -1.9020],\n",
       "         [-1.5978,  0.0381,  0.8426,  ...,  1.9424, -0.3681,  0.3489],\n",
       "         ...,\n",
       "         [-1.5822,  2.0832,  1.3789,  ...,  0.6617,  1.2086,  0.3329],\n",
       "         [ 0.6340,  0.8750, -0.7819,  ..., -1.1112, -0.4104,  1.1650],\n",
       "         [-0.2395, -0.2285,  2.7601,  ..., -0.8677,  0.1191,  0.6050]],\n",
       "\n",
       "        [[-0.0528,  0.2037,  0.6378,  ...,  0.3151,  0.6242, -0.8783],\n",
       "         [ 0.1871, -2.3039,  0.3342,  ...,  0.6360, -0.7092,  0.6103],\n",
       "         [ 0.1770, -0.8538, -0.9144,  ...,  0.2156, -0.2421, -1.9055],\n",
       "         ...,\n",
       "         [-0.3127,  0.0653, -0.4903,  ..., -0.7365, -0.2558, -2.5542],\n",
       "         [-0.0130, -1.6853, -0.5143,  ..., -0.4904, -0.3547, -0.2130],\n",
       "         [ 0.5484, -2.5859,  0.6973,  ..., -1.7452,  0.0744,  0.3998]],\n",
       "\n",
       "        [[-0.3151,  0.5994,  1.5478,  ...,  0.3991, -1.0024, -2.0475],\n",
       "         [ 2.6797,  0.3584, -1.9800,  ..., -0.2032,  2.0148,  0.2483],\n",
       "         [-0.9131,  0.5593, -0.8395,  ...,  1.4077, -0.7171, -0.9131],\n",
       "         ...,\n",
       "         [ 0.1906,  0.0343, -0.5646,  ...,  0.2572, -1.8194, -0.6010],\n",
       "         [ 0.1385, -0.8043,  1.0302,  ..., -0.6350,  1.6793,  0.0805],\n",
       "         [ 1.1565,  2.0659, -1.4539,  ..., -0.6577, -1.7389, -2.0971]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 20, 512)\n",
    "\n",
    "decoder_layer = DecoderLayer(512, 2048, 8, 0.1)\n",
    "encoder_out = encoder_layer(x)\n",
    "decoder_layer(x, encoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_heads: int,\n",
    "        dropout_ratio: float,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(self.seq_len, self.d_model)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(self.d_model, self.d_ff, self.num_heads, self.dropout_ratio)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_out, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x *= (self.d_model ** 0.5)\n",
    "        x += self.positional_encoding(x)\n",
    "\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, encoder_out, mask)\n",
    "        output = x\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.7404e-01,  1.3745e-02,  1.3542e+00,  ..., -1.0685e+00,\n",
       "          -4.3461e-01,  2.5127e-01],\n",
       "         [ 1.1715e+00,  5.2758e-01,  4.6482e-01,  ...,  8.1149e-02,\n",
       "           4.0944e-01, -7.3005e-01],\n",
       "         [ 4.7255e-01,  3.1805e-01,  1.0587e+00,  ..., -5.2677e-01,\n",
       "          -7.0303e-01, -6.6924e-01],\n",
       "         ...,\n",
       "         [-5.1178e-01,  4.9820e-01,  5.7020e-01,  ..., -1.8420e-01,\n",
       "           2.9207e-01, -4.9416e-01],\n",
       "         [ 1.2077e+00, -4.2450e-01,  7.2602e-01,  ..., -4.5297e-02,\n",
       "           1.3392e-01, -4.9377e-01],\n",
       "         [ 1.1497e+00,  4.7561e-01,  7.0672e-01,  ...,  4.8869e-02,\n",
       "          -5.4957e-02, -1.4425e-01]],\n",
       "\n",
       "        [[ 6.7264e-01,  8.7291e-01,  1.0770e+00,  ...,  1.9916e+00,\n",
       "           1.5692e+00, -4.6445e-01],\n",
       "         [ 7.0538e-01,  4.7634e-01,  9.8668e-01,  ...,  1.2911e+00,\n",
       "           1.0658e+00, -7.0589e-01],\n",
       "         [ 1.8358e+00,  8.0074e-01,  8.4752e-01,  ...,  1.0242e+00,\n",
       "           1.4352e+00, -5.6544e-01],\n",
       "         ...,\n",
       "         [ 1.0219e+00,  8.3862e-01,  1.3450e+00,  ...,  1.8373e+00,\n",
       "           1.3637e+00, -1.2995e+00],\n",
       "         [ 5.7862e-01,  4.7988e-01,  9.8628e-01,  ...,  8.9463e-01,\n",
       "           1.3301e+00, -1.3295e+00],\n",
       "         [ 1.3166e+00,  3.4608e-01,  6.4584e-01,  ...,  1.3877e+00,\n",
       "           1.0478e+00, -7.7368e-01]],\n",
       "\n",
       "        [[-1.7013e+00,  1.8300e-01,  9.7952e-01,  ..., -2.8413e-01,\n",
       "          -7.8798e-01, -1.2323e+00],\n",
       "         [-3.0867e+00, -3.8652e-01,  6.9992e-01,  ..., -4.9350e-01,\n",
       "           1.6346e-01, -1.3120e+00],\n",
       "         [-2.7126e+00,  8.5268e-02, -2.1528e-01,  ..., -2.1050e-01,\n",
       "          -7.4152e-01, -1.5562e+00],\n",
       "         ...,\n",
       "         [-3.2548e+00,  2.1905e-01,  5.8982e-01,  ..., -4.7441e-01,\n",
       "           1.2246e-01, -8.6896e-01],\n",
       "         [-3.0662e+00,  1.5759e-01,  2.9769e-01,  ..., -4.0538e-01,\n",
       "          -5.6199e-01, -1.5573e+00],\n",
       "         [-2.8083e+00, -1.3629e-01,  1.6314e-01,  ..., -3.8340e-01,\n",
       "          -8.0784e-01, -1.4726e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.1754e-01, -1.0762e+00,  1.0774e-01,  ...,  3.1649e-02,\n",
       "           8.2986e-01,  4.5219e-01],\n",
       "         [ 8.8601e-02, -8.1232e-01,  7.4476e-01,  ..., -9.9904e-02,\n",
       "           1.7157e+00,  1.5970e-03],\n",
       "         [-2.4147e-01, -8.1367e-01,  1.1079e-01,  ..., -3.5118e-02,\n",
       "           5.0332e-01,  3.5680e-01],\n",
       "         ...,\n",
       "         [ 3.3410e-01, -5.7373e-01, -2.3882e-01,  ..., -1.1661e-01,\n",
       "           1.6097e+00,  3.8674e-01],\n",
       "         [-3.0288e-01, -1.0202e+00, -1.7203e-02,  ...,  3.5113e-01,\n",
       "           1.2624e+00,  4.9523e-01],\n",
       "         [ 7.1896e-02, -9.3333e-01,  5.4750e-01,  ...,  8.1460e-02,\n",
       "           2.0124e+00,  5.4855e-01]],\n",
       "\n",
       "        [[ 4.7762e-01, -6.6832e-01,  1.9098e+00,  ..., -5.2483e-01,\n",
       "           4.6051e-02,  1.7098e-01],\n",
       "         [ 8.1180e-01, -6.4038e-01,  7.3388e-01,  ..., -3.9186e-01,\n",
       "          -5.6386e-01,  2.0722e-01],\n",
       "         [ 1.0514e+00, -7.4304e-01,  9.6186e-01,  ...,  1.6783e-01,\n",
       "          -3.0157e-01,  2.6258e-01],\n",
       "         ...,\n",
       "         [ 8.8904e-01, -8.2764e-01,  1.6263e+00,  ..., -8.2383e-01,\n",
       "          -3.7483e-01, -1.9693e-01],\n",
       "         [ 1.2823e+00, -5.5644e-01,  9.5270e-01,  ..., -5.1248e-01,\n",
       "          -2.8734e-02, -2.5014e-01],\n",
       "         [ 5.5129e-01, -7.8543e-01,  1.8839e+00,  ..., -5.8193e-01,\n",
       "          -7.2768e-01,  4.7919e-02]],\n",
       "\n",
       "        [[ 1.1779e+00,  7.5690e-01,  1.6657e+00,  ..., -1.2306e+00,\n",
       "          -3.2182e-01, -5.5529e-01],\n",
       "         [ 2.0552e+00, -4.3615e-01,  1.4901e+00,  ..., -1.1767e+00,\n",
       "          -4.3990e-01, -5.6076e-01],\n",
       "         [ 1.0884e+00,  3.1654e-01,  7.4160e-01,  ..., -1.1107e+00,\n",
       "          -5.1514e-01, -5.5418e-01],\n",
       "         ...,\n",
       "         [ 1.4825e+00,  3.0735e-01,  1.8799e+00,  ..., -9.7475e-01,\n",
       "          -5.0018e-01, -1.2190e+00],\n",
       "         [ 1.1317e+00,  3.3098e-01,  1.2553e+00,  ..., -1.0942e+00,\n",
       "          -5.5523e-01, -7.6876e-01],\n",
       "         [ 2.1804e+00,  4.1051e-01,  1.6032e+00,  ..., -1.2490e+00,\n",
       "          -1.5454e-02, -7.8865e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 1000, (32, 20))\n",
    "\n",
    "decoder = Decoder(20, 1000, 6, 512, 2048, 8, 0.1)\n",
    "decoder(x, encoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        encoder_vocab_size: int,\n",
    "        decoder_vocab_size: int,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_heads: int,\n",
    "        dropout_ratio: float,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            self.seq_len,\n",
    "            self.encoder_vocab_size,\n",
    "            self.num_layers,\n",
    "            self.d_model,\n",
    "            self.d_ff,\n",
    "            self.num_heads,\n",
    "            self.dropout_ratio,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            self.seq_len,\n",
    "            self.decoder_vocab_size,\n",
    "            self.num_layers,\n",
    "            self.d_model,\n",
    "            self.d_ff,\n",
    "            self.num_heads,\n",
    "            self.dropout_ratio,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.d_model, self.decoder_vocab_size)\n",
    "\n",
    "    def create_padding_mask(self, x):\n",
    "        padding_mask = torch.where(x==0, 1, 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return padding_mask\n",
    "    \n",
    "    def _create_look_ahead_mask(self, x):\n",
    "        look_ahead_mask = torch.triu(torch.ones(self.seq_len, self.seq_len), diagonal=1)\n",
    "\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def create_decoder_mask(self, x):\n",
    "        padding_mask = self.create_padding_mask(x).squeeze(1)  # batch_size, 1, 1, seq_len -> batch_size, 1, seq_len\n",
    "        look_ahead_mask = self._create_look_ahead_mask(x).unsqueeze(0)  # seq_len, seq_len -> batch_size, seq_len, seq_len\n",
    "        decoder_mask = torch.max(padding_mask, look_ahead_mask) # batch_size, seq_len, seq_len\n",
    "        decoder_mask = decoder_mask.unsqueeze(1)    # batch_size, num_heads, seq_len, seq_len\n",
    "\n",
    "        return decoder_mask\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_padding_mask = self.create_padding_mask(encoder_input)\n",
    "        decoder_mask = self.create_decoder_mask(decoder_input)\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input, encoder_padding_mask)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, decoder_mask)\n",
    "        output = self.linear(decoder_output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    20,\n",
    "    1000,\n",
    "    2000,\n",
    "    6,\n",
    "    512,\n",
    "    2048,\n",
    "    8,\n",
    "    0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0820, -1.1812, -0.1125,  ...,  1.4003, -0.2161,  0.1365],\n",
       "         [-0.4783, -0.3005, -0.3276,  ...,  0.7966,  0.4404,  0.2628],\n",
       "         [-0.5069, -0.5645,  0.0715,  ...,  0.9491,  0.0972, -0.2219],\n",
       "         ...,\n",
       "         [-0.4835, -0.2690, -0.1206,  ...,  0.5565,  0.0024,  0.0447],\n",
       "         [-0.0030, -0.3034, -0.4317,  ...,  0.6190,  0.1846,  0.1748],\n",
       "         [ 0.1121, -0.7533, -0.6052,  ...,  0.9467,  0.6045, -0.1068]],\n",
       "\n",
       "        [[ 0.0652, -0.0562, -0.4453,  ...,  0.1493,  0.0606, -0.5253],\n",
       "         [ 0.2705, -0.1263, -0.4444,  ..., -0.3763, -0.2798, -0.1256],\n",
       "         [ 0.3461, -0.5293, -0.3556,  ..., -0.0823, -0.4584, -0.4155],\n",
       "         ...,\n",
       "         [ 0.4573, -0.4650, -0.6154,  ..., -0.0576,  0.1152, -0.4791],\n",
       "         [ 0.6136, -0.4380, -0.5972,  ...,  0.0552, -0.2022, -0.6156],\n",
       "         [ 0.4565, -0.0433, -0.8857,  ..., -0.2820, -0.3574, -0.6190]],\n",
       "\n",
       "        [[ 0.0997, -0.1967,  0.2268,  ...,  0.1672, -0.6713,  0.4421],\n",
       "         [-0.0685, -0.2879,  0.3876,  ...,  0.2685, -0.4341,  0.6359],\n",
       "         [ 0.0935, -0.1327,  0.3351,  ...,  0.6251, -0.5398,  0.6132],\n",
       "         ...,\n",
       "         [-0.1168, -0.3222,  0.5486,  ...,  0.4864, -0.4612,  0.9694],\n",
       "         [-0.2241, -0.1773,  0.1957,  ...,  0.3738, -0.4091,  0.9269],\n",
       "         [ 0.5259, -0.5975,  0.2886,  ...,  0.7919, -0.5232,  0.3865]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3963, -0.2374, -0.1596,  ..., -0.9092, -0.5284,  0.6057],\n",
       "         [-0.0634,  0.1882, -0.8380,  ..., -1.1330, -0.3827,  0.7329],\n",
       "         [-0.4132, -0.0485, -0.2381,  ..., -0.8926,  0.1626,  0.9863],\n",
       "         ...,\n",
       "         [ 0.1213, -0.1795, -0.3161,  ..., -0.8486, -0.3873,  0.9637],\n",
       "         [-0.2098, -0.3645, -0.2525,  ..., -0.7116, -0.4039,  0.5928],\n",
       "         [-0.2413, -0.3811,  0.2197,  ..., -1.0145, -0.2657,  0.6031]],\n",
       "\n",
       "        [[ 0.1497, -0.4746, -0.1178,  ..., -0.1131,  0.0526,  0.2097],\n",
       "         [-0.0248, -0.3432, -0.2052,  ..., -0.4403,  0.1881,  0.1628],\n",
       "         [ 0.4729, -0.6453, -0.1398,  ...,  0.2933,  0.2714,  0.1694],\n",
       "         ...,\n",
       "         [-0.1632, -0.6976,  0.1102,  ...,  0.1304,  0.2237,  0.4947],\n",
       "         [ 0.1493, -0.5277, -0.0921,  ...,  0.1087,  0.0689,  0.0355],\n",
       "         [ 0.1009, -0.3210, -0.0823,  ...,  0.1419,  0.2207,  0.1184]],\n",
       "\n",
       "        [[ 0.3657, -1.0526,  0.5379,  ..., -1.0619, -0.4716,  0.4686],\n",
       "         [ 0.5109, -1.0100,  0.6436,  ..., -0.7834, -0.2554,  0.1695],\n",
       "         [ 0.4062, -1.0867,  0.7728,  ..., -0.6112, -0.3884,  0.3834],\n",
       "         ...,\n",
       "         [ 0.5347, -1.1623,  0.4949,  ..., -0.7768, -0.0159,  0.1601],\n",
       "         [ 0.2760, -1.0014,  0.7039,  ..., -0.6680,  0.2200,  0.2005],\n",
       "         [ 0.1816, -1.0130,  0.3604,  ..., -0.6408, -0.1289,  0.3446]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoder = torch.randint(0, 1000, (32, 20))\n",
    "x_decoder = torch.randint(0, 2000, (32, 20))\n",
    "\n",
    "transformer(x_encoder, x_decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4af6128c7e0808fede432f38729c473c5b0d80882e83c469acdb54455c56396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
